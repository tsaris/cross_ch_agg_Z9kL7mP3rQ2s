
Lmod is automatically replacing "cce/18.0.1" with "gcc-native/13.2".


Lmod is automatically replacing "PrgEnv-cray/8.6.0" with "PrgEnv-gnu/8.6.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) cray-libsci/24.11.0     2) cray-mpich/8.1.31     3) darshan-runtime/3.4.6-mpi


Lmod is automatically replacing "gcc-native/13.2" with "gcc/12.2.0".


Inactive Modules:
  1) darshan-runtime

The following have been reloaded with a version change:
  1) cray-libsci/24.11.0 => cray-libsci/23.09.1.1
  2) cray-mpich/8.1.31 => cray-mpich/8.1.27
  3) libfabric/1.22.0 => libfabric/1.20.1

Lmod has detected the following error: These module(s) or extension(s) exist
but cannot be loaded as requested: "darshan-runtime"
   Try: "module spider darshan-runtime" to see how to load the module(s).



for i in {orbit,}; do for j in {2,}; do for k in {1}; do python train.py \ configs/appl.yaml \ --max_epochs 10000 \ --fsdp_size 2 \ --simple_ddp_size 2 \ --seq_par_size 1 \ --tensor_par_size 8 \ --dataset appl \ --arch $i \ --batch_size $j \ --embed_dim 4096 \ --depth 32 \ --num_heads 32 echo "sleeping..." sleep 5 echo "Done" done done done
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,555] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,555] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,673] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,673] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,673] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,695] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,695] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,695] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,695] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,695] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,696] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,782] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,782] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,782] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,782] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,782] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,862] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,862] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,862] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,970] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,970] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,970] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,970] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,973] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,173] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,173] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,263] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,263] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,263] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,263] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,282] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,282] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01082.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01082.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01082.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01082.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01082.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01082.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01082.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01082.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01082.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01082.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01082.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01082.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01082.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01082.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01082.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01082.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
HERE0 Namespace(arch='orbit', batch_size=2, dataset='appl', depth=32, embed_dim=4096, fsdp_size=2, log_dir='./output_dir', max_epochs=10000, num_heads=32, seq_par_size=1, simple_ddp_size=2, tensor_par_size=8, yaml_config='configs/appl.yaml')
[W socket.cpp:464] [c10d] The server socket cannot be initialized on [::]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01082.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  32
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01082.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  32
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01082.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  32
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01082.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01082.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01082.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01082.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  32
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01082.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  32
rank 2 Before initialize parallelism groups
rank 4 Before initialize parallelism groups
rank 1 Before initialize parallelism groups
rank 3 Before initialize parallelism groups
rank 7 Before initialize parallelism groups
rank 6 Before initialize parallelism groups
rank 1 After initialize parallelism groups
rank 2 After initialize parallelism groups
rank 7 After initialize parallelism groups
rank 6 After initialize parallelism groups
rank 4 After initialize parallelism groups
rank 3 After initialize parallelism groups
rank 5 Before initialize parallelism groups
rank 5 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01082.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  32
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01082.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01082.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01082.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01082.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01082.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  32
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01082.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  32
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01082.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  32
rank 24 Before initialize parallelism groups
rank 26 Before initialize parallelism groups
rank 29 Before initialize parallelism groups
rank 31 Before initialize parallelism groups
rank 27 Before initialize parallelism groups
rank 28 Before initialize parallelism groups
rank 30 Before initialize parallelism groups
rank 25 Before initialize parallelism groups
rank 27 After initialize parallelism groups
rank 28 After initialize parallelism groups
rank 30 After initialize parallelism groups
rank 26 After initialize parallelism groups
rank 29 After initialize parallelism groups
rank 31 After initialize parallelism groups
rank 24 After initialize parallelism groups
rank 25 After initialize parallelism groups
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
rank 9 Before initialize parallelism groups
rank 13 Before initialize parallelism groups
rank 12 Before initialize parallelism groups
rank 14 Before initialize parallelism groups
rank 15 Before initialize parallelism groups
rank 8 Before initialize parallelism groups
rank 11 Before initialize parallelism groups
rank 10 Before initialize parallelism groups
rank 9 After initialize parallelism groups
rank 11 After initialize parallelism groups
rank 14 After initialize parallelism groups
rank 12 After initialize parallelism groups
rank 8 After initialize parallelism groups
rank 10 After initialize parallelism groups
rank 15 After initialize parallelism groups
rank 13 After initialize parallelism groups
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
config_path  configs/appl.yaml
rank 16 Before initialize parallelism groups
rank 22 Before initialize parallelism groups
rank 18 Before initialize parallelism groups
rank 20 Before initialize parallelism groups
rank 23 Before initialize parallelism groups
rank 17 Before initialize parallelism groups
rank 16 After initialize parallelism groups
rank 22 After initialize parallelism groups
rank 18 After initialize parallelism groups
rank 23 After initialize parallelism groups
rank 17 After initialize parallelism groups
rank 20 After initialize parallelism groups
rank 21 Before initialize parallelism groups
{'seed_everything': 42, 'trainer': {'max_epochs': 1, 'checkpoint_path': '/lustre/orion/stf218/world-shared/atsaris/tmp/tmp_checkpoints', 'checkpoint_filename': 'multi_last', 'resume_from_checkpoint': False}, 'parallelism': {'cpu_offloading': False}, 'model': {'lr': 0.015, 'beta_1': 0.9, 'beta_2': 0.95, 'weight_decay': 0.05, 'warmup_steps': 10, 'max_steps': 20, 'warmup_start_lr': '1e-8', 'eta_min': '1e-8', 'net': {'class_path': 'climax.arch.ClimaX', 'init_args': {'default_vars': ['R', 'G', 'B', 'W7N', 'TXN', 'D6V', '8Z7', 'QNY', 'QBR', 'QOM', 'IMY', 'DGZ', 'UY8', 'EMJ', 'DXK', 'NSH', 'Z4V', 'O8Y', 'RA6', 'QDP', '6BF', 'X8M', 'B61', '61N', 'A8K', 'CN5', 'NF9', 'CRB', '4RF', 'SAO', 'ZA6', 'A6P', 'EME', 'TVJ', 'SR7', 'J6U', 'GE6', 'OGJ', 'Y4M', 'TEF', '601', 'CS6', '2N3', '4HM', 'CEL', 'XFH', '40J', 'JBR', '94X', 'PI7', 'CGA', 'U1A', 'EY3', 'GBW', 'VW0', 'TT5', '43R', 'XAK', 'WS0', 'FRW', 'X9X', 'QA7', 'JCH', 'HFQ', 'EP3', 'Z3F', 'JIU', 'K8O', '4TP', 'GNL', 'HLC', 'L7R', 'WSZ', '0X1', 'GHZ', 'KJ9', 'F7F', 'H4I', 'I53', 'YOI', 'YK6', 'YEC', 'JVR', 'V47', 'TXP', 'XPI', '4FX', 'FNW', 'I94', 'U5X', 'I10', 'OFY', 'CHF', '6MU', '923', 'K81', '8NF', 'FY1', 'NX4', 'YUG', '8Y1', 'BWQ', 'FLJ', '9DF', 'OBB', 'B7R', 'DB7', 'SH1', 'G2Y', 'CR4', '26W', 'QWK', '45X', '25D', '2G7', '5I5', '52R', 'KIU', 'K02', 'B3K', '16C', '7T8', '9Z1', '5VG', 'MIR', 'NBI', 'FMU', 'HTH', 'VNJ', 'Y3O', 'KGJ', '9VO', 'HKU', 'OKP', '54D', 'KBJ', 'F24', 'WBJ', 'B2M', 'VSV', 'JZM', 'BPX', 'EKE', 'UHE', 'KF5', 'TYL', 'S3E', 'NLF', 'NNK', 'WII', 'FS6', 'JWH', 'K1X', 'QWR', 'GWX', '1GB', 'O7P', 'ALX', 'TQZ', '4BL', 'UUD', 'NS6', 'IA5', '1T8', 'LUN', 'FAA', 'C0R', 'DUI', 'NP5', '88J', '5X8', 'A64', '1NB', 'CQK', '0UX', 'E23', 'WCZ', 'CSZ', 'AG5', 'IQN', 'X0B', 'S95', 'S4N', 'IZB', 'JTV', 'H87', 'UCJ', 'UVM', '22N', 'P4G', '8QH', '775', 'Q4A', 'KI7', 'PCT', 'A8M', 'KJ7', '7W5', 'XQ6', 'Q3K', 'AZS', '23S', '5LT', 'WAI', 'VQ1', 'BFG', 'IOY', 'IZ7', 'FCO', '2KJ', 'DF6', 'UXO', '7K5', 'A3A', 'XSW', 'JHB', '7XL', 'M01', 'EWU', '1CW', '8IH', 'MYA', '3P8', 'R6H', 'W0G', 'Q0S', '4VR', 'K2G', '7M5', '77Y', 'AN8', 'E7N', 'K2N', '02I', 'CZY', '738', 'G0B', 'Q9T', 'FZF', 'WJ9', '6P0', 'QQX', '0GV', '516', 'PBP', '25S', 'YGN', '47U', '54E', 'NAK', 'IET', '256', 'QXK', 'JBX', 'COZ', 'V5U', 'VG4', '198', 'EDT', 'A5V', '080', '2L9', 'OXJ', 'DGL', '1AQ', 'JXZ', 'TWF', '248', '4NF', 'KCD', '61Z', 'B3R', 'R6U', 'XGF', 'EFV', '32S', 'C64', '606', 'XGK', 'OOJ', '7L0', 'XXD', 'K9Y', 'PET', '6PU', 'GMY', 'OCP', '6YV', '5AU', '6EQ', 'ICF', 'JV9', '1OW', 'EVL', 'C3N', '6LH', 'L3H', 'Z55', '6XD', '37V', '68M', 'RNT', '8ND', 'GU5', 'QW5', 'VYW', '3AQ', '8H8', 'QG5', 'E64', 'DF9', 'X5C', '15Q', '3KP', 'LFV', 'SRT', 'SAM', 'IKB', 'YLV', '4FM', '6Z3', '910', 'BLL', 'AYL', 'XDS', 'TUY', '4GQ', 'SZ8', '76V', 'ZJG', 'B6V', 'N6B', '00B', 'IEM', 'H4B', '1JM', 'PJA', '597', 'AYY', 'M89', '60M', 'UE1', '5GU', '541', '2AQ', 'VY8', 'GDT', 'RSI', 'C1Q', 'B0H', '5KR', '04V', 'G61', 'HR3', 'FOD', '757', 'OWT', '5OS', 'QF9', '1W0', 'SFA', 'JU4', 'NV2', 'OMG', 'GO7', 'WBV', '741', 'KW9', '3GK', 'E2Z', 'YB3', 'T46', 'B2I', '1Y8', '58P', 'NX4', 'YTR', '4L8', 'B7F', 'PA9', 'CS3', '2TR', 'P8E', 'XLF', 'UQ2', '73B', 'IX7', '8ZP', '1UO', 'DBX', '4SL', 'K1F', '9OD', 'QZC', 'OG8', '54G', 'SN9', 'FEI', '6LZ', 'JFS', '3SA', 'YD5', 'PTP', 'QJQ', 'Y92', '6LV', 'TGT', '9EN', 'MZY', '18U', 'ZII', '9LD', '51X', 'ZSQ', 'VWM', 'QH9', 'BA3', 'DLS', 'G58', 'VRW', 'SBV', 'CVW', 'X3I', 'DSU', 'HNF', 'LLI', '557', 'XMI', 'J31', 'M5Q', 'CIU', 'S3P', 'LVQ', 'O8H', 'QUI', 'WJQ', 'R4Z', 'WB5', 'GL6', 'PE4', '3G8', 'XY0', 'OBR', '7BZ', 'O5T', '1OP', 'IPK', 'TCR', 'XXM', '21C', 'SBV', '4SL', 'WIE', 'A40', 'AKQ', 'PPO', 'PAV', 'QOX', '6N6', 'AKJ', 'UBM', '2F3', 'CI2', '8PX', 'ZUN', 'PLM', 'UCK', '5V0', 'WZP', 'AOJ', 'WWO', '005', 'KB3', 'Y4U', 'SFO', 'JUK', 'GVA', 'EQ3', '35G', '8X0', 'Q4I', 'YCJ', 'Y5A', 'JAW', 'KML', '01I', 'SCH', 'RJO', 'DHT', 'KVE', 'AAO', 'U4E', 'S3H', '59B', 'JIX', 'D5Y'], 'img_size': [192, 384], 'patch_size': 6, 'embed_dim': 512, 'depth': 8, 'decoder_depth': 2, 'num_heads': 16, 'mlp_ratio': 4, 'drop_path': 0.1, 'drop_rate': 0.1, 'aggregated_variables': 1, 'mask_ratio': 0.75}}}, 'data': {'dict_root_dirs': {'mpi-esm': '/lustre/orion/stf218/scratch/atsaris/APPL_DATA'}, 'dict_in_variables': {'mpi-esm': ['R', 'G', 'B', 'W7N', 'TXN', 'D6V', '8Z7', 'QNY', 'QBR', 'QOM', 'IMY', 'DGZ', 'UY8', 'EMJ', 'DXK', 'NSH', 'Z4V', 'O8Y', 'RA6', 'QDP', '6BF', 'X8M', 'B61', '61N', 'A8K', 'CN5', 'NF9', 'CRB', '4RF', 'SAO', 'ZA6', 'A6P', 'EME', 'TVJ', 'SR7', 'J6U', 'GE6', 'OGJ', 'Y4M', 'TEF', '601', 'CS6', '2N3', '4HM', 'CEL', 'XFH', '40J', 'JBR', '94X', 'PI7', 'CGA', 'U1A', 'EY3', 'GBW', 'VW0', 'TT5', '43R', 'XAK', 'WS0', 'FRW', 'X9X', 'QA7', 'JCH', 'HFQ', 'EP3', 'Z3F', 'JIU', 'K8O', '4TP', 'GNL', 'HLC', 'L7R', 'WSZ', '0X1', 'GHZ', 'KJ9', 'F7F', 'H4I', 'I53', 'YOI', 'YK6', 'YEC', 'JVR', 'V47', 'TXP', 'XPI', '4FX', 'FNW', 'I94', 'U5X', 'I10', 'OFY', 'CHF', '6MU', '923', 'K81', '8NF', 'FY1', 'NX4', 'YUG', '8Y1', 'BWQ', 'FLJ', '9DF', 'OBB', 'B7R', 'DB7', 'SH1', 'G2Y', 'CR4', '26W', 'QWK', '45X', '25D', '2G7', '5I5', '52R', 'KIU', 'K02', 'B3K', '16C', '7T8', '9Z1', '5VG', 'MIR', 'NBI', 'FMU', 'HTH', 'VNJ', 'Y3O', 'KGJ', '9VO', 'HKU', 'OKP', '54D', 'KBJ', 'F24', 'WBJ', 'B2M', 'VSV', 'JZM', 'BPX', 'EKE', 'UHE', 'KF5', 'TYL', 'S3E', 'NLF', 'NNK', 'WII', 'FS6', 'JWH', 'K1X', 'QWR', 'GWX', '1GB', 'O7P', 'ALX', 'TQZ', '4BL', 'UUD', 'NS6', 'IA5', '1T8', 'LUN', 'FAA', 'C0R', 'DUI', 'NP5', '88J', '5X8', 'A64', '1NB', 'CQK', '0UX', 'E23', 'WCZ', 'CSZ', 'AG5', 'IQN', 'X0B', 'S95', 'S4N', 'IZB', 'JTV', 'H87', 'UCJ', 'UVM', '22N', 'P4G', '8QH', '775', 'Q4A', 'KI7', 'PCT', 'A8M', 'KJ7', '7W5', 'XQ6', 'Q3K', 'AZS', '23S', '5LT', 'WAI', 'VQ1', 'BFG', 'IOY', 'IZ7', 'FCO', '2KJ', 'DF6', 'UXO', '7K5', 'A3A', 'XSW', 'JHB', '7XL', 'M01', 'EWU', '1CW', '8IH', 'MYA', '3P8', 'R6H', 'W0G', 'Q0S', '4VR', 'K2G', '7M5', '77Y', 'AN8', 'E7N', 'K2N', '02I', 'CZY', '738', 'G0B', 'Q9T', 'FZF', 'WJ9', '6P0', 'QQX', '0GV', '516', 'PBP', '25S', 'YGN', '47U', '54E', 'NAK', 'IET', '256', 'QXK', 'JBX', 'COZ', 'V5U', 'VG4', '198', 'EDT', 'A5V', '080', '2L9', 'OXJ', 'DGL', '1AQ', 'JXZ', 'TWF', '248', '4NF', 'KCD', '61Z', 'B3R', 'R6U', 'XGF', 'EFV', '32S', 'C64', '606', 'XGK', 'OOJ', '7L0', 'XXD', 'K9Y', 'PET', '6PU', 'GMY', 'OCP', '6YV', '5AU', '6EQ', 'ICF', 'JV9', '1OW', 'EVL', 'C3N', '6LH', 'L3H', 'Z55', '6XD', '37V', '68M', 'RNT', '8ND', 'GU5', 'QW5', 'VYW', '3AQ', '8H8', 'QG5', 'E64', 'DF9', 'X5C', '15Q', '3KP', 'LFV', 'SRT', 'SAM', 'IKB', 'YLV', '4FM', '6Z3', '910', 'BLL', 'AYL', 'XDS', 'TUY', '4GQ', 'SZ8', '76V', 'ZJG', 'B6V', 'N6B', '00B', 'IEM', 'H4B', '1JM', 'PJA', '597', 'AYY', 'M89', '60M', 'UE1', '5GU', '541', '2AQ', 'VY8', 'GDT', 'RSI', 'C1Q', 'B0H', '5KR', '04V', 'G61', 'HR3', 'FOD', '757', 'OWT', '5OS', 'QF9', '1W0', 'SFA', 'JU4', 'NV2', 'OMG', 'GO7', 'WBV', '741', 'KW9', '3GK', 'E2Z', 'YB3', 'T46', 'B2I', '1Y8', '58P', 'NX4', 'YTR', '4L8', 'B7F', 'PA9', 'CS3', '2TR', 'P8E', 'XLF', 'UQ2', '73B', 'IX7', '8ZP', '1UO', 'DBX', '4SL', 'K1F', '9OD', 'QZC', 'OG8', '54G', 'SN9', 'FEI', '6LZ', 'JFS', '3SA', 'YD5', 'PTP', 'QJQ', 'Y92', '6LV', 'TGT', '9EN', 'MZY', '18U', 'ZII', '9LD', '51X', 'ZSQ', 'VWM', 'QH9', 'BA3', 'DLS', 'G58', 'VRW', 'SBV', 'CVW', 'X3I', 'DSU', 'HNF', 'LLI', '557', 'XMI', 'J31', 'M5Q', 'CIU', 'S3P', 'LVQ', 'O8H', 'QUI', 'WJQ', 'R4Z', 'WB5', 'GL6', 'PE4', '3G8', 'XY0', 'OBR', '7BZ', 'O5T', '1OP', 'IPK', 'TCR', 'XXM', '21C', 'SBV', '4SL', 'WIE', 'A40', 'AKQ', 'PPO', 'PAV', 'QOX', '6N6', 'AKJ', 'UBM', '2F3', 'CI2', '8PX', 'ZUN', 'PLM', 'UCK', '5V0', 'WZP', 'AOJ', 'WWO', '005', 'KB3', 'Y4U', 'SFO', 'JUK', 'GVA', 'EQ3', '35G', '8X0', 'Q4I', 'YCJ', 'Y5A', 'JAW', 'KML', '01I', 'SCH', 'RJO', 'DHT', 'KVE', 'AAO', 'U4E', 'S3H', '59B', 'JIX', 'D5Y']}, 'batch_size': 2, 'num_workers': 4, 'pin_memory': True}}
rank 19 Before initialize parallelism groups
rank 21 After initialize parallelism groups
max_epochs 10000 data_par_size 4 fsdp_size 2 simple_ddp_size 2 tensor_par_size 8 seq_par_size 1 cpu_offloading False
lr  0.015 beta_1  0.9 beta_2 0.95 weight_decay 0.05 class_path climax.arch.ClimaX default_vars ['R', 'G', 'B', 'W7N', 'TXN', 'D6V', '8Z7', 'QNY', 'QBR', 'QOM', 'IMY', 'DGZ', 'UY8', 'EMJ', 'DXK', 'NSH', 'Z4V', 'O8Y', 'RA6', 'QDP', '6BF', 'X8M', 'B61', '61N', 'A8K', 'CN5', 'NF9', 'CRB', '4RF', 'SAO', 'ZA6', 'A6P', 'EME', 'TVJ', 'SR7', 'J6U', 'GE6', 'OGJ', 'Y4M', 'TEF', '601', 'CS6', '2N3', '4HM', 'CEL', 'XFH', '40J', 'JBR', '94X', 'PI7', 'CGA', 'U1A', 'EY3', 'GBW', 'VW0', 'TT5', '43R', 'XAK', 'WS0', 'FRW', 'X9X', 'QA7', 'JCH', 'HFQ', 'EP3', 'Z3F', 'JIU', 'K8O', '4TP', 'GNL', 'HLC', 'L7R', 'WSZ', '0X1', 'GHZ', 'KJ9', 'F7F', 'H4I', 'I53', 'YOI', 'YK6', 'YEC', 'JVR', 'V47', 'TXP', 'XPI', '4FX', 'FNW', 'I94', 'U5X', 'I10', 'OFY', 'CHF', '6MU', '923', 'K81', '8NF', 'FY1', 'NX4', 'YUG', '8Y1', 'BWQ', 'FLJ', '9DF', 'OBB', 'B7R', 'DB7', 'SH1', 'G2Y', 'CR4', '26W', 'QWK', '45X', '25D', '2G7', '5I5', '52R', 'KIU', 'K02', 'B3K', '16C', '7T8', '9Z1', '5VG', 'MIR', 'NBI', 'FMU', 'HTH', 'VNJ', 'Y3O', 'KGJ', '9VO', 'HKU', 'OKP', '54D', 'KBJ', 'F24', 'WBJ', 'B2M', 'VSV', 'JZM', 'BPX', 'EKE', 'UHE', 'KF5', 'TYL', 'S3E', 'NLF', 'NNK', 'WII', 'FS6', 'JWH', 'K1X', 'QWR', 'GWX', '1GB', 'O7P', 'ALX', 'TQZ', '4BL', 'UUD', 'NS6', 'IA5', '1T8', 'LUN', 'FAA', 'C0R', 'DUI', 'NP5', '88J', '5X8', 'A64', '1NB', 'CQK', '0UX', 'E23', 'WCZ', 'CSZ', 'AG5', 'IQN', 'X0B', 'S95', 'S4N', 'IZB', 'JTV', 'H87', 'UCJ', 'UVM', '22N', 'P4G', '8QH', '775', 'Q4A', 'KI7', 'PCT', 'A8M', 'KJ7', '7W5', 'XQ6', 'Q3K', 'AZS', '23S', '5LT', 'WAI', 'VQ1', 'BFG', 'IOY', 'IZ7', 'FCO', '2KJ', 'DF6', 'UXO', '7K5', 'A3A', 'XSW', 'JHB', '7XL', 'M01', 'EWU', '1CW', '8IH', 'MYA', '3P8', 'R6H', 'W0G', 'Q0S', '4VR', 'K2G', '7M5', '77Y', 'AN8', 'E7N', 'K2N', '02I', 'CZY', '738', 'G0B', 'Q9T', 'FZF', 'WJ9', '6P0', 'QQX', '0GV', '516', 'PBP', '25S', 'YGN', '47U', '54E', 'NAK', 'IET', '256', 'QXK', 'JBX', 'COZ', 'V5U', 'VG4', '198', 'EDT', 'A5V', '080', '2L9', 'OXJ', 'DGL', '1AQ', 'JXZ', 'TWF', '248', '4NF', 'KCD', '61Z', 'B3R', 'R6U', 'XGF', 'EFV', '32S', 'C64', '606', 'XGK', 'OOJ', '7L0', 'XXD', 'K9Y', 'PET', '6PU', 'GMY', 'OCP', '6YV', '5AU', '6EQ', 'ICF', 'JV9', '1OW', 'EVL', 'C3N', '6LH', 'L3H', 'Z55', '6XD', '37V', '68M', 'RNT', '8ND', 'GU5', 'QW5', 'VYW', '3AQ', '8H8', 'QG5', 'E64', 'DF9', 'X5C', '15Q', '3KP', 'LFV', 'SRT', 'SAM', 'IKB', 'YLV', '4FM', '6Z3', '910', 'BLL', 'AYL', 'XDS', 'TUY', '4GQ', 'SZ8', '76V', 'ZJG', 'B6V', 'N6B', '00B', 'IEM', 'H4B', '1JM', 'PJA', '597', 'AYY', 'M89', '60M', 'UE1', '5GU', '541', '2AQ', 'VY8', 'GDT', 'RSI', 'C1Q', 'B0H', '5KR', '04V', 'G61', 'HR3', 'FOD', '757', 'OWT', '5OS', 'QF9', '1W0', 'SFA', 'JU4', 'NV2', 'OMG', 'GO7', 'WBV', '741', 'KW9', '3GK', 'E2Z', 'YB3', 'T46', 'B2I', '1Y8', '58P', 'NX4', 'YTR', '4L8', 'B7F', 'PA9', 'CS3', '2TR', 'P8E', 'XLF', 'UQ2', '73B', 'IX7', '8ZP', '1UO', 'DBX', '4SL', 'K1F', '9OD', 'QZC', 'OG8', '54G', 'SN9', 'FEI', '6LZ', 'JFS', '3SA', 'YD5', 'PTP', 'QJQ', 'Y92', '6LV', 'TGT', '9EN', 'MZY', '18U', 'ZII', '9LD', '51X', 'ZSQ', 'VWM', 'QH9', 'BA3', 'DLS', 'G58', 'VRW', 'SBV', 'CVW', 'X3I', 'DSU', 'HNF', 'LLI', '557', 'XMI', 'J31', 'M5Q', 'CIU', 'S3P', 'LVQ', 'O8H', 'QUI', 'WJQ', 'R4Z', 'WB5', 'GL6', 'PE4', '3G8', 'XY0', 'OBR', '7BZ', 'O5T', '1OP', 'IPK', 'TCR', 'XXM', '21C', 'SBV', '4SL', 'WIE', 'A40', 'AKQ', 'PPO', 'PAV', 'QOX', '6N6', 'AKJ', 'UBM', '2F3', 'CI2', '8PX', 'ZUN', 'PLM', 'UCK', '5V0', 'WZP', 'AOJ', 'WWO', '005', 'KB3', 'Y4U', 'SFO', 'JUK', 'GVA', 'EQ3', '35G', '8X0', 'Q4I', 'YCJ', 'Y5A', 'JAW', 'KML', '01I', 'SCH', 'RJO', 'DHT', 'KVE', 'AAO', 'U4E', 'S3H', '59B', 'JIX', 'D5Y']
img_size [192, 384] img_size_x 192 img_size_y 384 patch_size 6 emb_dim 4096 depth 32 decoder_depth 2 num_heads 32 mlp_ratio 4 drop_path 0.1 drop_rate 0.1 aggregated_variables 1
warmup_steps 10 max_steps 20 warmup_start_lr 1e-08 eta_min 1e-08
checkpoint_path /lustre/orion/stf218/world-shared/atsaris/tmp/tmp_checkpoints checkpoint_filename multi_last resume_from_checkpoint False
rank 0 Before initialize parallelism groups
rank 19 After initialize parallelism groups
rank 0 After initialize parallelism groups
rank 1 After initialize model
rank 1 before dist.barrier(group)
rank 3 After initialize model
rank 3 before dist.barrier(group)
rank 6 After initialize model
rank 6 before dist.barrier(group)
rank 2 After initialize model
rank 2 before dist.barrier(group)
rank 7 After initialize model
rank 7 before dist.barrier(group)
rank 25 After initialize model
rank 25 before dist.barrier(group)
rank 4 After initialize model
rank 4 before dist.barrier(group)
rank 5 After initialize model
rank 5 before dist.barrier(group)
rank 24 After initialize model
rank 24 before dist.barrier(group)
rank 27 After initialize model
rank 27 before dist.barrier(group)
rank 26 After initialize model
rank 26 before dist.barrier(group)
rank 28 After initialize model
rank 28 before dist.barrier(group)
rank 30 After initialize model
rank 30 before dist.barrier(group)
rank 31 After initialize model
rank 31 before dist.barrier(group)
rank 29 After initialize model
rank 29 before dist.barrier(group)
rank 0 After initialize model
resume from checkpoint was set to False. Pretrain from scratch.
rank 0 init_model_dict.keys() dict_keys(['var_embed', 'var_query', 'pos_embed', 'mask_token', 'decoder_pos_embed', 'token_embeds.0.proj.weight', 'token_embeds.0.proj.bias', 'token_embeds.1.proj.weight', 'token_embeds.1.proj.bias', 'token_embeds.2.proj.weight', 'token_embeds.2.proj.bias', 'token_embeds.3.proj.weight', 'token_embeds.3.proj.bias', 'token_embeds.4.proj.weight', 'token_embeds.4.proj.bias', 'token_embeds.5.proj.weight', 'token_embeds.5.proj.bias', 'token_embeds.6.proj.weight', 'token_embeds.6.proj.bias', 'token_embeds.7.proj.weight', 'token_embeds.7.proj.bias', 'token_embeds.8.proj.weight', 'token_embeds.8.proj.bias', 'token_embeds.9.proj.weight', 'token_embeds.9.proj.bias', 'token_embeds.10.proj.weight', 'token_embeds.10.proj.bias', 'token_embeds.11.proj.weight', 'token_embeds.11.proj.bias', 'token_embeds.12.proj.weight', 'token_embeds.12.proj.bias', 'token_embeds.13.proj.weight', 'token_embeds.13.proj.bias', 'token_embeds.14.proj.weight', 'token_embeds.14.proj.bias', 'token_embeds.15.proj.weight', 'token_embeds.15.proj.bias', 'token_embeds.16.proj.weight', 'token_embeds.16.proj.bias', 'token_embeds.17.proj.weight', 'token_embeds.17.proj.bias', 'token_embeds.18.proj.weight', 'token_embeds.18.proj.bias', 'token_embeds.19.proj.weight', 'token_embeds.19.proj.bias', 'token_embeds.20.proj.weight', 'token_embeds.20.proj.bias', 'token_embeds.21.proj.weight', 'token_embeds.21.proj.bias', 'token_embeds.22.proj.weight', 'token_embeds.22.proj.bias', 'token_embeds.23.proj.weight', 'token_embeds.23.proj.bias', 'token_embeds.24.proj.weight', 'token_embeds.24.proj.bias', 'token_embeds.25.proj.weight', 'token_embeds.25.proj.bias', 'token_embeds.26.proj.weight', 'token_embeds.26.proj.bias', 'token_embeds.27.proj.weight', 'token_embeds.27.proj.bias', 'token_embeds.28.proj.weight', 'token_embeds.28.proj.bias', 'token_embeds.29.proj.weight', 'token_embeds.29.proj.bias', 'token_embeds.30.proj.weight', 'token_embeds.30.proj.bias', 'token_embeds.31.proj.weight', 'token_embeds.31.proj.bias', 'token_embeds.32.proj.weight', 'token_embeds.32.proj.bias', 'token_embeds.33.proj.weight', 'token_embeds.33.proj.bias', 'token_embeds.34.proj.weight', 'token_embeds.34.proj.bias', 'token_embeds.35.proj.weight', 'token_embeds.35.proj.bias', 'token_embeds.36.proj.weight', 'token_embeds.36.proj.bias', 'token_embeds.37.proj.weight', 'token_embeds.37.proj.bias', 'token_embeds.38.proj.weight', 'token_embeds.38.proj.bias', 'token_embeds.39.proj.weight', 'token_embeds.39.proj.bias', 'token_embeds.40.proj.weight', 'token_embeds.40.proj.bias', 'token_embeds.41.proj.weight', 'token_embeds.41.proj.bias', 'token_embeds.42.proj.weight', 'token_embeds.42.proj.bias', 'token_embeds.43.proj.weight', 'token_embeds.43.proj.bias', 'token_embeds.44.proj.weight', 'token_embeds.44.proj.bias', 'token_embeds.45.proj.weight', 'token_embeds.45.proj.bias', 'token_embeds.46.proj.weight', 'token_embeds.46.proj.bias', 'token_embeds.47.proj.weight', 'token_embeds.47.proj.bias', 'token_embeds.48.proj.weight', 'token_embeds.48.proj.bias', 'token_embeds.49.proj.weight', 'token_embeds.49.proj.bias', 'token_embeds.50.proj.weight', 'token_embeds.50.proj.bias', 'token_embeds.51.proj.weight', 'token_embeds.51.proj.bias', 'token_embeds.52.proj.weight', 'token_embeds.52.proj.bias', 'token_embeds.53.proj.weight', 'token_embeds.53.proj.bias', 'token_embeds.54.proj.weight', 'token_embeds.54.proj.bias', 'token_embeds.55.proj.weight', 'token_embeds.55.proj.bias', 'token_embeds.56.proj.weight', 'token_embeds.56.proj.bias', 'token_embeds.57.proj.weight', 'token_embeds.57.proj.bias', 'token_embeds.58.proj.weight', 'token_embeds.58.proj.bias', 'token_embeds.59.proj.weight', 'token_embeds.59.proj.bias', 'token_embeds.60.proj.weight', 'token_embeds.60.proj.bias', 'token_embeds.61.proj.weight', 'token_embeds.61.proj.bias', 'token_embeds.62.proj.weight', 'token_embeds.62.proj.bias', 'token_embeds.63.proj.weight', 'token_embeds.63.proj.bias', 'token_embeds.64.proj.weight', 'token_embeds.64.proj.bias', 'token_embeds.65.proj.weight', 'token_embeds.65.proj.bias', 'token_embeds.66.proj.weight', 'token_embeds.66.proj.bias', 'token_embeds.67.proj.weight', 'token_embeds.67.proj.bias', 'token_embeds.68.proj.weight', 'token_embeds.68.proj.bias', 'token_embeds.69.proj.weight', 'token_embeds.69.proj.bias', 'token_embeds.70.proj.weight', 'token_embeds.70.proj.bias', 'token_embeds.71.proj.weight', 'token_embeds.71.proj.bias', 'token_embeds.72.proj.weight', 'token_embeds.72.proj.bias', 'token_embeds.73.proj.weight', 'token_embeds.73.proj.bias', 'token_embeds.74.proj.weight', 'token_embeds.74.proj.bias', 'token_embeds.75.proj.weight', 'token_embeds.75.proj.bias', 'token_embeds.76.proj.weight', 'token_embeds.76.proj.bias', 'token_embeds.77.proj.weight', 'token_embeds.77.proj.bias', 'token_embeds.78.proj.weight', 'token_embeds.78.proj.bias', 'token_embeds.79.proj.weight', 'token_embeds.79.proj.bias', 'token_embeds.80.proj.weight', 'token_embeds.80.proj.bias', 'token_embeds.81.proj.weight', 'token_embeds.81.proj.bias', 'token_embeds.82.proj.weight', 'token_embeds.82.proj.bias', 'token_embeds.83.proj.weight', 'token_embeds.83.proj.bias', 'token_embeds.84.proj.weight', 'token_embeds.84.proj.bias', 'token_embeds.85.proj.weight', 'token_embeds.85.proj.bias', 'token_embeds.86.proj.weight', 'token_embeds.86.proj.bias', 'token_embeds.87.proj.weight', 'token_embeds.87.proj.bias', 'token_embeds.88.proj.weight', 'token_embeds.88.proj.bias', 'token_embeds.89.proj.weight', 'token_embeds.89.proj.bias', 'token_embeds.90.proj.weight', 'token_embeds.90.proj.bias', 'token_embeds.91.proj.weight', 'token_embeds.91.proj.bias', 'token_embeds.92.proj.weight', 'token_embeds.92.proj.bias', 'token_embeds.93.proj.weight', 'token_embeds.93.proj.bias', 'token_embeds.94.proj.weight', 'token_embeds.94.proj.bias', 'token_embeds.95.proj.weight', 'token_embeds.95.proj.bias', 'token_embeds.96.proj.weight', 'token_embeds.96.proj.bias', 'token_embeds.97.proj.weight', 'token_embeds.97.proj.bias', 'token_embeds.98.proj.weight', 'token_embeds.98.proj.bias', 'token_embeds.99.proj.weight', 'token_embeds.99.proj.bias', 'token_embeds.100.proj.weight', 'token_embeds.100.proj.bias', 'token_embeds.101.proj.weight', 'token_embeds.101.proj.bias', 'token_embeds.102.proj.weight', 'token_embeds.102.proj.bias', 'token_embeds.103.proj.weight', 'token_embeds.103.proj.bias', 'token_embeds.104.proj.weight', 'token_embeds.104.proj.bias', 'token_embeds.105.proj.weight', 'token_embeds.105.proj.bias', 'token_embeds.106.proj.weight', 'token_embeds.106.proj.bias', 'token_embeds.107.proj.weight', 'token_embeds.107.proj.bias', 'token_embeds.108.proj.weight', 'token_embeds.108.proj.bias', 'token_embeds.109.proj.weight', 'token_embeds.109.proj.bias', 'token_embeds.110.proj.weight', 'token_embeds.110.proj.bias', 'token_embeds.111.proj.weight', 'token_embeds.111.proj.bias', 'token_embeds.112.proj.weight', 'token_embeds.112.proj.bias', 'token_embeds.113.proj.weight', 'token_embeds.113.proj.bias', 'token_embeds.114.proj.weight', 'token_embeds.114.proj.bias', 'token_embeds.115.proj.weight', 'token_embeds.115.proj.bias', 'token_embeds.116.proj.weight', 'token_embeds.116.proj.bias', 'token_embeds.117.proj.weight', 'token_embeds.117.proj.bias', 'token_embeds.118.proj.weight', 'token_embeds.118.proj.bias', 'token_embeds.119.proj.weight', 'token_embeds.119.proj.bias', 'token_embeds.120.proj.weight', 'token_embeds.120.proj.bias', 'token_embeds.121.proj.weight', 'token_embeds.121.proj.bias', 'token_embeds.122.proj.weight', 'token_embeds.122.proj.bias', 'token_embeds.123.proj.weight', 'token_embeds.123.proj.bias', 'token_embeds.124.proj.weight', 'token_embeds.124.proj.bias', 'token_embeds.125.proj.weight', 'token_embeds.125.proj.bias', 'token_embeds.126.proj.weight', 'token_embeds.126.proj.bias', 'token_embeds.127.proj.weight', 'token_embeds.127.proj.bias', 'token_embeds.128.proj.weight', 'token_embeds.128.proj.bias', 'token_embeds.129.proj.weight', 'token_embeds.129.proj.bias', 'token_embeds.130.proj.weight', 'token_embeds.130.proj.bias', 'token_embeds.131.proj.weight', 'token_embeds.131.proj.bias', 'token_embeds.132.proj.weight', 'token_embeds.132.proj.bias', 'token_embeds.133.proj.weight', 'token_embeds.133.proj.bias', 'token_embeds.134.proj.weight', 'token_embeds.134.proj.bias', 'token_embeds.135.proj.weight', 'token_embeds.135.proj.bias', 'token_embeds.136.proj.weight', 'token_embeds.136.proj.bias', 'token_embeds.137.proj.weight', 'token_embeds.137.proj.bias', 'token_embeds.138.proj.weight', 'token_embeds.138.proj.bias', 'token_embeds.139.proj.weight', 'token_embeds.139.proj.bias', 'token_embeds.140.proj.weight', 'token_embeds.140.proj.bias', 'token_embeds.141.proj.weight', 'token_embeds.141.proj.bias', 'token_embeds.142.proj.weight', 'token_embeds.142.proj.bias', 'token_embeds.143.proj.weight', 'token_embeds.143.proj.bias', 'token_embeds.144.proj.weight', 'token_embeds.144.proj.bias', 'token_embeds.145.proj.weight', 'token_embeds.145.proj.bias', 'token_embeds.146.proj.weight', 'token_embeds.146.proj.bias', 'token_embeds.147.proj.weight', 'token_embeds.147.proj.bias', 'token_embeds.148.proj.weight', 'token_embeds.148.proj.bias', 'token_embeds.149.proj.weight', 'token_embeds.149.proj.bias', 'token_embeds.150.proj.weight', 'token_embeds.150.proj.bias', 'token_embeds.151.proj.weight', 'token_embeds.151.proj.bias', 'token_embeds.152.proj.weight', 'token_embeds.152.proj.bias', 'token_embeds.153.proj.weight', 'token_embeds.153.proj.bias', 'token_embeds.154.proj.weight', 'token_embeds.154.proj.bias', 'token_embeds.155.proj.weight', 'token_embeds.155.proj.bias', 'token_embeds.156.proj.weight', 'token_embeds.156.proj.bias', 'token_embeds.157.proj.weight', 'token_embeds.157.proj.bias', 'token_embeds.158.proj.weight', 'token_embeds.158.proj.bias', 'token_embeds.159.proj.weight', 'token_embeds.159.proj.bias', 'token_embeds.160.proj.weight', 'token_embeds.160.proj.bias', 'token_embeds.161.proj.weight', 'token_embeds.161.proj.bias', 'token_embeds.162.proj.weight', 'token_embeds.162.proj.bias', 'token_embeds.163.proj.weight', 'token_embeds.163.proj.bias', 'token_embeds.164.proj.weight', 'token_embeds.164.proj.bias', 'token_embeds.165.proj.weight', 'token_embeds.165.proj.bias', 'token_embeds.166.proj.weight', 'token_embeds.166.proj.bias', 'token_embeds.167.proj.weight', 'token_embeds.167.proj.bias', 'token_embeds.168.proj.weight', 'token_embeds.168.proj.bias', 'token_embeds.169.proj.weight', 'token_embeds.169.proj.bias', 'token_embeds.170.proj.weight', 'token_embeds.170.proj.bias', 'token_embeds.171.proj.weight', 'token_embeds.171.proj.bias', 'token_embeds.172.proj.weight', 'token_embeds.172.proj.bias', 'token_embeds.173.proj.weight', 'token_embeds.173.proj.bias', 'token_embeds.174.proj.weight', 'token_embeds.174.proj.bias', 'token_embeds.175.proj.weight', 'token_embeds.175.proj.bias', 'token_embeds.176.proj.weight', 'token_embeds.176.proj.bias', 'token_embeds.177.proj.weight', 'token_embeds.177.proj.bias', 'token_embeds.178.proj.weight', 'token_embeds.178.proj.bias', 'token_embeds.179.proj.weight', 'token_embeds.179.proj.bias', 'token_embeds.180.proj.weight', 'token_embeds.180.proj.bias', 'token_embeds.181.proj.weight', 'token_embeds.181.proj.bias', 'token_embeds.182.proj.weight', 'token_embeds.182.proj.bias', 'token_embeds.183.proj.weight', 'token_embeds.183.proj.bias', 'token_embeds.184.proj.weight', 'token_embeds.184.proj.bias', 'token_embeds.185.proj.weight', 'token_embeds.185.proj.bias', 'token_embeds.186.proj.weight', 'token_embeds.186.proj.bias', 'token_embeds.187.proj.weight', 'token_embeds.187.proj.bias', 'token_embeds.188.proj.weight', 'token_embeds.188.proj.bias', 'token_embeds.189.proj.weight', 'token_embeds.189.proj.bias', 'token_embeds.190.proj.weight', 'token_embeds.190.proj.bias', 'token_embeds.191.proj.weight', 'token_embeds.191.proj.bias', 'token_embeds.192.proj.weight', 'token_embeds.192.proj.bias', 'token_embeds.193.proj.weight', 'token_embeds.193.proj.bias', 'token_embeds.194.proj.weight', 'token_embeds.194.proj.bias', 'token_embeds.195.proj.weight', 'token_embeds.195.proj.bias', 'token_embeds.196.proj.weight', 'token_embeds.196.proj.bias', 'token_embeds.197.proj.weight', 'token_embeds.197.proj.bias', 'token_embeds.198.proj.weight', 'token_embeds.198.proj.bias', 'token_embeds.199.proj.weight', 'token_embeds.199.proj.bias', 'token_embeds.200.proj.weight', 'token_embeds.200.proj.bias', 'token_embeds.201.proj.weight', 'token_embeds.201.proj.bias', 'token_embeds.202.proj.weight', 'token_embeds.202.proj.bias', 'token_embeds.203.proj.weight', 'token_embeds.203.proj.bias', 'token_embeds.204.proj.weight', 'token_embeds.204.proj.bias', 'token_embeds.205.proj.weight', 'token_embeds.205.proj.bias', 'token_embeds.206.proj.weight', 'token_embeds.206.proj.bias', 'token_embeds.207.proj.weight', 'token_embeds.207.proj.bias', 'token_embeds.208.proj.weight', 'token_embeds.208.proj.bias', 'token_embeds.209.proj.weight', 'token_embeds.209.proj.bias', 'token_embeds.210.proj.weight', 'token_embeds.210.proj.bias', 'token_embeds.211.proj.weight', 'token_embeds.211.proj.bias', 'token_embeds.212.proj.weight', 'token_embeds.212.proj.bias', 'token_embeds.213.proj.weight', 'token_embeds.213.proj.bias', 'token_embeds.214.proj.weight', 'token_embeds.214.proj.bias', 'token_embeds.215.proj.weight', 'token_embeds.215.proj.bias', 'token_embeds.216.proj.weight', 'token_embeds.216.proj.bias', 'token_embeds.217.proj.weight', 'token_embeds.217.proj.bias', 'token_embeds.218.proj.weight', 'token_embeds.218.proj.bias', 'token_embeds.219.proj.weight', 'token_embeds.219.proj.bias', 'token_embeds.220.proj.weight', 'token_embeds.220.proj.bias', 'token_embeds.221.proj.weight', 'token_embeds.221.proj.bias', 'token_embeds.222.proj.weight', 'token_embeds.222.proj.bias', 'token_embeds.223.proj.weight', 'token_embeds.223.proj.bias', 'token_embeds.224.proj.weight', 'token_embeds.224.proj.bias', 'token_embeds.225.proj.weight', 'token_embeds.225.proj.bias', 'token_embeds.226.proj.weight', 'token_embeds.226.proj.bias', 'token_embeds.227.proj.weight', 'token_embeds.227.proj.bias', 'token_embeds.228.proj.weight', 'token_embeds.228.proj.bias', 'token_embeds.229.proj.weight', 'token_embeds.229.proj.bias', 'token_embeds.230.proj.weight', 'token_embeds.230.proj.bias', 'token_embeds.231.proj.weight', 'token_embeds.231.proj.bias', 'token_embeds.232.proj.weight', 'token_embeds.232.proj.bias', 'token_embeds.233.proj.weight', 'token_embeds.233.proj.bias', 'token_embeds.234.proj.weight', 'token_embeds.234.proj.bias', 'token_embeds.235.proj.weight', 'token_embeds.235.proj.bias', 'token_embeds.236.proj.weight', 'token_embeds.236.proj.bias', 'token_embeds.237.proj.weight', 'token_embeds.237.proj.bias', 'token_embeds.238.proj.weight', 'token_embeds.238.proj.bias', 'token_embeds.239.proj.weight', 'token_embeds.239.proj.bias', 'token_embeds.240.proj.weight', 'token_embeds.240.proj.bias', 'token_embeds.241.proj.weight', 'token_embeds.241.proj.bias', 'token_embeds.242.proj.weight', 'token_embeds.242.proj.bias', 'token_embeds.243.proj.weight', 'token_embeds.243.proj.bias', 'token_embeds.244.proj.weight', 'token_embeds.244.proj.bias', 'token_embeds.245.proj.weight', 'token_embeds.245.proj.bias', 'token_embeds.246.proj.weight', 'token_embeds.246.proj.bias', 'token_embeds.247.proj.weight', 'token_embeds.247.proj.bias', 'token_embeds.248.proj.weight', 'token_embeds.248.proj.bias', 'token_embeds.249.proj.weight', 'token_embeds.249.proj.bias', 'token_embeds.250.proj.weight', 'token_embeds.250.proj.bias', 'token_embeds.251.proj.weight', 'token_embeds.251.proj.bias', 'token_embeds.252.proj.weight', 'token_embeds.252.proj.bias', 'token_embeds.253.proj.weight', 'token_embeds.253.proj.bias', 'token_embeds.254.proj.weight', 'token_embeds.254.proj.bias', 'token_embeds.255.proj.weight', 'token_embeds.255.proj.bias', 'token_embeds.256.proj.weight', 'token_embeds.256.proj.bias', 'token_embeds.257.proj.weight', 'token_embeds.257.proj.bias', 'token_embeds.258.proj.weight', 'token_embeds.258.proj.bias', 'token_embeds.259.proj.weight', 'token_embeds.259.proj.bias', 'token_embeds.260.proj.weight', 'token_embeds.260.proj.bias', 'token_embeds.261.proj.weight', 'token_embeds.261.proj.bias', 'token_embeds.262.proj.weight', 'token_embeds.262.proj.bias', 'token_embeds.263.proj.weight', 'token_embeds.263.proj.bias', 'token_embeds.264.proj.weight', 'token_embeds.264.proj.bias', 'token_embeds.265.proj.weight', 'token_embeds.265.proj.bias', 'token_embeds.266.proj.weight', 'token_embeds.266.proj.bias', 'token_embeds.267.proj.weight', 'token_embeds.267.proj.bias', 'token_embeds.268.proj.weight', 'token_embeds.268.proj.bias', 'token_embeds.269.proj.weight', 'token_embeds.269.proj.bias', 'token_embeds.270.proj.weight', 'token_embeds.270.proj.bias', 'token_embeds.271.proj.weight', 'token_embeds.271.proj.bias', 'token_embeds.272.proj.weight', 'token_embeds.272.proj.bias', 'token_embeds.273.proj.weight', 'token_embeds.273.proj.bias', 'token_embeds.274.proj.weight', 'token_embeds.274.proj.bias', 'token_embeds.275.proj.weight', 'token_embeds.275.proj.bias', 'token_embeds.276.proj.weight', 'token_embeds.276.proj.bias', 'token_embeds.277.proj.weight', 'token_embeds.277.proj.bias', 'token_embeds.278.proj.weight', 'token_embeds.278.proj.bias', 'token_embeds.279.proj.weight', 'token_embeds.279.proj.bias', 'token_embeds.280.proj.weight', 'token_embeds.280.proj.bias', 'token_embeds.281.proj.weight', 'token_embeds.281.proj.bias', 'token_embeds.282.proj.weight', 'token_embeds.282.proj.bias', 'token_embeds.283.proj.weight', 'token_embeds.283.proj.bias', 'token_embeds.284.proj.weight', 'token_embeds.284.proj.bias', 'token_embeds.285.proj.weight', 'token_embeds.285.proj.bias', 'token_embeds.286.proj.weight', 'token_embeds.286.proj.bias', 'token_embeds.287.proj.weight', 'token_embeds.287.proj.bias', 'token_embeds.288.proj.weight', 'token_embeds.288.proj.bias', 'token_embeds.289.proj.weight', 'token_embeds.289.proj.bias', 'token_embeds.290.proj.weight', 'token_embeds.290.proj.bias', 'token_embeds.291.proj.weight', 'token_embeds.291.proj.bias', 'token_embeds.292.proj.weight', 'token_embeds.292.proj.bias', 'token_embeds.293.proj.weight', 'token_embeds.293.proj.bias', 'token_embeds.294.proj.weight', 'token_embeds.294.proj.bias', 'token_embeds.295.proj.weight', 'token_embeds.295.proj.bias', 'token_embeds.296.proj.weight', 'token_embeds.296.proj.bias', 'token_embeds.297.proj.weight', 'token_embeds.297.proj.bias', 'token_embeds.298.proj.weight', 'token_embeds.298.proj.bias', 'token_embeds.299.proj.weight', 'token_embeds.299.proj.bias', 'token_embeds.300.proj.weight', 'token_embeds.300.proj.bias', 'token_embeds.301.proj.weight', 'token_embeds.301.proj.bias', 'token_embeds.302.proj.weight', 'token_embeds.302.proj.bias', 'token_embeds.303.proj.weight', 'token_embeds.303.proj.bias', 'token_embeds.304.proj.weight', 'token_embeds.304.proj.bias', 'token_embeds.305.proj.weight', 'token_embeds.305.proj.bias', 'token_embeds.306.proj.weight', 'token_embeds.306.proj.bias', 'token_embeds.307.proj.weight', 'token_embeds.307.proj.bias', 'token_embeds.308.proj.weight', 'token_embeds.308.proj.bias', 'token_embeds.309.proj.weight', 'token_embeds.309.proj.bias', 'token_embeds.310.proj.weight', 'token_embeds.310.proj.bias', 'token_embeds.311.proj.weight', 'token_embeds.311.proj.bias', 'token_embeds.312.proj.weight', 'token_embeds.312.proj.bias', 'token_embeds.313.proj.weight', 'token_embeds.313.proj.bias', 'token_embeds.314.proj.weight', 'token_embeds.314.proj.bias', 'token_embeds.315.proj.weight', 'token_embeds.315.proj.bias', 'token_embeds.316.proj.weight', 'token_embeds.316.proj.bias', 'token_embeds.317.proj.weight', 'token_embeds.317.proj.bias', 'token_embeds.318.proj.weight', 'token_embeds.318.proj.bias', 'token_embeds.319.proj.weight', 'token_embeds.319.proj.bias', 'token_embeds.320.proj.weight', 'token_embeds.320.proj.bias', 'token_embeds.321.proj.weight', 'token_embeds.321.proj.bias', 'token_embeds.322.proj.weight', 'token_embeds.322.proj.bias', 'token_embeds.323.proj.weight', 'token_embeds.323.proj.bias', 'token_embeds.324.proj.weight', 'token_embeds.324.proj.bias', 'token_embeds.325.proj.weight', 'token_embeds.325.proj.bias', 'token_embeds.326.proj.weight', 'token_embeds.326.proj.bias', 'token_embeds.327.proj.weight', 'token_embeds.327.proj.bias', 'token_embeds.328.proj.weight', 'token_embeds.328.proj.bias', 'token_embeds.329.proj.weight', 'token_embeds.329.proj.bias', 'token_embeds.330.proj.weight', 'token_embeds.330.proj.bias', 'token_embeds.331.proj.weight', 'token_embeds.331.proj.bias', 'token_embeds.332.proj.weight', 'token_embeds.332.proj.bias', 'token_embeds.333.proj.weight', 'token_embeds.333.proj.bias', 'token_embeds.334.proj.weight', 'token_embeds.334.proj.bias', 'token_embeds.335.proj.weight', 'token_embeds.335.proj.bias', 'token_embeds.336.proj.weight', 'token_embeds.336.proj.bias', 'token_embeds.337.proj.weight', 'token_embeds.337.proj.bias', 'token_embeds.338.proj.weight', 'token_embeds.338.proj.bias', 'token_embeds.339.proj.weight', 'token_embeds.339.proj.bias', 'token_embeds.340.proj.weight', 'token_embeds.340.proj.bias', 'token_embeds.341.proj.weight', 'token_embeds.341.proj.bias', 'token_embeds.342.proj.weight', 'token_embeds.342.proj.bias', 'token_embeds.343.proj.weight', 'token_embeds.343.proj.bias', 'token_embeds.344.proj.weight', 'token_embeds.344.proj.bias', 'token_embeds.345.proj.weight', 'token_embeds.345.proj.bias', 'token_embeds.346.proj.weight', 'token_embeds.346.proj.bias', 'token_embeds.347.proj.weight', 'token_embeds.347.proj.bias', 'token_embeds.348.proj.weight', 'token_embeds.348.proj.bias', 'token_embeds.349.proj.weight', 'token_embeds.349.proj.bias', 'token_embeds.350.proj.weight', 'token_embeds.350.proj.bias', 'token_embeds.351.proj.weight', 'token_embeds.351.proj.bias', 'token_embeds.352.proj.weight', 'token_embeds.352.proj.bias', 'token_embeds.353.proj.weight', 'token_embeds.353.proj.bias', 'token_embeds.354.proj.weight', 'token_embeds.354.proj.bias', 'token_embeds.355.proj.weight', 'token_embeds.355.proj.bias', 'token_embeds.356.proj.weight', 'token_embeds.356.proj.bias', 'token_embeds.357.proj.weight', 'token_embeds.357.proj.bias', 'token_embeds.358.proj.weight', 'token_embeds.358.proj.bias', 'token_embeds.359.proj.weight', 'token_embeds.359.proj.bias', 'token_embeds.360.proj.weight', 'token_embeds.360.proj.bias', 'token_embeds.361.proj.weight', 'token_embeds.361.proj.bias', 'token_embeds.362.proj.weight', 'token_embeds.362.proj.bias', 'token_embeds.363.proj.weight', 'token_embeds.363.proj.bias', 'token_embeds.364.proj.weight', 'token_embeds.364.proj.bias', 'token_embeds.365.proj.weight', 'token_embeds.365.proj.bias', 'token_embeds.366.proj.weight', 'token_embeds.366.proj.bias', 'token_embeds.367.proj.weight', 'token_embeds.367.proj.bias', 'token_embeds.368.proj.weight', 'token_embeds.368.proj.bias', 'token_embeds.369.proj.weight', 'token_embeds.369.proj.bias', 'token_embeds.370.proj.weight', 'token_embeds.370.proj.bias', 'token_embeds.371.proj.weight', 'token_embeds.371.proj.bias', 'token_embeds.372.proj.weight', 'token_embeds.372.proj.bias', 'token_embeds.373.proj.weight', 'token_embeds.373.proj.bias', 'token_embeds.374.proj.weight', 'token_embeds.374.proj.bias', 'token_embeds.375.proj.weight', 'token_embeds.375.proj.bias', 'token_embeds.376.proj.weight', 'token_embeds.376.proj.bias', 'token_embeds.377.proj.weight', 'token_embeds.377.proj.bias', 'token_embeds.378.proj.weight', 'token_embeds.378.proj.bias', 'token_embeds.379.proj.weight', 'token_embeds.379.proj.bias', 'token_embeds.380.proj.weight', 'token_embeds.380.proj.bias', 'token_embeds.381.proj.weight', 'token_embeds.381.proj.bias', 'token_embeds.382.proj.weight', 'token_embeds.382.proj.bias', 'token_embeds.383.proj.weight', 'token_embeds.383.proj.bias', 'token_embeds.384.proj.weight', 'token_embeds.384.proj.bias', 'token_embeds.385.proj.weight', 'token_embeds.385.proj.bias', 'token_embeds.386.proj.weight', 'token_embeds.386.proj.bias', 'token_embeds.387.proj.weight', 'token_embeds.387.proj.bias', 'token_embeds.388.proj.weight', 'token_embeds.388.proj.bias', 'token_embeds.389.proj.weight', 'token_embeds.389.proj.bias', 'token_embeds.390.proj.weight', 'token_embeds.390.proj.bias', 'token_embeds.391.proj.weight', 'token_embeds.391.proj.bias', 'token_embeds.392.proj.weight', 'token_embeds.392.proj.bias', 'token_embeds.393.proj.weight', 'token_embeds.393.proj.bias', 'token_embeds.394.proj.weight', 'token_embeds.394.proj.bias', 'token_embeds.395.proj.weight', 'token_embeds.395.proj.bias', 'token_embeds.396.proj.weight', 'token_embeds.396.proj.bias', 'token_embeds.397.proj.weight', 'token_embeds.397.proj.bias', 'token_embeds.398.proj.weight', 'token_embeds.398.proj.bias', 'token_embeds.399.proj.weight', 'token_embeds.399.proj.bias', 'token_embeds.400.proj.weight', 'token_embeds.400.proj.bias', 'token_embeds.401.proj.weight', 'token_embeds.401.proj.bias', 'token_embeds.402.proj.weight', 'token_embeds.402.proj.bias', 'token_embeds.403.proj.weight', 'token_embeds.403.proj.bias', 'token_embeds.404.proj.weight', 'token_embeds.404.proj.bias', 'token_embeds.405.proj.weight', 'token_embeds.405.proj.bias', 'token_embeds.406.proj.weight', 'token_embeds.406.proj.bias', 'token_embeds.407.proj.weight', 'token_embeds.407.proj.bias', 'token_embeds.408.proj.weight', 'token_embeds.408.proj.bias', 'token_embeds.409.proj.weight', 'token_embeds.409.proj.bias', 'token_embeds.410.proj.weight', 'token_embeds.410.proj.bias', 'token_embeds.411.proj.weight', 'token_embeds.411.proj.bias', 'token_embeds.412.proj.weight', 'token_embeds.412.proj.bias', 'token_embeds.413.proj.weight', 'token_embeds.413.proj.bias', 'token_embeds.414.proj.weight', 'token_embeds.414.proj.bias', 'token_embeds.415.proj.weight', 'token_embeds.415.proj.bias', 'token_embeds.416.proj.weight', 'token_embeds.416.proj.bias', 'token_embeds.417.proj.weight', 'token_embeds.417.proj.bias', 'token_embeds.418.proj.weight', 'token_embeds.418.proj.bias', 'token_embeds.419.proj.weight', 'token_embeds.419.proj.bias', 'token_embeds.420.proj.weight', 'token_embeds.420.proj.bias', 'token_embeds.421.proj.weight', 'token_embeds.421.proj.bias', 'token_embeds.422.proj.weight', 'token_embeds.422.proj.bias', 'token_embeds.423.proj.weight', 'token_embeds.423.proj.bias', 'token_embeds.424.proj.weight', 'token_embeds.424.proj.bias', 'token_embeds.425.proj.weight', 'token_embeds.425.proj.bias', 'token_embeds.426.proj.weight', 'token_embeds.426.proj.bias', 'token_embeds.427.proj.weight', 'token_embeds.427.proj.bias', 'token_embeds.428.proj.weight', 'token_embeds.428.proj.bias', 'token_embeds.429.proj.weight', 'token_embeds.429.proj.bias', 'token_embeds.430.proj.weight', 'token_embeds.430.proj.bias', 'token_embeds.431.proj.weight', 'token_embeds.431.proj.bias', 'token_embeds.432.proj.weight', 'token_embeds.432.proj.bias', 'token_embeds.433.proj.weight', 'token_embeds.433.proj.bias', 'token_embeds.434.proj.weight', 'token_embeds.434.proj.bias', 'token_embeds.435.proj.weight', 'token_embeds.435.proj.bias', 'token_embeds.436.proj.weight', 'token_embeds.436.proj.bias', 'token_embeds.437.proj.weight', 'token_embeds.437.proj.bias', 'token_embeds.438.proj.weight', 'token_embeds.438.proj.bias', 'token_embeds.439.proj.weight', 'token_embeds.439.proj.bias', 'token_embeds.440.proj.weight', 'token_embeds.440.proj.bias', 'token_embeds.441.proj.weight', 'token_embeds.441.proj.bias', 'token_embeds.442.proj.weight', 'token_embeds.442.proj.bias', 'token_embeds.443.proj.weight', 'token_embeds.443.proj.bias', 'token_embeds.444.proj.weight', 'token_embeds.444.proj.bias', 'token_embeds.445.proj.weight', 'token_embeds.445.proj.bias', 'token_embeds.446.proj.weight', 'token_embeds.446.proj.bias', 'token_embeds.447.proj.weight', 'token_embeds.447.proj.bias', 'token_embeds.448.proj.weight', 'token_embeds.448.proj.bias', 'token_embeds.449.proj.weight', 'token_embeds.449.proj.bias', 'token_embeds.450.proj.weight', 'token_embeds.450.proj.bias', 'token_embeds.451.proj.weight', 'token_embeds.451.proj.bias', 'token_embeds.452.proj.weight', 'token_embeds.452.proj.bias', 'token_embeds.453.proj.weight', 'token_embeds.453.proj.bias', 'token_embeds.454.proj.weight', 'token_embeds.454.proj.bias', 'token_embeds.455.proj.weight', 'token_embeds.455.proj.bias', 'token_embeds.456.proj.weight', 'token_embeds.456.proj.bias', 'token_embeds.457.proj.weight', 'token_embeds.457.proj.bias', 'token_embeds.458.proj.weight', 'token_embeds.458.proj.bias', 'token_embeds.459.proj.weight', 'token_embeds.459.proj.bias', 'token_embeds.460.proj.weight', 'token_embeds.460.proj.bias', 'token_embeds.461.proj.weight', 'token_embeds.461.proj.bias', 'token_embeds.462.proj.weight', 'token_embeds.462.proj.bias', 'token_embeds.463.proj.weight', 'token_embeds.463.proj.bias', 'token_embeds.464.proj.weight', 'token_embeds.464.proj.bias', 'token_embeds.465.proj.weight', 'token_embeds.465.proj.bias', 'token_embeds.466.proj.weight', 'token_embeds.466.proj.bias', 'token_embeds.467.proj.weight', 'token_embeds.467.proj.bias', 'token_embeds.468.proj.weight', 'token_embeds.468.proj.bias', 'token_embeds.469.proj.weight', 'token_embeds.469.proj.bias', 'token_embeds.470.proj.weight', 'token_embeds.470.proj.bias', 'token_embeds.471.proj.weight', 'token_embeds.471.proj.bias', 'token_embeds.472.proj.weight', 'token_embeds.472.proj.bias', 'token_embeds.473.proj.weight', 'token_embeds.473.proj.bias', 'token_embeds.474.proj.weight', 'token_embeds.474.proj.bias', 'token_embeds.475.proj.weight', 'token_embeds.475.proj.bias', 'token_embeds.476.proj.weight', 'token_embeds.476.proj.bias', 'token_embeds.477.proj.weight', 'token_embeds.477.proj.bias', 'token_embeds.478.proj.weight', 'token_embeds.478.proj.bias', 'token_embeds.479.proj.weight', 'token_embeds.479.proj.bias', 'token_embeds.480.proj.weight', 'token_embeds.480.proj.bias', 'token_embeds.481.proj.weight', 'token_embeds.481.proj.bias', 'token_embeds.482.proj.weight', 'token_embeds.482.proj.bias', 'token_embeds.483.proj.weight', 'token_embeds.483.proj.bias', 'token_embeds.484.proj.weight', 'token_embeds.484.proj.bias', 'token_embeds.485.proj.weight', 'token_embeds.485.proj.bias', 'token_embeds.486.proj.weight', 'token_embeds.486.proj.bias', 'token_embeds.487.proj.weight', 'token_embeds.487.proj.bias', 'token_embeds.488.proj.weight', 'token_embeds.488.proj.bias', 'token_embeds.489.proj.weight', 'token_embeds.489.proj.bias', 'token_embeds.490.proj.weight', 'token_embeds.490.proj.bias', 'token_embeds.491.proj.weight', 'token_embeds.491.proj.bias', 'token_embeds.492.proj.weight', 'token_embeds.492.proj.bias', 'token_embeds.493.proj.weight', 'token_embeds.493.proj.bias', 'token_embeds.494.proj.weight', 'token_embeds.494.proj.bias', 'token_embeds.495.proj.weight', 'token_embeds.495.proj.bias', 'lead_time_embed.weight', 'lead_time_embed.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.12.norm1.weight', 'blocks.12.norm1.bias', 'blocks.12.norm2.weight', 'blocks.12.norm2.bias', 'blocks.13.norm1.weight', 'blocks.13.norm1.bias', 'blocks.13.norm2.weight', 'blocks.13.norm2.bias', 'blocks.14.norm1.weight', 'blocks.14.norm1.bias', 'blocks.14.norm2.weight', 'blocks.14.norm2.bias', 'blocks.15.norm1.weight', 'blocks.15.norm1.bias', 'blocks.15.norm2.weight', 'blocks.15.norm2.bias', 'blocks.16.norm1.weight', 'blocks.16.norm1.bias', 'blocks.16.norm2.weight', 'blocks.16.norm2.bias', 'blocks.17.norm1.weight', 'blocks.17.norm1.bias', 'blocks.17.norm2.weight', 'blocks.17.norm2.bias', 'blocks.18.norm1.weight', 'blocks.18.norm1.bias', 'blocks.18.norm2.weight', 'blocks.18.norm2.bias', 'blocks.19.norm1.weight', 'blocks.19.norm1.bias', 'blocks.19.norm2.weight', 'blocks.19.norm2.bias', 'blocks.20.norm1.weight', 'blocks.20.norm1.bias', 'blocks.20.norm2.weight', 'blocks.20.norm2.bias', 'blocks.21.norm1.weight', 'blocks.21.norm1.bias', 'blocks.21.norm2.weight', 'blocks.21.norm2.bias', 'blocks.22.norm1.weight', 'blocks.22.norm1.bias', 'blocks.22.norm2.weight', 'blocks.22.norm2.bias', 'blocks.23.norm1.weight', 'blocks.23.norm1.bias', 'blocks.23.norm2.weight', 'blocks.23.norm2.bias', 'blocks.24.norm1.weight', 'blocks.24.norm1.bias', 'blocks.24.norm2.weight', 'blocks.24.norm2.bias', 'blocks.25.norm1.weight', 'blocks.25.norm1.bias', 'blocks.25.norm2.weight', 'blocks.25.norm2.bias', 'blocks.26.norm1.weight', 'blocks.26.norm1.bias', 'blocks.26.norm2.weight', 'blocks.26.norm2.bias', 'blocks.27.norm1.weight', 'blocks.27.norm1.bias', 'blocks.27.norm2.weight', 'blocks.27.norm2.bias', 'blocks.28.norm1.weight', 'blocks.28.norm1.bias', 'blocks.28.norm2.weight', 'blocks.28.norm2.bias', 'blocks.29.norm1.weight', 'blocks.29.norm1.bias', 'blocks.29.norm2.weight', 'blocks.29.norm2.bias', 'blocks.30.norm1.weight', 'blocks.30.norm1.bias', 'blocks.30.norm2.weight', 'blocks.30.norm2.bias', 'blocks.31.norm1.weight', 'blocks.31.norm1.bias', 'blocks.31.norm2.weight', 'blocks.31.norm2.bias', 'norm.weight', 'norm.bias', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias'])
rank 9 After initialize model
rank 9 before dist.barrier(group)
rank 8 After initialize model
rank 8 before dist.barrier(group)
rank 17 After initialize model
rank 17 before dist.barrier(group)
rank 16 After initialize model
rank 16 before dist.barrier(group)
rank 11 After initialize model
rank 11 before dist.barrier(group)
rank 10 After initialize model
rank 10 before dist.barrier(group)
rank 12 After initialize model
rank 12 before dist.barrier(group)
rank 19 After initialize model
rank 19 before dist.barrier(group)
rank 18 After initialize model
rank 18 before dist.barrier(group)
rank 14 After initialize model
rank 14 before dist.barrier(group)
rank 13 After initialize model
rank 13 before dist.barrier(group)
rank 15 After initialize model
rank 15 before dist.barrier(group)
rank 20 After initialize model
rank 20 before dist.barrier(group)
rank 22 After initialize model
rank 22 before dist.barrier(group)
rank 23 After initialize model
rank 23 before dist.barrier(group)
rank 21 After initialize model
rank 21 before dist.barrier(group)
rank 0 after torch.save for initial
rank 0 before dist.barrier(group)
rank 16 after dist.barrier(group)
rank 16 Before the second dist.barrier()
rank 24 after dist.barrier(group)
rank 24 Before the second dist.barrier()
rank 17 after dist.barrier(group)
rank 27 after dist.barrier(group)
rank 27 Before the second dist.barrier()
rank 17 Before the second dist.barrier()
rank 28 after dist.barrier(group)
rank 28 Before the second dist.barrier()
rank 29 after dist.barrier(group)
rank 29 Before the second dist.barrier()
rank 30 after dist.barrier(group)
rank 30 Before the second dist.barrier()
rank 31 after dist.barrier(group)
rank 31 Before the second dist.barrier()
rank 25 after dist.barrier(group)
rank 0 after dist.barrier(group)
rank 0 Before the second dist.barrier()
rank 25 Before the second dist.barrier()
rank 1 after dist.barrier(group)
rank 1 src_rank 0
rank 26 after dist.barrier(group)
rank 5 after dist.barrier(group)
rank 5 src_rank 0
rank 26 Before the second dist.barrier()
rank 21 after dist.barrier(group)
rank 21 Before the second dist.barrier()
rank 4 after dist.barrier(group)
rank 4 src_rank 0
rank 8 after dist.barrier(group)
rank 8 Before the second dist.barrier()
rank 9 after dist.barrier(group)
rank 9 Before the second dist.barrier()
rank 13 after dist.barrier(group)
rank 13 Before the second dist.barrier()
rank 12 after dist.barrier(group)
rank 20 after dist.barrier(group)
rank 20 Before the second dist.barrier()
rank 12 Before the second dist.barrier()
rank 6 after dist.barrier(group)
rank 6 src_rank 0
rank 22 after dist.barrier(group)
rank 22 Before the second dist.barrier()
rank 23 after dist.barrier(group)
rank 23 Before the second dist.barrier()
rank 7 after dist.barrier(group)
rank 7 src_rank 0
rank 3 after dist.barrier(group)
rank 3 src_rank 0
rank 14 after dist.barrier(group)
rank 14 Before the second dist.barrier()
rank 15 after dist.barrier(group)
rank 15 Before the second dist.barrier()
rank 18 after dist.barrier(group)
rank 18 Before the second dist.barrier()
rank 2 after dist.barrier(group)
rank 2 src_rank 0
rank 19 after dist.barrier(group)
rank 19 Before the second dist.barrier()
rank 10 after dist.barrier(group)
rank 10 Before the second dist.barrier()
rank 11 after dist.barrier(group)
rank 11 Before the second dist.barrier()
rank 1 Before the second dist.barrier()
rank 3 Before the second dist.barrier()
rank 2 Before the second dist.barrier()
rank 4 Before the second dist.barrier()
rank 6 Before the second dist.barrier()
rank 7 Before the second dist.barrier()
rank 5 Before the second dist.barrier()
rank 24 After the second dist.barrier()
rank 25 After the second dist.barrier()
rank 28 After the second dist.barrier()
rank 29 After the second dist.barrier()
rank 8 After the second dist.barrier()
rank 9 After the second dist.barrier()
rank 12 After the second dist.barrier()
rank 13 After the second dist.barrier()
rank 0 After the second dist.barrier()
rank 1 After the second dist.barrier()
rank 26 After the second dist.barrier()
parameter name  var_embed  requires_gradient  True size torch.Size([1, 496, 4096])
rank 27 After the second dist.barrier()
rank 30 After the second dist.barrier()
rank 31 After the second dist.barrier()
rank 16 After the second dist.barrier()
rank 10 After the second dist.barrier()
rank 17 After the second dist.barrier()
rank 11 After the second dist.barrier()
rank 18 After the second dist.barrier()
rank 15 After the second dist.barrier()
rank 19 After the second dist.barrier()
rank 22 After the second dist.barrier()
rank 23 After the second dist.barrier()
rank 6 After the second dist.barrier()
rank 21 After the second dist.barrier()
rank 14 After the second dist.barrier()
rank 7 After the second dist.barrier()
rank 4 After the second dist.barrier()
rank 5 After the second dist.barrier()
rank 2 After the second dist.barrier()
rank 3 After the second dist.barrier()
rank 20 After the second dist.barrier()
parameter name  var_query  requires_gradient  True size torch.Size([1, 1, 4096])
parameter name  pos_embed  requires_gradient  True size torch.Size([1, 2048, 4096])
parameter name  mask_token  requires_gradient  True size torch.Size([1, 1, 128])
parameter name  decoder_pos_embed  requires_gradient  True size torch.Size([1, 2048, 128])
parameter name  token_embeds.0.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.0.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.1.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.1.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.2.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.2.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.3.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.3.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.4.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.4.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.5.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.5.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.6.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.6.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.7.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.7.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.8.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.8.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.9.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.9.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.10.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.10.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.11.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.11.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.12.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.12.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.13.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.13.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.14.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.14.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.15.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.15.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.16.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.16.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.17.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.17.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.18.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.18.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.19.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.19.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.20.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.20.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.21.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.21.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.22.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.22.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.23.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.23.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.24.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.24.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.25.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.25.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.26.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.26.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.27.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.27.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.28.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.28.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.29.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.29.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.30.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.30.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.31.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.31.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.32.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.32.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.33.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.33.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.34.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.34.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.35.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.35.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.36.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.36.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.37.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.37.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.38.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.38.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.39.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.39.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.40.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.40.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.41.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.41.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.42.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.42.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.43.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.43.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.44.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.44.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.45.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.45.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.46.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.46.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.47.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.47.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.48.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.48.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.49.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.49.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.50.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.50.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.51.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.51.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.52.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.52.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.53.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.53.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.54.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.54.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.55.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.55.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.56.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.56.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.57.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.57.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.58.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.58.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.59.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.59.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.60.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.60.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.61.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.61.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.62.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.62.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.63.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.63.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.64.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.64.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.65.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.65.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.66.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.66.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.67.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.67.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.68.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.68.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.69.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.69.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.70.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.70.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.71.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.71.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.72.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.72.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.73.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.73.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.74.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.74.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.75.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.75.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.76.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.76.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.77.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.77.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.78.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.78.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.79.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.79.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.80.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.80.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.81.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.81.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.82.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.82.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.83.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.83.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.84.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.84.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.85.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.85.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.86.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.86.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.87.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.87.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.88.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.88.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.89.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.89.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.90.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.90.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.91.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.91.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.92.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.92.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.93.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.93.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.94.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.94.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.95.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.95.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.96.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.96.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.97.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.97.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.98.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.98.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.99.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.99.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.100.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.100.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.101.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.101.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.102.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.102.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.103.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.103.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.104.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.104.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.105.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.105.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.106.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.106.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.107.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.107.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.108.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.108.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.109.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.109.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.110.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.110.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.111.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.111.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.112.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.112.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.113.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.113.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.114.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.114.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.115.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.115.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.116.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.116.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.117.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.117.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.118.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.118.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.119.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.119.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.120.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.120.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.121.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.121.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.122.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.122.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.123.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.123.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.124.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.124.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.125.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.125.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.126.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.126.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.127.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.127.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.128.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.128.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.129.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.129.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.130.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.130.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.131.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.131.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.132.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.132.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.133.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.133.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.134.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.134.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.135.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.135.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.136.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.136.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.137.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.137.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.138.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.138.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.139.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.139.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.140.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.140.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.141.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.141.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.142.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.142.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.143.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.143.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.144.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.144.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.145.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.145.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.146.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.146.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.147.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.147.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.148.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.148.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.149.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.149.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.150.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.150.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.151.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.151.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.152.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.152.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.153.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.153.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.154.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.154.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.155.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.155.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.156.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.156.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.157.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.157.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.158.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.158.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.159.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.159.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.160.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.160.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.161.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.161.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.162.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.162.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.163.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.163.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.164.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.164.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.165.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.165.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.166.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.166.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.167.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.167.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.168.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.168.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.169.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.169.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.170.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.170.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.171.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.171.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.172.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.172.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.173.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.173.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.174.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.174.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.175.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.175.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.176.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.176.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.177.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.177.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.178.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.178.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.179.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.179.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.180.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.180.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.181.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.181.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.182.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.182.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.183.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.183.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.184.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.184.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.185.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.185.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.186.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.186.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.187.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.187.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.188.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.188.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.189.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.189.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.190.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.190.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.191.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.191.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.192.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.192.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.193.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.193.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.194.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.194.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.195.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.195.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.196.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.196.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.197.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.197.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.198.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.198.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.199.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.199.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.200.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.200.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.201.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.201.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.202.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.202.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.203.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.203.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.204.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.204.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.205.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.205.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.206.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.206.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.207.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.207.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.208.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.208.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.209.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.209.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.210.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.210.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.211.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.211.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.212.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.212.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.213.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.213.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.214.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.214.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.215.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.215.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.216.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.216.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.217.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.217.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.218.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.218.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.219.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.219.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.220.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.220.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.221.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.221.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.222.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.222.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.223.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.223.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.224.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.224.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.225.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.225.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.226.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.226.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.227.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.227.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.228.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.228.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.229.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.229.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.230.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.230.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.231.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.231.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.232.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.232.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.233.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.233.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.234.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.234.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.235.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.235.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.236.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.236.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.237.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.237.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.238.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.238.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.239.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.239.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.240.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.240.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.241.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.241.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.242.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.242.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.243.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.243.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.244.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.244.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.245.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.245.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.246.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.246.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.247.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.247.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.248.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.248.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.249.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.249.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.250.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.250.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.251.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.251.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.252.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.252.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.253.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.253.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.254.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.254.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.255.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.255.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.256.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.256.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.257.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.257.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.258.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.258.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.259.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.259.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.260.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.260.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.261.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.261.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.262.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.262.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.263.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.263.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.264.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.264.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.265.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.265.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.266.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.266.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.267.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.267.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.268.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.268.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.269.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.269.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.270.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.270.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.271.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.271.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.272.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.272.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.273.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.273.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.274.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.274.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.275.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.275.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.276.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.276.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.277.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.277.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.278.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.278.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.279.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.279.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.280.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.280.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.281.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.281.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.282.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.282.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.283.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.283.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.284.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.284.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.285.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.285.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.286.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.286.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.287.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.287.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.288.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.288.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.289.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.289.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.290.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.290.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.291.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.291.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.292.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.292.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.293.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.293.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.294.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.294.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.295.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.295.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.296.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.296.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.297.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.297.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.298.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.298.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.299.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.299.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.300.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.300.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.301.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.301.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.302.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.302.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.303.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.303.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.304.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.304.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.305.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.305.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.306.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.306.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.307.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.307.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.308.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.308.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.309.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.309.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.310.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.310.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.311.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.311.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.312.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.312.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.313.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.313.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.314.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.314.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.315.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.315.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.316.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.316.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.317.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.317.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.318.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.318.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.319.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.319.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.320.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.320.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.321.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.321.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.322.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.322.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.323.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.323.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.324.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.324.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.325.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.325.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.326.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.326.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.327.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.327.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.328.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.328.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.329.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.329.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.330.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.330.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.331.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.331.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.332.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.332.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.333.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.333.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.334.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.334.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.335.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.335.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.336.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.336.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.337.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.337.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.338.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.338.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.339.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.339.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.340.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.340.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.341.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.341.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.342.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.342.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.343.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.343.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.344.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.344.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.345.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.345.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.346.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.346.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.347.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.347.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.348.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.348.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.349.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.349.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.350.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.350.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.351.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.351.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.352.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.352.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.353.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.353.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.354.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.354.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.355.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.355.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.356.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.356.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.357.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.357.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.358.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.358.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.359.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.359.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.360.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.360.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.361.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.361.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.362.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.362.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.363.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.363.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.364.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.364.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.365.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.365.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.366.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.366.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.367.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.367.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.368.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.368.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.369.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.369.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.370.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.370.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.371.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.371.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.372.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.372.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.373.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.373.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.374.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.374.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.375.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.375.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.376.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.376.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.377.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.377.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.378.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.378.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.379.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.379.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.380.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.380.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.381.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.381.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.382.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.382.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.383.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.383.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.384.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.384.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.385.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.385.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.386.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.386.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.387.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.387.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.388.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.388.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.389.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.389.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.390.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.390.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.391.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.391.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.392.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.392.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.393.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.393.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.394.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.394.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.395.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.395.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.396.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.396.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.397.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.397.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.398.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.398.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.399.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.399.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.400.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.400.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.401.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.401.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.402.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.402.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.403.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.403.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.404.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.404.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.405.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.405.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.406.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.406.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.407.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.407.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.408.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.408.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.409.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.409.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.410.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.410.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.411.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.411.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.412.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.412.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.413.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.413.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.414.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.414.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.415.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.415.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.416.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.416.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.417.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.417.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.418.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.418.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.419.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.419.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.420.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.420.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.421.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.421.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.422.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.422.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.423.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.423.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.424.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.424.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.425.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.425.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.426.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.426.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.427.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.427.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.428.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.428.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.429.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.429.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.430.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.430.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.431.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.431.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.432.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.432.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.433.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.433.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.434.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.434.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.435.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.435.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.436.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.436.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.437.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.437.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.438.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.438.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.439.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.439.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.440.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.440.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.441.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.441.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.442.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.442.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.443.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.443.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.444.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.444.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.445.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.445.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.446.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.446.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.447.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.447.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.448.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.448.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.449.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.449.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.450.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.450.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.451.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.451.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.452.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.452.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.453.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.453.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.454.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.454.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.455.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.455.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.456.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.456.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.457.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.457.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.458.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.458.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.459.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.459.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.460.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.460.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.461.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.461.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.462.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.462.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.463.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.463.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.464.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.464.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.465.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.465.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.466.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.466.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.467.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.467.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.468.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.468.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.469.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.469.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.470.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.470.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.471.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.471.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.472.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.472.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.473.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.473.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.474.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.474.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.475.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.475.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.476.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.476.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.477.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.477.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.478.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.478.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.479.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.479.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.480.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.480.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.481.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.481.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.482.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.482.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.483.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.483.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.484.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.484.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.485.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.485.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.486.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.486.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.487.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.487.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.488.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.488.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.489.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.489.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.490.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.490.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.491.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.491.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.492.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.492.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.493.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.493.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.494.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.494.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.495.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.495.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  var_agg.q.weight  requires_gradient  True size torch.Size([512, 4096])
parameter name  var_agg.kv.weight  requires_gradient  True size torch.Size([1024, 4096])
parameter name  var_agg.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  var_agg.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  lead_time_embed.weight  requires_gradient  True size torch.Size([4096, 1])
parameter name  lead_time_embed.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.0.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.0.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.0.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.0.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.0.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.0.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.0.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.0.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.0.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.0.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.0.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.0.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.1.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.1.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.1.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.1.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.1.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.1.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.1.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.1.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.1.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.1.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.1.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.1.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.2.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.2.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.2.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.2.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.2.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.2.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.2.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.2.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.2.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.2.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.2.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.2.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.3.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.3.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.3.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.3.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.3.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.3.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.3.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.3.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.3.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.3.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.3.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.3.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.4.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.4.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.4.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.4.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.4.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.4.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.4.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.4.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.4.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.4.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.4.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.4.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.5.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.5.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.5.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.5.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.5.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.5.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.5.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.5.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.5.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.5.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.5.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.5.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.6.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.6.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.6.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.6.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.6.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.6.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.6.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.6.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.6.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.6.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.6.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.6.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.7.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.7.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.7.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.7.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.7.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.7.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.7.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.7.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.7.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.7.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.7.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.7.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.8.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.8.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.8.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.8.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.8.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.8.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.8.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.8.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.8.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.8.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.8.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.8.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.9.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.9.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.9.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.9.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.9.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.9.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.9.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.9.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.9.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.9.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.9.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.9.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.10.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.10.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.10.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.10.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.10.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.10.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.10.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.10.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.10.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.10.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.10.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.10.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.11.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.11.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.11.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.11.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.11.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.11.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.11.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.11.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.11.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.11.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.11.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.11.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.12.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.12.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.12.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.12.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.12.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.12.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.12.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.12.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.12.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.12.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.12.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.12.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.13.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.13.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.13.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.13.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.13.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.13.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.13.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.13.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.13.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.13.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.13.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.13.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.14.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.14.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.14.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.14.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.14.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.14.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.14.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.14.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.14.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.14.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.14.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.14.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.15.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.15.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.15.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.15.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.15.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.15.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.15.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.15.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.15.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.15.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.15.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.15.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.16.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.16.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.16.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.16.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.16.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.16.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.16.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.16.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.16.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.16.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.16.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.16.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.17.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.17.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.17.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.17.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.17.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.17.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.17.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.17.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.17.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.17.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.17.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.17.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.18.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.18.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.18.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.18.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.18.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.18.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.18.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.18.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.18.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.18.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.18.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.18.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.19.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.19.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.19.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.19.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.19.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.19.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.19.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.19.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.19.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.19.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.19.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.19.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.20.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.20.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.20.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.20.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.20.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.20.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.20.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.20.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.20.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.20.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.20.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.20.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.21.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.21.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.21.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.21.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.21.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.21.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.21.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.21.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.21.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.21.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.21.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.21.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.22.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.22.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.22.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.22.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.22.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.22.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.22.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.22.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.22.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.22.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.22.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.22.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.23.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.23.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.23.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.23.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.23.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.23.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.23.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.23.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.23.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.23.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.23.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.23.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.24.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.24.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.24.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.24.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.24.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.24.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.24.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.24.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.24.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.24.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.24.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.24.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.25.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.25.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.25.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.25.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.25.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.25.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.25.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.25.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.25.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.25.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.25.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.25.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.26.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.26.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.26.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.26.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.26.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.26.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.26.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.26.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.26.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.26.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.26.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.26.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.27.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.27.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.27.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.27.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.27.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.27.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.27.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.27.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.27.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.27.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.27.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.27.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.28.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.28.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.28.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.28.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.28.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.28.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.28.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.28.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.28.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.28.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.28.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.28.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.29.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.29.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.29.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.29.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.29.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.29.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.29.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.29.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.29.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.29.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.29.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.29.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.30.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.30.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.30.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.30.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.30.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.30.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.30.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.30.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.30.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.30.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.30.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.30.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.31.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.31.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.31.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.31.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.31.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.31.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.31.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.31.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.31.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.31.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.31.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.31.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  norm.weight  requires_gradient  True size torch.Size([4096])
parameter name  norm.bias  requires_gradient  True size torch.Size([4096])
parameter name  decoder_embed.weight  requires_gradient  True size torch.Size([128, 4096])
parameter name  decoder_embed.bias  requires_gradient  True size torch.Size([128])
parameter name  decoder_blocks.0.norm1.weight  requires_gradient  True size torch.Size([128])
parameter name  decoder_blocks.0.norm1.bias  requires_gradient  True size torch.Size([128])
parameter name  decoder_blocks.0.attn.qkv.weight  requires_gradient  True size torch.Size([384, 128])
parameter name  decoder_blocks.0.attn.qkv.bias  requires_gradient  True size torch.Size([384])
parameter name  decoder_blocks.0.attn.proj.weight  requires_gradient  True size torch.Size([128, 128])
parameter name  decoder_blocks.0.attn.proj.bias  requires_gradient  True size torch.Size([128])
parameter name  decoder_blocks.0.norm2.weight  requires_gradient  True size torch.Size([128])
parameter name  decoder_blocks.0.norm2.bias  requires_gradient  True size torch.Size([128])
parameter name  decoder_blocks.0.mlp.fc1.weight  requires_gradient  True size torch.Size([512, 128])
parameter name  decoder_blocks.0.mlp.fc1.bias  requires_gradient  True size torch.Size([512])
parameter name  decoder_blocks.0.mlp.fc2.weight  requires_gradient  True size torch.Size([128, 512])
parameter name  decoder_blocks.0.mlp.fc2.bias  requires_gradient  True size torch.Size([128])
parameter name  decoder_blocks.1.norm1.weight  requires_gradient  True size torch.Size([128])
parameter name  decoder_blocks.1.norm1.bias  requires_gradient  True size torch.Size([128])
parameter name  decoder_blocks.1.attn.qkv.weight  requires_gradient  True size torch.Size([384, 128])
parameter name  decoder_blocks.1.attn.qkv.bias  requires_gradient  True size torch.Size([384])
parameter name  decoder_blocks.1.attn.proj.weight  requires_gradient  True size torch.Size([128, 128])
parameter name  decoder_blocks.1.attn.proj.bias  requires_gradient  True size torch.Size([128])
parameter name  decoder_blocks.1.norm2.weight  requires_gradient  True size torch.Size([128])
parameter name  decoder_blocks.1.norm2.bias  requires_gradient  True size torch.Size([128])
parameter name  decoder_blocks.1.mlp.fc1.weight  requires_gradient  True size torch.Size([512, 128])
parameter name  decoder_blocks.1.mlp.fc1.bias  requires_gradient  True size torch.Size([512])
parameter name  decoder_blocks.1.mlp.fc2.weight  requires_gradient  True size torch.Size([128, 512])
parameter name  decoder_blocks.1.mlp.fc2.bias  requires_gradient  True size torch.Size([128])
parameter name  decoder_norm.weight  requires_gradient  True size torch.Size([128])
parameter name  decoder_norm.bias  requires_gradient  True size torch.Size([128])
parameter name  decoder_pred.weight  requires_gradient  True size torch.Size([17856, 128])
parameter name  decoder_pred.bias  requires_gradient  True size torch.Size([17856])
total_params before FSDP tensor(6604997568) params_per_gpu tensor(903697600)
total_params after FSDP FullyShardedDataParallel(
  (_fsdp_wrapped_module): ClimaX(
    (token_embeds): ModuleList(
      (0-495): 496 x PatchEmbed(
        (proj): Conv2d(1, 4096, kernel_size=(6, 6), stride=(6, 6))
        (norm): Identity()
      )
    )
    (var_agg): FullyShardedDataParallel(
      (_fsdp_wrapped_module): CheckpointWrapper(
        (_checkpoint_wrapped_module): VariableMapping_Attention(
          (q): Linear(in_features=4096, out_features=512, bias=False)
          (kv): Linear(in_features=4096, out_features=1024, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=4096, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (lead_time_embed): Linear(in_features=1, out_features=4096, bias=True)
    (pos_drop): Dropout(p=0.1, inplace=False)
    (blocks): ModuleList(
      (0-31): 32 x FullyShardedDataParallel(
        (_fsdp_wrapped_module): CheckpointWrapper(
          (_checkpoint_wrapped_module): Block(
            (norm1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=4096, out_features=1536, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=512, out_features=4096, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (norm2): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=4096, out_features=2048, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (fc2): Linear(in_features=2048, out_features=4096, bias=True)
            )
            (ls2): Identity()
          )
        )
      )
    )
    (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
    (decoder_embed): Linear(in_features=4096, out_features=128, bias=True)
    (decoder_blocks): ModuleList(
      (0-1): 2 x FullyShardedDataParallel(
        (_fsdp_wrapped_module): CheckpointWrapper(
          (_checkpoint_wrapped_module): Block(
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=128, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=128, out_features=128, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=512, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (fc2): Linear(in_features=512, out_features=128, bias=True)
            )
            (ls2): Identity()
          )
        )
      )
    )
    (decoder_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (decoder_pred): Linear(in_features=128, out_features=17856, bias=True)
  )
)
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
memory stats reset, ready to track
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
MIOpen(HIP): Warning [PlainTextDb] Unable to create a directory: "/tmp/.config/miopen"
MIOpen(HIP): Warning [PlainTextDb] Unable to create a directory: "/tmp/.config/miopen"
MIOpen(HIP): Warning [PlainTextDb] Unable to create a directory: "/tmp/.config/miopen"
epoch-train:  0 batch_idx 0 step 0  loss  0.05908203125 get_lr 
epoch-train:  0 batch_idx 1 step 1  loss  0.057861328125 get_lr 
epoch-train:  0 batch_idx 2 step 2  loss  0.056884765625 get_lr 
[2025-03-27 14:30:42,227] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:42,228] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:42,228] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:42,228] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:42,229] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:42,229] [INFO] [profiler.py:80:start_profile] Flops profiler started
epoch-train:  0 batch_idx 3 step 3  loss  0.057373046875 get_lr 
[2025-03-27 14:30:42,229] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:42,229] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:42,230] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:42,230] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:42,230] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:42,231] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:42,231] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:42,231] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:42,231] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:42,231] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:42,231] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:42,231] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:42,231] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:42,231] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:42,231] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:42,231] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:42,231] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:42,231] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:42,231] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:42,231] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:42,231] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:42,231] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:42,231] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:42,232] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:42,232] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:42,232] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:45,933] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:45,933] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:45,943] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:45,943] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:45,945] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:45,945] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:45,956] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:45,956] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:45,957] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:45,957] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:45,958] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:45,958] [INFO] [profiler.py:226:end_profile] Flops profiler finished
epoch-train:  0 batch_idx 4 step 4  loss  0.061767578125 get_lr 
[2025-03-27 14:30:45,968] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:45,969] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:45,972] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:45,971] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:45,975] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:45,976] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:45,976] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:45,976] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:45,976] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:45,976] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:45,976] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:45,977] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:46,011] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:46,012] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:46,030] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:46,030] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:46,037] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:46,037] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:46,038] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:46,039] [INFO] [profiler.py:226:end_profile] Flops profiler finished
epoch:  0  epoch_loss  0.004725302419354839

--> cuda max reserved memory = 62.748
--> max reserved percentage = 98.07 %

--> cuda max memory allocated = 60.1523
--> max allocated percentage = 94.01 %

--> peak active memory = 60.1751
--> peak active memory 94.05 %

cudaMalloc retries = 6
cuda OOM = 0

--> Recommend decreasing batch size...cuda retries can greatly degrade perf!

HERE1 Namespace(arch='orbit', batch_size=2, dataset='appl', depth=32, embed_dim=4096, fsdp_size=2, log_dir='./logs/2025-03-27--14:29:28.381665_dataset[appl]_batch_size[2]/', max_epochs=10000, num_heads=32, seq_par_size=1, simple_ddp_size=2, tensor_par_size=8, yaml_config='configs/appl.yaml') 62.748 8.567408842143234 tensor(6604997568)

/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
0.02user 0.07system 2:06.86elapsed 0%CPU (0avgtext+0avgdata 18432maxresident)k
2966inputs+624outputs (10major+3225minor)pagefaults 0swaps
