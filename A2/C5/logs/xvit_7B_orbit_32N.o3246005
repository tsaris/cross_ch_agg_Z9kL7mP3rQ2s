
Lmod is automatically replacing "cce/18.0.1" with "gcc-native/13.2".


Lmod is automatically replacing "PrgEnv-cray/8.6.0" with "PrgEnv-gnu/8.6.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) cray-libsci/24.11.0     2) cray-mpich/8.1.31     3) darshan-runtime/3.4.6-mpi


Lmod is automatically replacing "gcc-native/13.2" with "gcc/12.2.0".


Inactive Modules:
  1) darshan-runtime

The following have been reloaded with a version change:
  1) cray-libsci/24.11.0 => cray-libsci/23.09.1.1
  2) cray-mpich/8.1.31 => cray-mpich/8.1.27
  3) libfabric/1.22.0 => libfabric/1.20.1

Lmod has detected the following error: These module(s) or extension(s) exist
but cannot be loaded as requested: "darshan-runtime"
   Try: "module spider darshan-runtime" to see how to load the module(s).



for i in {orbit,}; do for j in {2,}; do for k in {1}; do python train.py \ configs/appl.yaml \ --max_epochs 10000 \ --fsdp_size 2 \ --simple_ddp_size 16 \ --seq_par_size 1 \ --tensor_par_size 8 \ --dataset appl \ --arch $i \ --batch_size $j \ --embed_dim 4096 \ --depth 32 \ --num_heads 32 echo "sleeping..." sleep 5 echo "Done" done done done
Fontconfig error: No writable cache directories
Fontconfig error: No writable cache directories
Fontconfig error: No writable cache directories
Fontconfig error: No writable cache directories
Fontconfig error: No writable cache directories
Fontconfig error: No writable cache directories
Fontconfig error: No writable cache directories
Fontconfig error: No writable cache directories
Fontconfig error: No writable cache directories
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,309] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,309] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,362] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,362] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,362] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,362] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,370] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,370] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,433] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,433] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,433] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,433] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,433] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,433] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,486] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,486] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,486] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,486] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,513] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,513] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,513] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,513] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,513] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,513] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,538] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,538] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,538] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,538] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,538] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,538] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,538] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,539] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,655] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,655] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,790] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,790] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,790] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,790] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,790] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:24,790] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:25,167] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:25,167] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:25,167] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:25,252] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:25,252] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:25,252] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:25,252] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:25,253] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:25,411] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:25,411] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:25,534] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:25,534] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:25,534] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:25,534] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:25,534] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:25,534] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:25,662] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:25,785] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:25,785] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:25,785] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:25,785] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:25,785] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:25,785] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:25,785] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,095] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,136] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,136] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,136] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,149] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,149] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,149] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,182] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,182] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,182] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,182] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,182] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,182] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,182] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,183] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,183] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,274] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,274] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,274] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,274] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,274] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,275] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,274] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,275] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,275] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,275] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,310] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,310] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,310] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,310] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,310] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,310] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,315] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,315] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,315] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,334] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,334] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,334] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,358] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,425] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,425] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,425] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,425] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,425] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,474] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,474] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,474] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,474] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,474] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,512] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,512] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,512] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,512] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,512] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,512] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,512] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,528] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,528] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,528] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,528] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,528] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,653] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,658] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,658] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,658] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,663] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,663] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,663] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,672] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,774] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,774] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,774] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,780] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,780] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,780] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,780] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,780] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,780] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,780] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,782] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,782] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,782] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,782] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,782] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,820] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,820] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,820] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,820] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,820] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,820] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,820] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,826] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,826] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,826] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,826] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,827] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,911] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,911] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,911] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,911] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,988] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,988] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,988] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:26,988] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,004] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,004] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,128] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,128] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,128] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,128] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,128] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,128] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,134] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,134] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,182] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,182] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,182] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,185] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,185] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,185] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,185] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,220] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,263] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,263] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,263] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,263] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,263] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,264] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,281] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,281] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,281] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,281] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,286] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,312] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,312] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,312] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,312] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,319] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,319] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,319] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,319] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,319] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,319] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,319] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,411] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,411] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,411] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,438] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,482] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,482] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,484] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,484] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,484] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,582] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,586] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,586] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,586] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,586] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,604] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,604] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,604] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,604] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,604] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,604] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,604] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,604] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,609] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,609] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,609] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,609] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,609] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,609] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,609] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,656] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,656] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,670] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,670] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,670] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,670] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,674] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,675] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,675] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,675] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,676] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,676] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,676] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,784] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,784] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,784] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,785] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,785] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-27 14:29:27,785] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
HERE0 Namespace(arch='orbit', batch_size=2, dataset='appl', depth=32, embed_dim=4096, fsdp_size=2, log_dir='./output_dir', max_epochs=10000, num_heads=32, seq_par_size=1, simple_ddp_size=16, tensor_par_size=8, yaml_config='configs/appl.yaml')
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:464] [c10d] The server socket cannot be initialized on [::]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
rank 45 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 45 After initialize parallelism groups
rank 179 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 179 After initialize parallelism groups
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 151 Before initialize parallelism groups
rank 214 Before initialize parallelism groups
rank 7 Before initialize parallelism groups
Using dist.init_process_group. world_size  256
rank 180 Before initialize parallelism groups
rank 151 After initialize parallelism groups
rank 214 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 146 Before initialize parallelism groups
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 7 After initialize parallelism groups
Using dist.init_process_group. world_size  256
rank 2 Before initialize parallelism groups
rank 180 After initialize parallelism groups
rank 210 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 106 Before initialize parallelism groups
rank 109 Before initialize parallelism groups
rank 105 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 104 Before initialize parallelism groups
rank 146 After initialize parallelism groups
rank 1 Before initialize parallelism groups
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 2 After initialize parallelism groups
Using dist.init_process_group. world_size  256
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
rank 108 Before initialize parallelism groups
rank 210 After initialize parallelism groups
rank 105 After initialize parallelism groups
rank 106 After initialize parallelism groups
rank 109 After initialize parallelism groups
Using dist.init_process_group. world_size  256
rank 104 After initialize parallelism groups
rank 1 After initialize parallelism groups
rank 147 Before initialize parallelism groups
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
rank 107 Before initialize parallelism groups
rank 108 After initialize parallelism groups
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 90 Before initialize parallelism groups
rank 93 Before initialize parallelism groups
rank 147 After initialize parallelism groups
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 107 After initialize parallelism groups
rank 177 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
rank 31 Before initialize parallelism groups
rank 25 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 93 After initialize parallelism groups
rank 90 After initialize parallelism groups
Using dist.init_process_group. world_size  256
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
rank 30 Before initialize parallelism groups
rank 116 Before initialize parallelism groups
rank 118 Before initialize parallelism groups
rank 29 Before initialize parallelism groups
rank 177 After initialize parallelism groups
rank 95 Before initialize parallelism groups
rank 88 Before initialize parallelism groups
rank 31 After initialize parallelism groups
rank 25 After initialize parallelism groups
rank 91 Before initialize parallelism groups
rank 26 Before initialize parallelism groups
rank 30 After initialize parallelism groups
rank 114 Before initialize parallelism groups
rank 29 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 95 After initialize parallelism groups
Using dist.init_process_group. world_size  256
rank 118 After initialize parallelism groups
rank 116 After initialize parallelism groups
rank 89 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 88 After initialize parallelism groups
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 24 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
rank 91 After initialize parallelism groups
rank 26 After initialize parallelism groups
rank 213 Before initialize parallelism groups
rank 114 After initialize parallelism groups
rank 115 Before initialize parallelism groups
Using dist.init_process_group. world_size  256
rank 112 Before initialize parallelism groups
rank 89 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 24 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 213 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 115 After initialize parallelism groups
rank 112 After initialize parallelism groups
Using dist.init_process_group. world_size  256
rank 208 Before initialize parallelism groups
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 27 Before initialize parallelism groups
rank 211 Before initialize parallelism groups
rank 94 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
rank 3 Before initialize parallelism groups
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 117 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
rank 92 Before initialize parallelism groups
rank 27 After initialize parallelism groups
Using dist.init_process_group. world_size  256
rank 94 After initialize parallelism groups
rank 208 After initialize parallelism groups
rank 211 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
rank 3 After initialize parallelism groups
rank 117 After initialize parallelism groups
Using dist.init_process_group. world_size  256
rank 92 After initialize parallelism groups
rank 6 Before initialize parallelism groups
rank 212 Before initialize parallelism groups
rank 215 Before initialize parallelism groups
rank 111 Before initialize parallelism groups
rank 209 Before initialize parallelism groups
rank 5 Before initialize parallelism groups
rank 243 Before initialize parallelism groups
rank 244 Before initialize parallelism groups
rank 241 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
rank 6 After initialize parallelism groups
rank 215 After initialize parallelism groups
rank 212 After initialize parallelism groups
rank 111 After initialize parallelism groups
rank 185 Before initialize parallelism groups
rank 190 Before initialize parallelism groups
rank 246 Before initialize parallelism groups
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 4 Before initialize parallelism groups
rank 243 After initialize parallelism groups
rank 5 After initialize parallelism groups
rank 28 Before initialize parallelism groups
rank 244 After initialize parallelism groups
Using dist.init_process_group. world_size  256
rank 241 After initialize parallelism groups
rank 209 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
rank 185 After initialize parallelism groups
rank 190 After initialize parallelism groups
rank 246 After initialize parallelism groups
Using dist.init_process_group. world_size  256
rank 4 After initialize parallelism groups
rank 28 After initialize parallelism groups
rank 110 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
rank 242 Before initialize parallelism groups
rank 110 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 76 Before initialize parallelism groups
rank 77 Before initialize parallelism groups
rank 72 Before initialize parallelism groups
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 240 Before initialize parallelism groups
Using dist.init_process_group. world_size  256
rank 242 After initialize parallelism groups
rank 186 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
rank 76 After initialize parallelism groups
rank 77 After initialize parallelism groups
rank 72 After initialize parallelism groups
rank 113 Before initialize parallelism groups
rank 240 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 186 After initialize parallelism groups
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 245 Before initialize parallelism groups
rank 113 After initialize parallelism groups
Using dist.init_process_group. world_size  256
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
rank 75 Before initialize parallelism groups
Using dist.init_process_group. world_size  256
rank 184 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 119 Before initialize parallelism groups
rank 64 Before initialize parallelism groups
rank 66 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 245 After initialize parallelism groups
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
rank 191 Before initialize parallelism groups
rank 11 Before initialize parallelism groups
rank 9 Before initialize parallelism groups
rank 75 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 184 After initialize parallelism groups
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 66 After initialize parallelism groups
rank 64 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 119 After initialize parallelism groups
Using dist.init_process_group. world_size  256
rank 39 Before initialize parallelism groups
rank 37 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
rank 153 Before initialize parallelism groups
rank 154 Before initialize parallelism groups
rank 152 Before initialize parallelism groups
rank 191 After initialize parallelism groups
Using dist.init_process_group. world_size  256
rank 9 After initialize parallelism groups
rank 11 After initialize parallelism groups
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 157 Before initialize parallelism groups
rank 39 After initialize parallelism groups
rank 37 After initialize parallelism groups
rank 254 Before initialize parallelism groups
rank 253 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
rank 154 After initialize parallelism groups
rank 189 Before initialize parallelism groups
rank 152 After initialize parallelism groups
rank 153 After initialize parallelism groups
rank 249 Before initialize parallelism groups
rank 252 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
rank 157 After initialize parallelism groups
rank 254 After initialize parallelism groups
rank 253 After initialize parallelism groups
rank 68 Before initialize parallelism groups
rank 255 Before initialize parallelism groups
rank 189 After initialize parallelism groups
rank 187 Before initialize parallelism groups
rank 249 After initialize parallelism groups
rank 252 After initialize parallelism groups
rank 78 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
rank 65 Before initialize parallelism groups
rank 38 Before initialize parallelism groups
Using dist.init_process_group. world_size  256
rank 71 Before initialize parallelism groups
rank 69 Before initialize parallelism groups
rank 188 Before initialize parallelism groups
rank 247 Before initialize parallelism groups
rank 32 Before initialize parallelism groups
rank 68 After initialize parallelism groups
rank 255 After initialize parallelism groups
rank 187 After initialize parallelism groups
rank 78 After initialize parallelism groups
rank 38 After initialize parallelism groups
rank 65 After initialize parallelism groups
rank 10 Before initialize parallelism groups
rank 71 After initialize parallelism groups
rank 248 Before initialize parallelism groups
rank 69 After initialize parallelism groups
rank 188 After initialize parallelism groups
rank 67 Before initialize parallelism groups
rank 32 After initialize parallelism groups
rank 247 After initialize parallelism groups
Using dist.init_process_group. world_size  256
rank 15 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 10 After initialize parallelism groups
rank 248 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 35 Before initialize parallelism groups
Using dist.init_process_group. world_size  256
rank 67 After initialize parallelism groups
rank 13 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 15 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 35 After initialize parallelism groups
rank 13 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 34 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 12 Before initialize parallelism groups
rank 250 Before initialize parallelism groups
rank 251 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 34 After initialize parallelism groups
rank 36 Before initialize parallelism groups
rank 79 Before initialize parallelism groups
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 12 After initialize parallelism groups
rank 250 After initialize parallelism groups
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 251 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 36 After initialize parallelism groups
Using dist.init_process_group. world_size  256
rank 79 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 155 Before initialize parallelism groups
rank 159 Before initialize parallelism groups
rank 33 Before initialize parallelism groups
rank 70 Before initialize parallelism groups
rank 74 Before initialize parallelism groups
rank 73 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 83 Before initialize parallelism groups
Using dist.init_process_group. world_size  256
rank 162 Before initialize parallelism groups
rank 165 Before initialize parallelism groups
rank 163 Before initialize parallelism groups
rank 155 After initialize parallelism groups
rank 102 Before initialize parallelism groups
rank 159 After initialize parallelism groups
rank 97 Before initialize parallelism groups
rank 98 Before initialize parallelism groups
rank 33 After initialize parallelism groups
rank 70 After initialize parallelism groups
rank 73 After initialize parallelism groups
rank 74 After initialize parallelism groups
rank 14 Before initialize parallelism groups
rank 136 Before initialize parallelism groups
rank 55 Before initialize parallelism groups
rank 51 Before initialize parallelism groups
rank 224 Before initialize parallelism groups
rank 231 Before initialize parallelism groups
Using dist.init_process_group. world_size  256
rank 227 Before initialize parallelism groups
rank 83 After initialize parallelism groups
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 162 After initialize parallelism groups
rank 165 After initialize parallelism groups
rank 163 After initialize parallelism groups
rank 102 After initialize parallelism groups
rank 97 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 156 Before initialize parallelism groups
rank 98 After initialize parallelism groups
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 136 After initialize parallelism groups
rank 14 After initialize parallelism groups
rank 51 After initialize parallelism groups
rank 55 After initialize parallelism groups
rank 231 After initialize parallelism groups
rank 224 After initialize parallelism groups
rank 8 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 227 After initialize parallelism groups
Using dist.init_process_group. world_size  256
rank 158 Before initialize parallelism groups
Using dist.init_process_group. world_size  256
rank 156 After initialize parallelism groups
rank 82 Before initialize parallelism groups
rank 84 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
rank 8 After initialize parallelism groups
rank 164 Before initialize parallelism groups
rank 158 After initialize parallelism groups
rank 82 After initialize parallelism groups
rank 160 Before initialize parallelism groups
rank 169 Before initialize parallelism groups
rank 168 Before initialize parallelism groups
rank 84 After initialize parallelism groups
rank 229 Before initialize parallelism groups
Using dist.init_process_group. world_size  256
rank 164 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 170 Before initialize parallelism groups
rank 160 After initialize parallelism groups
rank 169 After initialize parallelism groups
rank 168 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 229 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
rank 17 Before initialize parallelism groups
rank 81 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 170 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 175 Before initialize parallelism groups
rank 81 After initialize parallelism groups
rank 17 After initialize parallelism groups
rank 140 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 103 Before initialize parallelism groups
rank 139 Before initialize parallelism groups
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 16 Before initialize parallelism groups
Using dist.init_process_group. world_size  256
rank 167 Before initialize parallelism groups
Using dist.init_process_group. world_size  256
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 175 After initialize parallelism groups
Using dist.init_process_group. world_size  256
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
rank 140 After initialize parallelism groups
Using dist.init_process_group. world_size  256
rank 80 Before initialize parallelism groups
rank 103 After initialize parallelism groups
Using dist.init_process_group. world_size  256
rank 139 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
rank 101 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 236 Before initialize parallelism groups
Using dist.init_process_group. world_size  256
rank 53 Before initialize parallelism groups
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 16 After initialize parallelism groups
rank 167 After initialize parallelism groups
Using dist.init_process_group. world_size  256
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 87 Before initialize parallelism groups
Using dist.init_process_group. world_size  256
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 217 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 80 After initialize parallelism groups
rank 101 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 86 Before initialize parallelism groups
Using dist.init_process_group. world_size  256
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
rank 236 After initialize parallelism groups
rank 53 After initialize parallelism groups
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
rank 198 Before initialize parallelism groups
rank 192 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 87 After initialize parallelism groups
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
rank 99 Before initialize parallelism groups
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 232 Before initialize parallelism groups
rank 171 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 217 After initialize parallelism groups
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 225 Before initialize parallelism groups
rank 131 Before initialize parallelism groups
rank 134 Before initialize parallelism groups
rank 132 Before initialize parallelism groups
rank 228 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 86 After initialize parallelism groups
rank 133 Before initialize parallelism groups
rank 166 Before initialize parallelism groups
rank 54 Before initialize parallelism groups
rank 219 Before initialize parallelism groups
rank 198 After initialize parallelism groups
rank 161 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 192 After initialize parallelism groups
rank 122 Before initialize parallelism groups
rank 235 Before initialize parallelism groups
rank 99 After initialize parallelism groups
Using dist.init_process_group. world_size  256
rank 171 After initialize parallelism groups
rank 232 After initialize parallelism groups
Using dist.init_process_group. world_size  256
rank 200 Before initialize parallelism groups
rank 225 After initialize parallelism groups
rank 228 After initialize parallelism groups
rank 132 After initialize parallelism groups
rank 134 After initialize parallelism groups
rank 131 After initialize parallelism groups
rank 201 Before initialize parallelism groups
Using dist.init_process_group. world_size  256
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 233 Before initialize parallelism groups
rank 137 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 133 After initialize parallelism groups
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 96 Before initialize parallelism groups
rank 219 After initialize parallelism groups
rank 50 Before initialize parallelism groups
rank 54 After initialize parallelism groups
rank 166 After initialize parallelism groups
rank 127 Before initialize parallelism groups
rank 52 Before initialize parallelism groups
rank 161 After initialize parallelism groups
rank 85 Before initialize parallelism groups
rank 220 Before initialize parallelism groups
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 235 After initialize parallelism groups
rank 174 Before initialize parallelism groups
rank 122 After initialize parallelism groups
rank 173 Before initialize parallelism groups
rank 205 Before initialize parallelism groups
rank 22 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
rank 200 After initialize parallelism groups
rank 49 Before initialize parallelism groups
rank 57 Before initialize parallelism groups
rank 58 Before initialize parallelism groups
rank 143 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 201 After initialize parallelism groups
rank 230 Before initialize parallelism groups
Using dist.init_process_group. world_size  256
rank 239 Before initialize parallelism groups
rank 142 Before initialize parallelism groups
rank 193 Before initialize parallelism groups
rank 100 Before initialize parallelism groups
rank 202 Before initialize parallelism groups
rank 226 Before initialize parallelism groups
rank 137 After initialize parallelism groups
Using dist.init_process_group. world_size  256
rank 233 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 207 Before initialize parallelism groups
rank 50 After initialize parallelism groups
rank 96 After initialize parallelism groups
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 204 Before initialize parallelism groups
rank 62 Before initialize parallelism groups
rank 52 After initialize parallelism groups
rank 220 After initialize parallelism groups
rank 59 Before initialize parallelism groups
rank 85 After initialize parallelism groups
rank 174 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 127 After initialize parallelism groups
rank 173 After initialize parallelism groups
Using dist.init_process_group. world_size  256
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 22 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 205 After initialize parallelism groups
rank 216 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 49 After initialize parallelism groups
rank 128 Before initialize parallelism groups
Using dist.init_process_group. world_size  256
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
rank 143 After initialize parallelism groups
rank 57 After initialize parallelism groups
rank 58 After initialize parallelism groups
rank 230 After initialize parallelism groups
rank 142 After initialize parallelism groups
rank 239 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 100 After initialize parallelism groups
Using dist.init_process_group. world_size  256
rank 193 After initialize parallelism groups
rank 223 Before initialize parallelism groups
rank 226 After initialize parallelism groups
rank 202 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 207 After initialize parallelism groups
rank 204 After initialize parallelism groups
rank 62 After initialize parallelism groups
Using dist.init_process_group. world_size  256
rank 59 After initialize parallelism groups
rank 135 Before initialize parallelism groups
rank 216 After initialize parallelism groups
rank 237 Before initialize parallelism groups
rank 138 Before initialize parallelism groups
rank 141 Before initialize parallelism groups
rank 21 Before initialize parallelism groups
rank 128 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 223 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
rank 221 Before initialize parallelism groups
rank 197 Before initialize parallelism groups
rank 19 Before initialize parallelism groups
rank 135 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
rank 234 Before initialize parallelism groups
rank 237 After initialize parallelism groups
rank 138 After initialize parallelism groups
rank 141 After initialize parallelism groups
rank 172 Before initialize parallelism groups
rank 21 After initialize parallelism groups
rank 130 Before initialize parallelism groups
rank 126 Before initialize parallelism groups
rank 123 Before initialize parallelism groups
rank 222 Before initialize parallelism groups
rank 206 Before initialize parallelism groups
rank 124 Before initialize parallelism groups
rank 48 Before initialize parallelism groups
rank 221 After initialize parallelism groups
rank 197 After initialize parallelism groups
rank 19 After initialize parallelism groups
rank 63 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 218 Before initialize parallelism groups
Using dist.init_process_group. world_size  256
rank 120 Before initialize parallelism groups
rank 61 Before initialize parallelism groups
rank 172 After initialize parallelism groups
rank 234 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 130 After initialize parallelism groups
Using dist.init_process_group. world_size  256
rank 126 After initialize parallelism groups
rank 123 After initialize parallelism groups
rank 222 After initialize parallelism groups
rank 206 After initialize parallelism groups
rank 48 After initialize parallelism groups
rank 124 After initialize parallelism groups
rank 18 Before initialize parallelism groups
rank 194 Before initialize parallelism groups
rank 63 After initialize parallelism groups
rank 20 Before initialize parallelism groups
Using dist.init_process_group. world_size  256
rank 218 After initialize parallelism groups
rank 61 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
rank 120 After initialize parallelism groups
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 23 Before initialize parallelism groups
rank 18 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
rank 194 After initialize parallelism groups
rank 20 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
rank 203 Before initialize parallelism groups
rank 23 After initialize parallelism groups
rank 196 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 46 Before initialize parallelism groups
rank 203 After initialize parallelism groups
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 40 Before initialize parallelism groups
rank 195 Before initialize parallelism groups
rank 196 After initialize parallelism groups
rank 238 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
rank 125 Before initialize parallelism groups
rank 46 After initialize parallelism groups
rank 40 After initialize parallelism groups
rank 195 After initialize parallelism groups
rank 60 Before initialize parallelism groups
rank 238 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier01175.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  256
rank 125 After initialize parallelism groups
rank 60 After initialize parallelism groups
rank 129 Before initialize parallelism groups
rank 199 Before initialize parallelism groups
rank 56 Before initialize parallelism groups
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 129 After initialize parallelism groups
rank 199 After initialize parallelism groups
rank 56 After initialize parallelism groups
rank 121 Before initialize parallelism groups
rank 43 Before initialize parallelism groups
rank 121 After initialize parallelism groups
rank 41 Before initialize parallelism groups
rank 43 After initialize parallelism groups
rank 41 After initialize parallelism groups
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 42 Before initialize parallelism groups
rank 47 Before initialize parallelism groups
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
rank 42 After initialize parallelism groups
rank 47 After initialize parallelism groups
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
Using dist.init_process_group. world_size  256
config_path  configs/appl.yaml
rank 149 Before initialize parallelism groups
rank 150 Before initialize parallelism groups
rank 144 Before initialize parallelism groups
rank 182 Before initialize parallelism groups
rank 176 Before initialize parallelism groups
rank 183 Before initialize parallelism groups
rank 181 Before initialize parallelism groups
rank 178 Before initialize parallelism groups
rank 145 Before initialize parallelism groups
rank 149 After initialize parallelism groups
rank 150 After initialize parallelism groups
rank 148 Before initialize parallelism groups
rank 144 After initialize parallelism groups
rank 44 Before initialize parallelism groups
rank 182 After initialize parallelism groups
rank 183 After initialize parallelism groups
rank 176 After initialize parallelism groups
rank 181 After initialize parallelism groups
rank 178 After initialize parallelism groups
rank 145 After initialize parallelism groups
{'seed_everything': 42, 'trainer': {'max_epochs': 1, 'checkpoint_path': '/lustre/orion/stf218/world-shared/atsaris/tmp/tmp_checkpoints', 'checkpoint_filename': 'multi_last', 'resume_from_checkpoint': False}, 'parallelism': {'cpu_offloading': False}, 'model': {'lr': 0.015, 'beta_1': 0.9, 'beta_2': 0.95, 'weight_decay': 0.05, 'warmup_steps': 10, 'max_steps': 20, 'warmup_start_lr': '1e-8', 'eta_min': '1e-8', 'net': {'class_path': 'climax.arch.ClimaX', 'init_args': {'default_vars': ['R', 'G', 'B', 'W7N', 'TXN', 'D6V', '8Z7', 'QNY', 'QBR', 'QOM', 'IMY', 'DGZ', 'UY8', 'EMJ', 'DXK', 'NSH', 'Z4V', 'O8Y', 'RA6', 'QDP', '6BF', 'X8M', 'B61', '61N', 'A8K', 'CN5', 'NF9', 'CRB', '4RF', 'SAO', 'ZA6', 'A6P', 'EME', 'TVJ', 'SR7', 'J6U', 'GE6', 'OGJ', 'Y4M', 'TEF', '601', 'CS6', '2N3', '4HM', 'CEL', 'XFH', '40J', 'JBR', '94X', 'PI7', 'CGA', 'U1A', 'EY3', 'GBW', 'VW0', 'TT5', '43R', 'XAK', 'WS0', 'FRW', 'X9X', 'QA7', 'JCH', 'HFQ', 'EP3', 'Z3F', 'JIU', 'K8O', '4TP', 'GNL', 'HLC', 'L7R', 'WSZ', '0X1', 'GHZ', 'KJ9', 'F7F', 'H4I', 'I53', 'YOI', 'YK6', 'YEC', 'JVR', 'V47', 'TXP', 'XPI', '4FX', 'FNW', 'I94', 'U5X', 'I10', 'OFY', 'CHF', '6MU', '923', 'K81', '8NF', 'FY1', 'NX4', 'YUG', '8Y1', 'BWQ', 'FLJ', '9DF', 'OBB', 'B7R', 'DB7', 'SH1', 'G2Y', 'CR4', '26W', 'QWK', '45X', '25D', '2G7', '5I5', '52R', 'KIU', 'K02', 'B3K', '16C', '7T8', '9Z1', '5VG', 'MIR', 'NBI', 'FMU', 'HTH', 'VNJ', 'Y3O', 'KGJ', '9VO', 'HKU', 'OKP', '54D', 'KBJ', 'F24', 'WBJ', 'B2M', 'VSV', 'JZM', 'BPX', 'EKE', 'UHE', 'KF5', 'TYL', 'S3E', 'NLF', 'NNK', 'WII', 'FS6', 'JWH', 'K1X', 'QWR', 'GWX', '1GB', 'O7P', 'ALX', 'TQZ', '4BL', 'UUD', 'NS6', 'IA5', '1T8', 'LUN', 'FAA', 'C0R', 'DUI', 'NP5', '88J', '5X8', 'A64', '1NB', 'CQK', '0UX', 'E23', 'WCZ', 'CSZ', 'AG5', 'IQN', 'X0B', 'S95', 'S4N', 'IZB', 'JTV', 'H87', 'UCJ', 'UVM', '22N', 'P4G', '8QH', '775', 'Q4A', 'KI7', 'PCT', 'A8M', 'KJ7', '7W5', 'XQ6', 'Q3K', 'AZS', '23S', '5LT', 'WAI', 'VQ1', 'BFG', 'IOY', 'IZ7', 'FCO', '2KJ', 'DF6', 'UXO', '7K5', 'A3A', 'XSW', 'JHB', '7XL', 'M01', 'EWU', '1CW', '8IH', 'MYA', '3P8', 'R6H', 'W0G', 'Q0S', '4VR', 'K2G', '7M5', '77Y', 'AN8', 'E7N', 'K2N', '02I', 'CZY', '738', 'G0B', 'Q9T', 'FZF', 'WJ9', '6P0', 'QQX', '0GV', '516', 'PBP', '25S', 'YGN', '47U', '54E', 'NAK', 'IET', '256', 'QXK', 'JBX', 'COZ', 'V5U', 'VG4', '198', 'EDT', 'A5V', '080', '2L9', 'OXJ', 'DGL', '1AQ', 'JXZ', 'TWF', '248', '4NF', 'KCD', '61Z', 'B3R', 'R6U', 'XGF', 'EFV', '32S', 'C64', '606', 'XGK', 'OOJ', '7L0', 'XXD', 'K9Y', 'PET', '6PU', 'GMY', 'OCP', '6YV', '5AU', '6EQ', 'ICF', 'JV9', '1OW', 'EVL', 'C3N', '6LH', 'L3H', 'Z55', '6XD', '37V', '68M', 'RNT', '8ND', 'GU5', 'QW5', 'VYW', '3AQ', '8H8', 'QG5', 'E64', 'DF9', 'X5C', '15Q', '3KP', 'LFV', 'SRT', 'SAM', 'IKB', 'YLV', '4FM', '6Z3', '910', 'BLL', 'AYL', 'XDS', 'TUY', '4GQ', 'SZ8', '76V', 'ZJG', 'B6V', 'N6B', '00B', 'IEM', 'H4B', '1JM', 'PJA', '597', 'AYY', 'M89', '60M', 'UE1', '5GU', '541', '2AQ', 'VY8', 'GDT', 'RSI', 'C1Q', 'B0H', '5KR', '04V', 'G61', 'HR3', 'FOD', '757', 'OWT', '5OS', 'QF9', '1W0', 'SFA', 'JU4', 'NV2', 'OMG', 'GO7', 'WBV', '741', 'KW9', '3GK', 'E2Z', 'YB3', 'T46', 'B2I', '1Y8', '58P', 'NX4', 'YTR', '4L8', 'B7F', 'PA9', 'CS3', '2TR', 'P8E', 'XLF', 'UQ2', '73B', 'IX7', '8ZP', '1UO', 'DBX', '4SL', 'K1F', '9OD', 'QZC', 'OG8', '54G', 'SN9', 'FEI', '6LZ', 'JFS', '3SA', 'YD5', 'PTP', 'QJQ', 'Y92', '6LV', 'TGT', '9EN', 'MZY', '18U', 'ZII', '9LD', '51X', 'ZSQ', 'VWM', 'QH9', 'BA3', 'DLS', 'G58', 'VRW', 'SBV', 'CVW', 'X3I', 'DSU', 'HNF', 'LLI', '557', 'XMI', 'J31', 'M5Q', 'CIU', 'S3P', 'LVQ', 'O8H', 'QUI', 'WJQ', 'R4Z', 'WB5', 'GL6', 'PE4', '3G8', 'XY0', 'OBR', '7BZ', 'O5T', '1OP', 'IPK', 'TCR', 'XXM', '21C', 'SBV', '4SL', 'WIE', 'A40', 'AKQ', 'PPO', 'PAV', 'QOX', '6N6', 'AKJ', 'UBM', '2F3', 'CI2', '8PX', 'ZUN', 'PLM', 'UCK', '5V0', 'WZP', 'AOJ', 'WWO', '005', 'KB3', 'Y4U', 'SFO', 'JUK', 'GVA', 'EQ3', '35G', '8X0', 'Q4I', 'YCJ', 'Y5A', 'JAW', 'KML', '01I', 'SCH', 'RJO', 'DHT', 'KVE', 'AAO', 'U4E', 'S3H', '59B', 'JIX', 'D5Y'], 'img_size': [192, 384], 'patch_size': 6, 'embed_dim': 512, 'depth': 8, 'decoder_depth': 2, 'num_heads': 16, 'mlp_ratio': 4, 'drop_path': 0.1, 'drop_rate': 0.1, 'aggregated_variables': 1, 'mask_ratio': 0.75}}}, 'data': {'dict_root_dirs': {'mpi-esm': '/lustre/orion/stf218/scratch/atsaris/APPL_DATA'}, 'dict_in_variables': {'mpi-esm': ['R', 'G', 'B', 'W7N', 'TXN', 'D6V', '8Z7', 'QNY', 'QBR', 'QOM', 'IMY', 'DGZ', 'UY8', 'EMJ', 'DXK', 'NSH', 'Z4V', 'O8Y', 'RA6', 'QDP', '6BF', 'X8M', 'B61', '61N', 'A8K', 'CN5', 'NF9', 'CRB', '4RF', 'SAO', 'ZA6', 'A6P', 'EME', 'TVJ', 'SR7', 'J6U', 'GE6', 'OGJ', 'Y4M', 'TEF', '601', 'CS6', '2N3', '4HM', 'CEL', 'XFH', '40J', 'JBR', '94X', 'PI7', 'CGA', 'U1A', 'EY3', 'GBW', 'VW0', 'TT5', '43R', 'XAK', 'WS0', 'FRW', 'X9X', 'QA7', 'JCH', 'HFQ', 'EP3', 'Z3F', 'JIU', 'K8O', '4TP', 'GNL', 'HLC', 'L7R', 'WSZ', '0X1', 'GHZ', 'KJ9', 'F7F', 'H4I', 'I53', 'YOI', 'YK6', 'YEC', 'JVR', 'V47', 'TXP', 'XPI', '4FX', 'FNW', 'I94', 'U5X', 'I10', 'OFY', 'CHF', '6MU', '923', 'K81', '8NF', 'FY1', 'NX4', 'YUG', '8Y1', 'BWQ', 'FLJ', '9DF', 'OBB', 'B7R', 'DB7', 'SH1', 'G2Y', 'CR4', '26W', 'QWK', '45X', '25D', '2G7', '5I5', '52R', 'KIU', 'K02', 'B3K', '16C', '7T8', '9Z1', '5VG', 'MIR', 'NBI', 'FMU', 'HTH', 'VNJ', 'Y3O', 'KGJ', '9VO', 'HKU', 'OKP', '54D', 'KBJ', 'F24', 'WBJ', 'B2M', 'VSV', 'JZM', 'BPX', 'EKE', 'UHE', 'KF5', 'TYL', 'S3E', 'NLF', 'NNK', 'WII', 'FS6', 'JWH', 'K1X', 'QWR', 'GWX', '1GB', 'O7P', 'ALX', 'TQZ', '4BL', 'UUD', 'NS6', 'IA5', '1T8', 'LUN', 'FAA', 'C0R', 'DUI', 'NP5', '88J', '5X8', 'A64', '1NB', 'CQK', '0UX', 'E23', 'WCZ', 'CSZ', 'AG5', 'IQN', 'X0B', 'S95', 'S4N', 'IZB', 'JTV', 'H87', 'UCJ', 'UVM', '22N', 'P4G', '8QH', '775', 'Q4A', 'KI7', 'PCT', 'A8M', 'KJ7', '7W5', 'XQ6', 'Q3K', 'AZS', '23S', '5LT', 'WAI', 'VQ1', 'BFG', 'IOY', 'IZ7', 'FCO', '2KJ', 'DF6', 'UXO', '7K5', 'A3A', 'XSW', 'JHB', '7XL', 'M01', 'EWU', '1CW', '8IH', 'MYA', '3P8', 'R6H', 'W0G', 'Q0S', '4VR', 'K2G', '7M5', '77Y', 'AN8', 'E7N', 'K2N', '02I', 'CZY', '738', 'G0B', 'Q9T', 'FZF', 'WJ9', '6P0', 'QQX', '0GV', '516', 'PBP', '25S', 'YGN', '47U', '54E', 'NAK', 'IET', '256', 'QXK', 'JBX', 'COZ', 'V5U', 'VG4', '198', 'EDT', 'A5V', '080', '2L9', 'OXJ', 'DGL', '1AQ', 'JXZ', 'TWF', '248', '4NF', 'KCD', '61Z', 'B3R', 'R6U', 'XGF', 'EFV', '32S', 'C64', '606', 'XGK', 'OOJ', '7L0', 'XXD', 'K9Y', 'PET', '6PU', 'GMY', 'OCP', '6YV', '5AU', '6EQ', 'ICF', 'JV9', '1OW', 'EVL', 'C3N', '6LH', 'L3H', 'Z55', '6XD', '37V', '68M', 'RNT', '8ND', 'GU5', 'QW5', 'VYW', '3AQ', '8H8', 'QG5', 'E64', 'DF9', 'X5C', '15Q', '3KP', 'LFV', 'SRT', 'SAM', 'IKB', 'YLV', '4FM', '6Z3', '910', 'BLL', 'AYL', 'XDS', 'TUY', '4GQ', 'SZ8', '76V', 'ZJG', 'B6V', 'N6B', '00B', 'IEM', 'H4B', '1JM', 'PJA', '597', 'AYY', 'M89', '60M', 'UE1', '5GU', '541', '2AQ', 'VY8', 'GDT', 'RSI', 'C1Q', 'B0H', '5KR', '04V', 'G61', 'HR3', 'FOD', '757', 'OWT', '5OS', 'QF9', '1W0', 'SFA', 'JU4', 'NV2', 'OMG', 'GO7', 'WBV', '741', 'KW9', '3GK', 'E2Z', 'YB3', 'T46', 'B2I', '1Y8', '58P', 'NX4', 'YTR', '4L8', 'B7F', 'PA9', 'CS3', '2TR', 'P8E', 'XLF', 'UQ2', '73B', 'IX7', '8ZP', '1UO', 'DBX', '4SL', 'K1F', '9OD', 'QZC', 'OG8', '54G', 'SN9', 'FEI', '6LZ', 'JFS', '3SA', 'YD5', 'PTP', 'QJQ', 'Y92', '6LV', 'TGT', '9EN', 'MZY', '18U', 'ZII', '9LD', '51X', 'ZSQ', 'VWM', 'QH9', 'BA3', 'DLS', 'G58', 'VRW', 'SBV', 'CVW', 'X3I', 'DSU', 'HNF', 'LLI', '557', 'XMI', 'J31', 'M5Q', 'CIU', 'S3P', 'LVQ', 'O8H', 'QUI', 'WJQ', 'R4Z', 'WB5', 'GL6', 'PE4', '3G8', 'XY0', 'OBR', '7BZ', 'O5T', '1OP', 'IPK', 'TCR', 'XXM', '21C', 'SBV', '4SL', 'WIE', 'A40', 'AKQ', 'PPO', 'PAV', 'QOX', '6N6', 'AKJ', 'UBM', '2F3', 'CI2', '8PX', 'ZUN', 'PLM', 'UCK', '5V0', 'WZP', 'AOJ', 'WWO', '005', 'KB3', 'Y4U', 'SFO', 'JUK', 'GVA', 'EQ3', '35G', '8X0', 'Q4I', 'YCJ', 'Y5A', 'JAW', 'KML', '01I', 'SCH', 'RJO', 'DHT', 'KVE', 'AAO', 'U4E', 'S3H', '59B', 'JIX', 'D5Y']}, 'batch_size': 2, 'num_workers': 4, 'pin_memory': True}}
max_epochs 10000 data_par_size 32 fsdp_size 2 simple_ddp_size 16 tensor_par_size 8 seq_par_size 1 cpu_offloading False
lr  0.015 beta_1  0.9 beta_2 0.95 weight_decay 0.05 class_path climax.arch.ClimaX default_vars ['R', 'G', 'B', 'W7N', 'TXN', 'D6V', '8Z7', 'QNY', 'QBR', 'QOM', 'IMY', 'DGZ', 'UY8', 'EMJ', 'DXK', 'NSH', 'Z4V', 'O8Y', 'RA6', 'QDP', '6BF', 'X8M', 'B61', '61N', 'A8K', 'CN5', 'NF9', 'CRB', '4RF', 'SAO', 'ZA6', 'A6P', 'EME', 'TVJ', 'SR7', 'J6U', 'GE6', 'OGJ', 'Y4M', 'TEF', '601', 'CS6', '2N3', '4HM', 'CEL', 'XFH', '40J', 'JBR', '94X', 'PI7', 'CGA', 'U1A', 'EY3', 'GBW', 'VW0', 'TT5', '43R', 'XAK', 'WS0', 'FRW', 'X9X', 'QA7', 'JCH', 'HFQ', 'EP3', 'Z3F', 'JIU', 'K8O', '4TP', 'GNL', 'HLC', 'L7R', 'WSZ', '0X1', 'GHZ', 'KJ9', 'F7F', 'H4I', 'I53', 'YOI', 'YK6', 'YEC', 'JVR', 'V47', 'TXP', 'XPI', '4FX', 'FNW', 'I94', 'U5X', 'I10', 'OFY', 'CHF', '6MU', '923', 'K81', '8NF', 'FY1', 'NX4', 'YUG', '8Y1', 'BWQ', 'FLJ', '9DF', 'OBB', 'B7R', 'DB7', 'SH1', 'G2Y', 'CR4', '26W', 'QWK', '45X', '25D', '2G7', '5I5', '52R', 'KIU', 'K02', 'B3K', '16C', '7T8', '9Z1', '5VG', 'MIR', 'NBI', 'FMU', 'HTH', 'VNJ', 'Y3O', 'KGJ', '9VO', 'HKU', 'OKP', '54D', 'KBJ', 'F24', 'WBJ', 'B2M', 'VSV', 'JZM', 'BPX', 'EKE', 'UHE', 'KF5', 'TYL', 'S3E', 'NLF', 'NNK', 'WII', 'FS6', 'JWH', 'K1X', 'QWR', 'GWX', '1GB', 'O7P', 'ALX', 'TQZ', '4BL', 'UUD', 'NS6', 'IA5', '1T8', 'LUN', 'FAA', 'C0R', 'DUI', 'NP5', '88J', '5X8', 'A64', '1NB', 'CQK', '0UX', 'E23', 'WCZ', 'CSZ', 'AG5', 'IQN', 'X0B', 'S95', 'S4N', 'IZB', 'JTV', 'H87', 'UCJ', 'UVM', '22N', 'P4G', '8QH', '775', 'Q4A', 'KI7', 'PCT', 'A8M', 'KJ7', '7W5', 'XQ6', 'Q3K', 'AZS', '23S', '5LT', 'WAI', 'VQ1', 'BFG', 'IOY', 'IZ7', 'FCO', '2KJ', 'DF6', 'UXO', '7K5', 'A3A', 'XSW', 'JHB', '7XL', 'M01', 'EWU', '1CW', '8IH', 'MYA', '3P8', 'R6H', 'W0G', 'Q0S', '4VR', 'K2G', '7M5', '77Y', 'AN8', 'E7N', 'K2N', '02I', 'CZY', '738', 'G0B', 'Q9T', 'FZF', 'WJ9', '6P0', 'QQX', '0GV', '516', 'PBP', '25S', 'YGN', '47U', '54E', 'NAK', 'IET', '256', 'QXK', 'JBX', 'COZ', 'V5U', 'VG4', '198', 'EDT', 'A5V', '080', '2L9', 'OXJ', 'DGL', '1AQ', 'JXZ', 'TWF', '248', '4NF', 'KCD', '61Z', 'B3R', 'R6U', 'XGF', 'EFV', '32S', 'C64', '606', 'XGK', 'OOJ', '7L0', 'XXD', 'K9Y', 'PET', '6PU', 'GMY', 'OCP', '6YV', '5AU', '6EQ', 'ICF', 'JV9', '1OW', 'EVL', 'C3N', '6LH', 'L3H', 'Z55', '6XD', '37V', '68M', 'RNT', '8ND', 'GU5', 'QW5', 'VYW', '3AQ', '8H8', 'QG5', 'E64', 'DF9', 'X5C', '15Q', '3KP', 'LFV', 'SRT', 'SAM', 'IKB', 'YLV', '4FM', '6Z3', '910', 'BLL', 'AYL', 'XDS', 'TUY', '4GQ', 'SZ8', '76V', 'ZJG', 'B6V', 'N6B', '00B', 'IEM', 'H4B', '1JM', 'PJA', '597', 'AYY', 'M89', '60M', 'UE1', '5GU', '541', '2AQ', 'VY8', 'GDT', 'RSI', 'C1Q', 'B0H', '5KR', '04V', 'G61', 'HR3', 'FOD', '757', 'OWT', '5OS', 'QF9', '1W0', 'SFA', 'JU4', 'NV2', 'OMG', 'GO7', 'WBV', '741', 'KW9', '3GK', 'E2Z', 'YB3', 'T46', 'B2I', '1Y8', '58P', 'NX4', 'YTR', '4L8', 'B7F', 'PA9', 'CS3', '2TR', 'P8E', 'XLF', 'UQ2', '73B', 'IX7', '8ZP', '1UO', 'DBX', '4SL', 'K1F', '9OD', 'QZC', 'OG8', '54G', 'SN9', 'FEI', '6LZ', 'JFS', '3SA', 'YD5', 'PTP', 'QJQ', 'Y92', '6LV', 'TGT', '9EN', 'MZY', '18U', 'ZII', '9LD', '51X', 'ZSQ', 'VWM', 'QH9', 'BA3', 'DLS', 'G58', 'VRW', 'SBV', 'CVW', 'X3I', 'DSU', 'HNF', 'LLI', '557', 'XMI', 'J31', 'M5Q', 'CIU', 'S3P', 'LVQ', 'O8H', 'QUI', 'WJQ', 'R4Z', 'WB5', 'GL6', 'PE4', '3G8', 'XY0', 'OBR', '7BZ', 'O5T', '1OP', 'IPK', 'TCR', 'XXM', '21C', 'SBV', '4SL', 'WIE', 'A40', 'AKQ', 'PPO', 'PAV', 'QOX', '6N6', 'AKJ', 'UBM', '2F3', 'CI2', '8PX', 'ZUN', 'PLM', 'UCK', '5V0', 'WZP', 'AOJ', 'WWO', '005', 'KB3', 'Y4U', 'SFO', 'JUK', 'GVA', 'EQ3', '35G', '8X0', 'Q4I', 'YCJ', 'Y5A', 'JAW', 'KML', '01I', 'SCH', 'RJO', 'DHT', 'KVE', 'AAO', 'U4E', 'S3H', '59B', 'JIX', 'D5Y']
img_size [192, 384] img_size_x 192 img_size_y 384 patch_size 6 emb_dim 4096 depth 32 decoder_depth 2 num_heads 32 mlp_ratio 4 drop_path 0.1 drop_rate 0.1 aggregated_variables 1
warmup_steps 10 max_steps 20 warmup_start_lr 1e-08 eta_min 1e-08
checkpoint_path /lustre/orion/stf218/world-shared/atsaris/tmp/tmp_checkpoints checkpoint_filename multi_last resume_from_checkpoint False
rank 0 Before initialize parallelism groups
rank 148 After initialize parallelism groups
rank 44 After initialize parallelism groups
rank 0 After initialize parallelism groups
rank 177 After initialize model
rank 177 before dist.barrier(group)
rank 179 After initialize model
rank 179 before dist.barrier(group)
rank 1 After initialize model
rank 1 before dist.barrier(group)
rank 209 After initialize model
rank 209 before dist.barrier(group)
rank 208 After initialize model
rank 208 before dist.barrier(group)
rank 45 After initialize model
rank 45 before dist.barrier(group)
rank 180 After initialize model
rank 180 before dist.barrier(group)
rank 146 After initialize model
rank 146 before dist.barrier(group)
rank 147 After initialize model
rank 147 before dist.barrier(group)
rank 105 After initialize model
rank 105 before dist.barrier(group)
rank 151 After initialize model
rank 151 before dist.barrier(group)
rank 104 After initialize model
rank 104 before dist.barrier(group)
rank 89 After initialize model
rank 89 before dist.barrier(group)
rank 185 After initialize model
rank 185 before dist.barrier(group)
rank 212 After initialize model
rank 212 before dist.barrier(group)
rank 2 After initialize model
rank 2 before dist.barrier(group)
rank 211 After initialize model
rank 211 before dist.barrier(group)
rank 214 After initialize model
rank 214 before dist.barrier(group)
rank 210 After initialize model
rank 210 before dist.barrier(group)
rank 88 After initialize model
rank 88 before dist.barrier(group)
rank 184 After initialize model
rank 184 before dist.barrier(group)
rank 24 After initialize model
rank 24 before dist.barrier(group)
rank 6 After initialize model
rank 3 After initialize model
rank 6 before dist.barrier(group)
rank 3 before dist.barrier(group)
rank 4 After initialize model
rank 4 before dist.barrier(group)
rank 25 After initialize model
rank 25 before dist.barrier(group)
rank 240 After initialize model
rank 240 before dist.barrier(group)
rank 113 After initialize model
rank 113 before dist.barrier(group)
rank 241 After initialize model
rank 241 before dist.barrier(group)
rank 215 After initialize model
rank 215 before dist.barrier(group)
rank 213 After initialize model
rank 213 before dist.barrier(group)
rank 112 After initialize model
rank 112 before dist.barrier(group)
rank 107 After initialize model
rank 107 before dist.barrier(group)
rank 5 After initialize model
rank 5 before dist.barrier(group)
rank 106 After initialize model
rank 106 before dist.barrier(group)
rank 40 After initialize model
rank 40 before dist.barrier(group)
rank 186 After initialize model
rank 186 before dist.barrier(group)
rank 111 After initialize model
rank 111 before dist.barrier(group)
rank 73 After initialize model
rank 73 before dist.barrier(group)
rank 72 After initialize model
rank 72 before dist.barrier(group)
rank 27 After initialize model
rank 27 before dist.barrier(group)
rank 115 After initialize model
rank 115 before dist.barrier(group)
rank 41 After initialize model
rank 41 before dist.barrier(group)
rank 7 After initialize model
rank 7 before dist.barrier(group)
rank 91 After initialize model
rank 91 before dist.barrier(group)
rank 187 After initialize model
rank 187 before dist.barrier(group)
rank 29 After initialize model
rank 29 before dist.barrier(group)
rank 242 After initialize model
rank 242 before dist.barrier(group)
rank 94 After initialize model
rank 94 before dist.barrier(group)
rank 28 After initialize model
rank 26 After initialize model
rank 28 before dist.barrier(group)
rank 26 before dist.barrier(group)
rank 30 After initialize model
rank 30 before dist.barrier(group)
rank 188 After initialize model
rank 188 before dist.barrier(group)
rank 33 After initialize model
rank 33 before dist.barrier(group)
rank 108 After initialize model
rank 110 After initialize model
rank 110 before dist.barrier(group)
rank 108 before dist.barrier(group)
rank 109 After initialize model
rank 109 before dist.barrier(group)
rank 243 After initialize model
rank 243 before dist.barrier(group)
rank 191 After initialize model
rank 191 before dist.barrier(group)
rank 90 After initialize model
rank 90 before dist.barrier(group)
rank 244 After initialize model
rank 244 before dist.barrier(group)
rank 32 After initialize model
rank 32 before dist.barrier(group)
rank 92 After initialize model
rank 65 After initialize model
rank 92 before dist.barrier(group)
rank 65 before dist.barrier(group)
rank 246 After initialize model
rank 246 before dist.barrier(group)
rank 95 After initialize model
rank 95 before dist.barrier(group)
rank 189 After initialize model
rank 189 before dist.barrier(group)
rank 64 After initialize model
rank 64 before dist.barrier(group)
rank 31 After initialize model
rank 31 before dist.barrier(group)
rank 249 After initialize model
rank 249 before dist.barrier(group)
rank 114 After initialize model
rank 114 before dist.barrier(group)
rank 46 After initialize model
rank 46 before dist.barrier(group)
rank 190 After initialize model
rank 190 before dist.barrier(group)
rank 9 After initialize model
rank 9 before dist.barrier(group)
rank 8 After initialize model
rank 8 before dist.barrier(group)
rank 116 After initialize model
rank 116 before dist.barrier(group)
rank 118 After initialize model
rank 118 before dist.barrier(group)
rank 117 After initialize model
rank 117 before dist.barrier(group)
rank 245 After initialize model
rank 245 before dist.barrier(group)
rank 248 After initialize model
rank 248 before dist.barrier(group)
rank 76 After initialize model
rank 76 before dist.barrier(group)
rank 75 After initialize model
rank 75 before dist.barrier(group)
rank 247 After initialize model
rank 247 before dist.barrier(group)
rank 93 After initialize model
rank 93 before dist.barrier(group)
rank 119 After initialize model
rank 119 before dist.barrier(group)
rank 153 After initialize model
rank 153 before dist.barrier(group)
rank 74 After initialize model
rank 74 before dist.barrier(group)
rank 152 After initialize model
rank 152 before dist.barrier(group)
rank 154 After initialize model
rank 78 After initialize model
rank 154 before dist.barrier(group)
rank 78 before dist.barrier(group)
rank 34 After initialize model
rank 34 before dist.barrier(group)
rank 35 After initialize model
rank 35 before dist.barrier(group)
rank 79 After initialize model
rank 79 before dist.barrier(group)
rank 77 After initialize model
rank 77 before dist.barrier(group)
rank 137 After initialize model
rank 137 before dist.barrier(group)
rank 136 After initialize model
rank 136 before dist.barrier(group)
rank 250 After initialize model
rank 250 before dist.barrier(group)
rank 37 After initialize model
rank 37 before dist.barrier(group)
rank 39 After initialize model
rank 39 before dist.barrier(group)
rank 155 After initialize model
rank 155 before dist.barrier(group)
rank 49 After initialize model
rank 49 before dist.barrier(group)
rank 176 After initialize model
rank 176 before dist.barrier(group)
rank 36 After initialize model
rank 36 before dist.barrier(group)
rank 38 After initialize model
rank 38 before dist.barrier(group)
rank 0 After initialize model
resume from checkpoint was set to False. Pretrain from scratch.
rank 0 init_model_dict.keys() dict_keys(['var_embed', 'var_query', 'pos_embed', 'mask_token', 'decoder_pos_embed', 'token_embeds.0.proj.weight', 'token_embeds.0.proj.bias', 'token_embeds.1.proj.weight', 'token_embeds.1.proj.bias', 'token_embeds.2.proj.weight', 'token_embeds.2.proj.bias', 'token_embeds.3.proj.weight', 'token_embeds.3.proj.bias', 'token_embeds.4.proj.weight', 'token_embeds.4.proj.bias', 'token_embeds.5.proj.weight', 'token_embeds.5.proj.bias', 'token_embeds.6.proj.weight', 'token_embeds.6.proj.bias', 'token_embeds.7.proj.weight', 'token_embeds.7.proj.bias', 'token_embeds.8.proj.weight', 'token_embeds.8.proj.bias', 'token_embeds.9.proj.weight', 'token_embeds.9.proj.bias', 'token_embeds.10.proj.weight', 'token_embeds.10.proj.bias', 'token_embeds.11.proj.weight', 'token_embeds.11.proj.bias', 'token_embeds.12.proj.weight', 'token_embeds.12.proj.bias', 'token_embeds.13.proj.weight', 'token_embeds.13.proj.bias', 'token_embeds.14.proj.weight', 'token_embeds.14.proj.bias', 'token_embeds.15.proj.weight', 'token_embeds.15.proj.bias', 'token_embeds.16.proj.weight', 'token_embeds.16.proj.bias', 'token_embeds.17.proj.weight', 'token_embeds.17.proj.bias', 'token_embeds.18.proj.weight', 'token_embeds.18.proj.bias', 'token_embeds.19.proj.weight', 'token_embeds.19.proj.bias', 'token_embeds.20.proj.weight', 'token_embeds.20.proj.bias', 'token_embeds.21.proj.weight', 'token_embeds.21.proj.bias', 'token_embeds.22.proj.weight', 'token_embeds.22.proj.bias', 'token_embeds.23.proj.weight', 'token_embeds.23.proj.bias', 'token_embeds.24.proj.weight', 'token_embeds.24.proj.bias', 'token_embeds.25.proj.weight', 'token_embeds.25.proj.bias', 'token_embeds.26.proj.weight', 'token_embeds.26.proj.bias', 'token_embeds.27.proj.weight', 'token_embeds.27.proj.bias', 'token_embeds.28.proj.weight', 'token_embeds.28.proj.bias', 'token_embeds.29.proj.weight', 'token_embeds.29.proj.bias', 'token_embeds.30.proj.weight', 'token_embeds.30.proj.bias', 'token_embeds.31.proj.weight', 'token_embeds.31.proj.bias', 'token_embeds.32.proj.weight', 'token_embeds.32.proj.bias', 'token_embeds.33.proj.weight', 'token_embeds.33.proj.bias', 'token_embeds.34.proj.weight', 'token_embeds.34.proj.bias', 'token_embeds.35.proj.weight', 'token_embeds.35.proj.bias', 'token_embeds.36.proj.weight', 'token_embeds.36.proj.bias', 'token_embeds.37.proj.weight', 'token_embeds.37.proj.bias', 'token_embeds.38.proj.weight', 'token_embeds.38.proj.bias', 'token_embeds.39.proj.weight', 'token_embeds.39.proj.bias', 'token_embeds.40.proj.weight', 'token_embeds.40.proj.bias', 'token_embeds.41.proj.weight', 'token_embeds.41.proj.bias', 'token_embeds.42.proj.weight', 'token_embeds.42.proj.bias', 'token_embeds.43.proj.weight', 'token_embeds.43.proj.bias', 'token_embeds.44.proj.weight', 'token_embeds.44.proj.bias', 'token_embeds.45.proj.weight', 'token_embeds.45.proj.bias', 'token_embeds.46.proj.weight', 'token_embeds.46.proj.bias', 'token_embeds.47.proj.weight', 'token_embeds.47.proj.bias', 'token_embeds.48.proj.weight', 'token_embeds.48.proj.bias', 'token_embeds.49.proj.weight', 'token_embeds.49.proj.bias', 'token_embeds.50.proj.weight', 'token_embeds.50.proj.bias', 'token_embeds.51.proj.weight', 'token_embeds.51.proj.bias', 'token_embeds.52.proj.weight', 'token_embeds.52.proj.bias', 'token_embeds.53.proj.weight', 'token_embeds.53.proj.bias', 'token_embeds.54.proj.weight', 'token_embeds.54.proj.bias', 'token_embeds.55.proj.weight', 'token_embeds.55.proj.bias', 'token_embeds.56.proj.weight', 'token_embeds.56.proj.bias', 'token_embeds.57.proj.weight', 'token_embeds.57.proj.bias', 'token_embeds.58.proj.weight', 'token_embeds.58.proj.bias', 'token_embeds.59.proj.weight', 'token_embeds.59.proj.bias', 'token_embeds.60.proj.weight', 'token_embeds.60.proj.bias', 'token_embeds.61.proj.weight', 'token_embeds.61.proj.bias', 'token_embeds.62.proj.weight', 'token_embeds.62.proj.bias', 'token_embeds.63.proj.weight', 'token_embeds.63.proj.bias', 'token_embeds.64.proj.weight', 'token_embeds.64.proj.bias', 'token_embeds.65.proj.weight', 'token_embeds.65.proj.bias', 'token_embeds.66.proj.weight', 'token_embeds.66.proj.bias', 'token_embeds.67.proj.weight', 'token_embeds.67.proj.bias', 'token_embeds.68.proj.weight', 'token_embeds.68.proj.bias', 'token_embeds.69.proj.weight', 'token_embeds.69.proj.bias', 'token_embeds.70.proj.weight', 'token_embeds.70.proj.bias', 'token_embeds.71.proj.weight', 'token_embeds.71.proj.bias', 'token_embeds.72.proj.weight', 'token_embeds.72.proj.bias', 'token_embeds.73.proj.weight', 'token_embeds.73.proj.bias', 'token_embeds.74.proj.weight', 'token_embeds.74.proj.bias', 'token_embeds.75.proj.weight', 'token_embeds.75.proj.bias', 'token_embeds.76.proj.weight', 'token_embeds.76.proj.bias', 'token_embeds.77.proj.weight', 'token_embeds.77.proj.bias', 'token_embeds.78.proj.weight', 'token_embeds.78.proj.bias', 'token_embeds.79.proj.weight', 'token_embeds.79.proj.bias', 'token_embeds.80.proj.weight', 'token_embeds.80.proj.bias', 'token_embeds.81.proj.weight', 'token_embeds.81.proj.bias', 'token_embeds.82.proj.weight', 'token_embeds.82.proj.bias', 'token_embeds.83.proj.weight', 'token_embeds.83.proj.bias', 'token_embeds.84.proj.weight', 'token_embeds.84.proj.bias', 'token_embeds.85.proj.weight', 'token_embeds.85.proj.bias', 'token_embeds.86.proj.weight', 'token_embeds.86.proj.bias', 'token_embeds.87.proj.weight', 'token_embeds.87.proj.bias', 'token_embeds.88.proj.weight', 'token_embeds.88.proj.bias', 'token_embeds.89.proj.weight', 'token_embeds.89.proj.bias', 'token_embeds.90.proj.weight', 'token_embeds.90.proj.bias', 'token_embeds.91.proj.weight', 'token_embeds.91.proj.bias', 'token_embeds.92.proj.weight', 'token_embeds.92.proj.bias', 'token_embeds.93.proj.weight', 'token_embeds.93.proj.bias', 'token_embeds.94.proj.weight', 'token_embeds.94.proj.bias', 'token_embeds.95.proj.weight', 'token_embeds.95.proj.bias', 'token_embeds.96.proj.weight', 'token_embeds.96.proj.bias', 'token_embeds.97.proj.weight', 'token_embeds.97.proj.bias', 'token_embeds.98.proj.weight', 'token_embeds.98.proj.bias', 'token_embeds.99.proj.weight', 'token_embeds.99.proj.bias', 'token_embeds.100.proj.weight', 'token_embeds.100.proj.bias', 'token_embeds.101.proj.weight', 'token_embeds.101.proj.bias', 'token_embeds.102.proj.weight', 'token_embeds.102.proj.bias', 'token_embeds.103.proj.weight', 'token_embeds.103.proj.bias', 'token_embeds.104.proj.weight', 'token_embeds.104.proj.bias', 'token_embeds.105.proj.weight', 'token_embeds.105.proj.bias', 'token_embeds.106.proj.weight', 'token_embeds.106.proj.bias', 'token_embeds.107.proj.weight', 'token_embeds.107.proj.bias', 'token_embeds.108.proj.weight', 'token_embeds.108.proj.bias', 'token_embeds.109.proj.weight', 'token_embeds.109.proj.bias', 'token_embeds.110.proj.weight', 'token_embeds.110.proj.bias', 'token_embeds.111.proj.weight', 'token_embeds.111.proj.bias', 'token_embeds.112.proj.weight', 'token_embeds.112.proj.bias', 'token_embeds.113.proj.weight', 'token_embeds.113.proj.bias', 'token_embeds.114.proj.weight', 'token_embeds.114.proj.bias', 'token_embeds.115.proj.weight', 'token_embeds.115.proj.bias', 'token_embeds.116.proj.weight', 'token_embeds.116.proj.bias', 'token_embeds.117.proj.weight', 'token_embeds.117.proj.bias', 'token_embeds.118.proj.weight', 'token_embeds.118.proj.bias', 'token_embeds.119.proj.weight', 'token_embeds.119.proj.bias', 'token_embeds.120.proj.weight', 'token_embeds.120.proj.bias', 'token_embeds.121.proj.weight', 'token_embeds.121.proj.bias', 'token_embeds.122.proj.weight', 'token_embeds.122.proj.bias', 'token_embeds.123.proj.weight', 'token_embeds.123.proj.bias', 'token_embeds.124.proj.weight', 'token_embeds.124.proj.bias', 'token_embeds.125.proj.weight', 'token_embeds.125.proj.bias', 'token_embeds.126.proj.weight', 'token_embeds.126.proj.bias', 'token_embeds.127.proj.weight', 'token_embeds.127.proj.bias', 'token_embeds.128.proj.weight', 'token_embeds.128.proj.bias', 'token_embeds.129.proj.weight', 'token_embeds.129.proj.bias', 'token_embeds.130.proj.weight', 'token_embeds.130.proj.bias', 'token_embeds.131.proj.weight', 'token_embeds.131.proj.bias', 'token_embeds.132.proj.weight', 'token_embeds.132.proj.bias', 'token_embeds.133.proj.weight', 'token_embeds.133.proj.bias', 'token_embeds.134.proj.weight', 'token_embeds.134.proj.bias', 'token_embeds.135.proj.weight', 'token_embeds.135.proj.bias', 'token_embeds.136.proj.weight', 'token_embeds.136.proj.bias', 'token_embeds.137.proj.weight', 'token_embeds.137.proj.bias', 'token_embeds.138.proj.weight', 'token_embeds.138.proj.bias', 'token_embeds.139.proj.weight', 'token_embeds.139.proj.bias', 'token_embeds.140.proj.weight', 'token_embeds.140.proj.bias', 'token_embeds.141.proj.weight', 'token_embeds.141.proj.bias', 'token_embeds.142.proj.weight', 'token_embeds.142.proj.bias', 'token_embeds.143.proj.weight', 'token_embeds.143.proj.bias', 'token_embeds.144.proj.weight', 'token_embeds.144.proj.bias', 'token_embeds.145.proj.weight', 'token_embeds.145.proj.bias', 'token_embeds.146.proj.weight', 'token_embeds.146.proj.bias', 'token_embeds.147.proj.weight', 'token_embeds.147.proj.bias', 'token_embeds.148.proj.weight', 'token_embeds.148.proj.bias', 'token_embeds.149.proj.weight', 'token_embeds.149.proj.bias', 'token_embeds.150.proj.weight', 'token_embeds.150.proj.bias', 'token_embeds.151.proj.weight', 'token_embeds.151.proj.bias', 'token_embeds.152.proj.weight', 'token_embeds.152.proj.bias', 'token_embeds.153.proj.weight', 'token_embeds.153.proj.bias', 'token_embeds.154.proj.weight', 'token_embeds.154.proj.bias', 'token_embeds.155.proj.weight', 'token_embeds.155.proj.bias', 'token_embeds.156.proj.weight', 'token_embeds.156.proj.bias', 'token_embeds.157.proj.weight', 'token_embeds.157.proj.bias', 'token_embeds.158.proj.weight', 'token_embeds.158.proj.bias', 'token_embeds.159.proj.weight', 'token_embeds.159.proj.bias', 'token_embeds.160.proj.weight', 'token_embeds.160.proj.bias', 'token_embeds.161.proj.weight', 'token_embeds.161.proj.bias', 'token_embeds.162.proj.weight', 'token_embeds.162.proj.bias', 'token_embeds.163.proj.weight', 'token_embeds.163.proj.bias', 'token_embeds.164.proj.weight', 'token_embeds.164.proj.bias', 'token_embeds.165.proj.weight', 'token_embeds.165.proj.bias', 'token_embeds.166.proj.weight', 'token_embeds.166.proj.bias', 'token_embeds.167.proj.weight', 'token_embeds.167.proj.bias', 'token_embeds.168.proj.weight', 'token_embeds.168.proj.bias', 'token_embeds.169.proj.weight', 'token_embeds.169.proj.bias', 'token_embeds.170.proj.weight', 'token_embeds.170.proj.bias', 'token_embeds.171.proj.weight', 'token_embeds.171.proj.bias', 'token_embeds.172.proj.weight', 'token_embeds.172.proj.bias', 'token_embeds.173.proj.weight', 'token_embeds.173.proj.bias', 'token_embeds.174.proj.weight', 'token_embeds.174.proj.bias', 'token_embeds.175.proj.weight', 'token_embeds.175.proj.bias', 'token_embeds.176.proj.weight', 'token_embeds.176.proj.bias', 'token_embeds.177.proj.weight', 'token_embeds.177.proj.bias', 'token_embeds.178.proj.weight', 'token_embeds.178.proj.bias', 'token_embeds.179.proj.weight', 'token_embeds.179.proj.bias', 'token_embeds.180.proj.weight', 'token_embeds.180.proj.bias', 'token_embeds.181.proj.weight', 'token_embeds.181.proj.bias', 'token_embeds.182.proj.weight', 'token_embeds.182.proj.bias', 'token_embeds.183.proj.weight', 'token_embeds.183.proj.bias', 'token_embeds.184.proj.weight', 'token_embeds.184.proj.bias', 'token_embeds.185.proj.weight', 'token_embeds.185.proj.bias', 'token_embeds.186.proj.weight', 'token_embeds.186.proj.bias', 'token_embeds.187.proj.weight', 'token_embeds.187.proj.bias', 'token_embeds.188.proj.weight', 'token_embeds.188.proj.bias', 'token_embeds.189.proj.weight', 'token_embeds.189.proj.bias', 'token_embeds.190.proj.weight', 'token_embeds.190.proj.bias', 'token_embeds.191.proj.weight', 'token_embeds.191.proj.bias', 'token_embeds.192.proj.weight', 'token_embeds.192.proj.bias', 'token_embeds.193.proj.weight', 'token_embeds.193.proj.bias', 'token_embeds.194.proj.weight', 'token_embeds.194.proj.bias', 'token_embeds.195.proj.weight', 'token_embeds.195.proj.bias', 'token_embeds.196.proj.weight', 'token_embeds.196.proj.bias', 'token_embeds.197.proj.weight', 'token_embeds.197.proj.bias', 'token_embeds.198.proj.weight', 'token_embeds.198.proj.bias', 'token_embeds.199.proj.weight', 'token_embeds.199.proj.bias', 'token_embeds.200.proj.weight', 'token_embeds.200.proj.bias', 'token_embeds.201.proj.weight', 'token_embeds.201.proj.bias', 'token_embeds.202.proj.weight', 'token_embeds.202.proj.bias', 'token_embeds.203.proj.weight', 'token_embeds.203.proj.bias', 'token_embeds.204.proj.weight', 'token_embeds.204.proj.bias', 'token_embeds.205.proj.weight', 'token_embeds.205.proj.bias', 'token_embeds.206.proj.weight', 'token_embeds.206.proj.bias', 'token_embeds.207.proj.weight', 'token_embeds.207.proj.bias', 'token_embeds.208.proj.weight', 'token_embeds.208.proj.bias', 'token_embeds.209.proj.weight', 'token_embeds.209.proj.bias', 'token_embeds.210.proj.weight', 'token_embeds.210.proj.bias', 'token_embeds.211.proj.weight', 'token_embeds.211.proj.bias', 'token_embeds.212.proj.weight', 'token_embeds.212.proj.bias', 'token_embeds.213.proj.weight', 'token_embeds.213.proj.bias', 'token_embeds.214.proj.weight', 'token_embeds.214.proj.bias', 'token_embeds.215.proj.weight', 'token_embeds.215.proj.bias', 'token_embeds.216.proj.weight', 'token_embeds.216.proj.bias', 'token_embeds.217.proj.weight', 'token_embeds.217.proj.bias', 'token_embeds.218.proj.weight', 'token_embeds.218.proj.bias', 'token_embeds.219.proj.weight', 'token_embeds.219.proj.bias', 'token_embeds.220.proj.weight', 'token_embeds.220.proj.bias', 'token_embeds.221.proj.weight', 'token_embeds.221.proj.bias', 'token_embeds.222.proj.weight', 'token_embeds.222.proj.bias', 'token_embeds.223.proj.weight', 'token_embeds.223.proj.bias', 'token_embeds.224.proj.weight', 'token_embeds.224.proj.bias', 'token_embeds.225.proj.weight', 'token_embeds.225.proj.bias', 'token_embeds.226.proj.weight', 'token_embeds.226.proj.bias', 'token_embeds.227.proj.weight', 'token_embeds.227.proj.bias', 'token_embeds.228.proj.weight', 'token_embeds.228.proj.bias', 'token_embeds.229.proj.weight', 'token_embeds.229.proj.bias', 'token_embeds.230.proj.weight', 'token_embeds.230.proj.bias', 'token_embeds.231.proj.weight', 'token_embeds.231.proj.bias', 'token_embeds.232.proj.weight', 'token_embeds.232.proj.bias', 'token_embeds.233.proj.weight', 'token_embeds.233.proj.bias', 'token_embeds.234.proj.weight', 'token_embeds.234.proj.bias', 'token_embeds.235.proj.weight', 'token_embeds.235.proj.bias', 'token_embeds.236.proj.weight', 'token_embeds.236.proj.bias', 'token_embeds.237.proj.weight', 'token_embeds.237.proj.bias', 'token_embeds.238.proj.weight', 'token_embeds.238.proj.bias', 'token_embeds.239.proj.weight', 'token_embeds.239.proj.bias', 'token_embeds.240.proj.weight', 'token_embeds.240.proj.bias', 'token_embeds.241.proj.weight', 'token_embeds.241.proj.bias', 'token_embeds.242.proj.weight', 'token_embeds.242.proj.bias', 'token_embeds.243.proj.weight', 'token_embeds.243.proj.bias', 'token_embeds.244.proj.weight', 'token_embeds.244.proj.bias', 'token_embeds.245.proj.weight', 'token_embeds.245.proj.bias', 'token_embeds.246.proj.weight', 'token_embeds.246.proj.bias', 'token_embeds.247.proj.weight', 'token_embeds.247.proj.bias', 'token_embeds.248.proj.weight', 'token_embeds.248.proj.bias', 'token_embeds.249.proj.weight', 'token_embeds.249.proj.bias', 'token_embeds.250.proj.weight', 'token_embeds.250.proj.bias', 'token_embeds.251.proj.weight', 'token_embeds.251.proj.bias', 'token_embeds.252.proj.weight', 'token_embeds.252.proj.bias', 'token_embeds.253.proj.weight', 'token_embeds.253.proj.bias', 'token_embeds.254.proj.weight', 'token_embeds.254.proj.bias', 'token_embeds.255.proj.weight', 'token_embeds.255.proj.bias', 'token_embeds.256.proj.weight', 'token_embeds.256.proj.bias', 'token_embeds.257.proj.weight', 'token_embeds.257.proj.bias', 'token_embeds.258.proj.weight', 'token_embeds.258.proj.bias', 'token_embeds.259.proj.weight', 'token_embeds.259.proj.bias', 'token_embeds.260.proj.weight', 'token_embeds.260.proj.bias', 'token_embeds.261.proj.weight', 'token_embeds.261.proj.bias', 'token_embeds.262.proj.weight', 'token_embeds.262.proj.bias', 'token_embeds.263.proj.weight', 'token_embeds.263.proj.bias', 'token_embeds.264.proj.weight', 'token_embeds.264.proj.bias', 'token_embeds.265.proj.weight', 'token_embeds.265.proj.bias', 'token_embeds.266.proj.weight', 'token_embeds.266.proj.bias', 'token_embeds.267.proj.weight', 'token_embeds.267.proj.bias', 'token_embeds.268.proj.weight', 'token_embeds.268.proj.bias', 'token_embeds.269.proj.weight', 'token_embeds.269.proj.bias', 'token_embeds.270.proj.weight', 'token_embeds.270.proj.bias', 'token_embeds.271.proj.weight', 'token_embeds.271.proj.bias', 'token_embeds.272.proj.weight', 'token_embeds.272.proj.bias', 'token_embeds.273.proj.weight', 'token_embeds.273.proj.bias', 'token_embeds.274.proj.weight', 'token_embeds.274.proj.bias', 'token_embeds.275.proj.weight', 'token_embeds.275.proj.bias', 'token_embeds.276.proj.weight', 'token_embeds.276.proj.bias', 'token_embeds.277.proj.weight', 'token_embeds.277.proj.bias', 'token_embeds.278.proj.weight', 'token_embeds.278.proj.bias', 'token_embeds.279.proj.weight', 'token_embeds.279.proj.bias', 'token_embeds.280.proj.weight', 'token_embeds.280.proj.bias', 'token_embeds.281.proj.weight', 'token_embeds.281.proj.bias', 'token_embeds.282.proj.weight', 'token_embeds.282.proj.bias', 'token_embeds.283.proj.weight', 'token_embeds.283.proj.bias', 'token_embeds.284.proj.weight', 'token_embeds.284.proj.bias', 'token_embeds.285.proj.weight', 'token_embeds.285.proj.bias', 'token_embeds.286.proj.weight', 'token_embeds.286.proj.bias', 'token_embeds.287.proj.weight', 'token_embeds.287.proj.bias', 'token_embeds.288.proj.weight', 'token_embeds.288.proj.bias', 'token_embeds.289.proj.weight', 'token_embeds.289.proj.bias', 'token_embeds.290.proj.weight', 'token_embeds.290.proj.bias', 'token_embeds.291.proj.weight', 'token_embeds.291.proj.bias', 'token_embeds.292.proj.weight', 'token_embeds.292.proj.bias', 'token_embeds.293.proj.weight', 'token_embeds.293.proj.bias', 'token_embeds.294.proj.weight', 'token_embeds.294.proj.bias', 'token_embeds.295.proj.weight', 'token_embeds.295.proj.bias', 'token_embeds.296.proj.weight', 'token_embeds.296.proj.bias', 'token_embeds.297.proj.weight', 'token_embeds.297.proj.bias', 'token_embeds.298.proj.weight', 'token_embeds.298.proj.bias', 'token_embeds.299.proj.weight', 'token_embeds.299.proj.bias', 'token_embeds.300.proj.weight', 'token_embeds.300.proj.bias', 'token_embeds.301.proj.weight', 'token_embeds.301.proj.bias', 'token_embeds.302.proj.weight', 'token_embeds.302.proj.bias', 'token_embeds.303.proj.weight', 'token_embeds.303.proj.bias', 'token_embeds.304.proj.weight', 'token_embeds.304.proj.bias', 'token_embeds.305.proj.weight', 'token_embeds.305.proj.bias', 'token_embeds.306.proj.weight', 'token_embeds.306.proj.bias', 'token_embeds.307.proj.weight', 'token_embeds.307.proj.bias', 'token_embeds.308.proj.weight', 'token_embeds.308.proj.bias', 'token_embeds.309.proj.weight', 'token_embeds.309.proj.bias', 'token_embeds.310.proj.weight', 'token_embeds.310.proj.bias', 'token_embeds.311.proj.weight', 'token_embeds.311.proj.bias', 'token_embeds.312.proj.weight', 'token_embeds.312.proj.bias', 'token_embeds.313.proj.weight', 'token_embeds.313.proj.bias', 'token_embeds.314.proj.weight', 'token_embeds.314.proj.bias', 'token_embeds.315.proj.weight', 'token_embeds.315.proj.bias', 'token_embeds.316.proj.weight', 'token_embeds.316.proj.bias', 'token_embeds.317.proj.weight', 'token_embeds.317.proj.bias', 'token_embeds.318.proj.weight', 'token_embeds.318.proj.bias', 'token_embeds.319.proj.weight', 'token_embeds.319.proj.bias', 'token_embeds.320.proj.weight', 'token_embeds.320.proj.bias', 'token_embeds.321.proj.weight', 'token_embeds.321.proj.bias', 'token_embeds.322.proj.weight', 'token_embeds.322.proj.bias', 'token_embeds.323.proj.weight', 'token_embeds.323.proj.bias', 'token_embeds.324.proj.weight', 'token_embeds.324.proj.bias', 'token_embeds.325.proj.weight', 'token_embeds.325.proj.bias', 'token_embeds.326.proj.weight', 'token_embeds.326.proj.bias', 'token_embeds.327.proj.weight', 'token_embeds.327.proj.bias', 'token_embeds.328.proj.weight', 'token_embeds.328.proj.bias', 'token_embeds.329.proj.weight', 'token_embeds.329.proj.bias', 'token_embeds.330.proj.weight', 'token_embeds.330.proj.bias', 'token_embeds.331.proj.weight', 'token_embeds.331.proj.bias', 'token_embeds.332.proj.weight', 'token_embeds.332.proj.bias', 'token_embeds.333.proj.weight', 'token_embeds.333.proj.bias', 'token_embeds.334.proj.weight', 'token_embeds.334.proj.bias', 'token_embeds.335.proj.weight', 'token_embeds.335.proj.bias', 'token_embeds.336.proj.weight', 'token_embeds.336.proj.bias', 'token_embeds.337.proj.weight', 'token_embeds.337.proj.bias', 'token_embeds.338.proj.weight', 'token_embeds.338.proj.bias', 'token_embeds.339.proj.weight', 'token_embeds.339.proj.bias', 'token_embeds.340.proj.weight', 'token_embeds.340.proj.bias', 'token_embeds.341.proj.weight', 'token_embeds.341.proj.bias', 'token_embeds.342.proj.weight', 'token_embeds.342.proj.bias', 'token_embeds.343.proj.weight', 'token_embeds.343.proj.bias', 'token_embeds.344.proj.weight', 'token_embeds.344.proj.bias', 'token_embeds.345.proj.weight', 'token_embeds.345.proj.bias', 'token_embeds.346.proj.weight', 'token_embeds.346.proj.bias', 'token_embeds.347.proj.weight', 'token_embeds.347.proj.bias', 'token_embeds.348.proj.weight', 'token_embeds.348.proj.bias', 'token_embeds.349.proj.weight', 'token_embeds.349.proj.bias', 'token_embeds.350.proj.weight', 'token_embeds.350.proj.bias', 'token_embeds.351.proj.weight', 'token_embeds.351.proj.bias', 'token_embeds.352.proj.weight', 'token_embeds.352.proj.bias', 'token_embeds.353.proj.weight', 'token_embeds.353.proj.bias', 'token_embeds.354.proj.weight', 'token_embeds.354.proj.bias', 'token_embeds.355.proj.weight', 'token_embeds.355.proj.bias', 'token_embeds.356.proj.weight', 'token_embeds.356.proj.bias', 'token_embeds.357.proj.weight', 'token_embeds.357.proj.bias', 'token_embeds.358.proj.weight', 'token_embeds.358.proj.bias', 'token_embeds.359.proj.weight', 'token_embeds.359.proj.bias', 'token_embeds.360.proj.weight', 'token_embeds.360.proj.bias', 'token_embeds.361.proj.weight', 'token_embeds.361.proj.bias', 'token_embeds.362.proj.weight', 'token_embeds.362.proj.bias', 'token_embeds.363.proj.weight', 'token_embeds.363.proj.bias', 'token_embeds.364.proj.weight', 'token_embeds.364.proj.bias', 'token_embeds.365.proj.weight', 'token_embeds.365.proj.bias', 'token_embeds.366.proj.weight', 'token_embeds.366.proj.bias', 'token_embeds.367.proj.weight', 'token_embeds.367.proj.bias', 'token_embeds.368.proj.weight', 'token_embeds.368.proj.bias', 'token_embeds.369.proj.weight', 'token_embeds.369.proj.bias', 'token_embeds.370.proj.weight', 'token_embeds.370.proj.bias', 'token_embeds.371.proj.weight', 'token_embeds.371.proj.bias', 'token_embeds.372.proj.weight', 'token_embeds.372.proj.bias', 'token_embeds.373.proj.weight', 'token_embeds.373.proj.bias', 'token_embeds.374.proj.weight', 'token_embeds.374.proj.bias', 'token_embeds.375.proj.weight', 'token_embeds.375.proj.bias', 'token_embeds.376.proj.weight', 'token_embeds.376.proj.bias', 'token_embeds.377.proj.weight', 'token_embeds.377.proj.bias', 'token_embeds.378.proj.weight', 'token_embeds.378.proj.bias', 'token_embeds.379.proj.weight', 'token_embeds.379.proj.bias', 'token_embeds.380.proj.weight', 'token_embeds.380.proj.bias', 'token_embeds.381.proj.weight', 'token_embeds.381.proj.bias', 'token_embeds.382.proj.weight', 'token_embeds.382.proj.bias', 'token_embeds.383.proj.weight', 'token_embeds.383.proj.bias', 'token_embeds.384.proj.weight', 'token_embeds.384.proj.bias', 'token_embeds.385.proj.weight', 'token_embeds.385.proj.bias', 'token_embeds.386.proj.weight', 'token_embeds.386.proj.bias', 'token_embeds.387.proj.weight', 'token_embeds.387.proj.bias', 'token_embeds.388.proj.weight', 'token_embeds.388.proj.bias', 'token_embeds.389.proj.weight', 'token_embeds.389.proj.bias', 'token_embeds.390.proj.weight', 'token_embeds.390.proj.bias', 'token_embeds.391.proj.weight', 'token_embeds.391.proj.bias', 'token_embeds.392.proj.weight', 'token_embeds.392.proj.bias', 'token_embeds.393.proj.weight', 'token_embeds.393.proj.bias', 'token_embeds.394.proj.weight', 'token_embeds.394.proj.bias', 'token_embeds.395.proj.weight', 'token_embeds.395.proj.bias', 'token_embeds.396.proj.weight', 'token_embeds.396.proj.bias', 'token_embeds.397.proj.weight', 'token_embeds.397.proj.bias', 'token_embeds.398.proj.weight', 'token_embeds.398.proj.bias', 'token_embeds.399.proj.weight', 'token_embeds.399.proj.bias', 'token_embeds.400.proj.weight', 'token_embeds.400.proj.bias', 'token_embeds.401.proj.weight', 'token_embeds.401.proj.bias', 'token_embeds.402.proj.weight', 'token_embeds.402.proj.bias', 'token_embeds.403.proj.weight', 'token_embeds.403.proj.bias', 'token_embeds.404.proj.weight', 'token_embeds.404.proj.bias', 'token_embeds.405.proj.weight', 'token_embeds.405.proj.bias', 'token_embeds.406.proj.weight', 'token_embeds.406.proj.bias', 'token_embeds.407.proj.weight', 'token_embeds.407.proj.bias', 'token_embeds.408.proj.weight', 'token_embeds.408.proj.bias', 'token_embeds.409.proj.weight', 'token_embeds.409.proj.bias', 'token_embeds.410.proj.weight', 'token_embeds.410.proj.bias', 'token_embeds.411.proj.weight', 'token_embeds.411.proj.bias', 'token_embeds.412.proj.weight', 'token_embeds.412.proj.bias', 'token_embeds.413.proj.weight', 'token_embeds.413.proj.bias', 'token_embeds.414.proj.weight', 'token_embeds.414.proj.bias', 'token_embeds.415.proj.weight', 'token_embeds.415.proj.bias', 'token_embeds.416.proj.weight', 'token_embeds.416.proj.bias', 'token_embeds.417.proj.weight', 'token_embeds.417.proj.bias', 'token_embeds.418.proj.weight', 'token_embeds.418.proj.bias', 'token_embeds.419.proj.weight', 'token_embeds.419.proj.bias', 'token_embeds.420.proj.weight', 'token_embeds.420.proj.bias', 'token_embeds.421.proj.weight', 'token_embeds.421.proj.bias', 'token_embeds.422.proj.weight', 'token_embeds.422.proj.bias', 'token_embeds.423.proj.weight', 'token_embeds.423.proj.bias', 'token_embeds.424.proj.weight', 'token_embeds.424.proj.bias', 'token_embeds.425.proj.weight', 'token_embeds.425.proj.bias', 'token_embeds.426.proj.weight', 'token_embeds.426.proj.bias', 'token_embeds.427.proj.weight', 'token_embeds.427.proj.bias', 'token_embeds.428.proj.weight', 'token_embeds.428.proj.bias', 'token_embeds.429.proj.weight', 'token_embeds.429.proj.bias', 'token_embeds.430.proj.weight', 'token_embeds.430.proj.bias', 'token_embeds.431.proj.weight', 'token_embeds.431.proj.bias', 'token_embeds.432.proj.weight', 'token_embeds.432.proj.bias', 'token_embeds.433.proj.weight', 'token_embeds.433.proj.bias', 'token_embeds.434.proj.weight', 'token_embeds.434.proj.bias', 'token_embeds.435.proj.weight', 'token_embeds.435.proj.bias', 'token_embeds.436.proj.weight', 'token_embeds.436.proj.bias', 'token_embeds.437.proj.weight', 'token_embeds.437.proj.bias', 'token_embeds.438.proj.weight', 'token_embeds.438.proj.bias', 'token_embeds.439.proj.weight', 'token_embeds.439.proj.bias', 'token_embeds.440.proj.weight', 'token_embeds.440.proj.bias', 'token_embeds.441.proj.weight', 'token_embeds.441.proj.bias', 'token_embeds.442.proj.weight', 'token_embeds.442.proj.bias', 'token_embeds.443.proj.weight', 'token_embeds.443.proj.bias', 'token_embeds.444.proj.weight', 'token_embeds.444.proj.bias', 'token_embeds.445.proj.weight', 'token_embeds.445.proj.bias', 'token_embeds.446.proj.weight', 'token_embeds.446.proj.bias', 'token_embeds.447.proj.weight', 'token_embeds.447.proj.bias', 'token_embeds.448.proj.weight', 'token_embeds.448.proj.bias', 'token_embeds.449.proj.weight', 'token_embeds.449.proj.bias', 'token_embeds.450.proj.weight', 'token_embeds.450.proj.bias', 'token_embeds.451.proj.weight', 'token_embeds.451.proj.bias', 'token_embeds.452.proj.weight', 'token_embeds.452.proj.bias', 'token_embeds.453.proj.weight', 'token_embeds.453.proj.bias', 'token_embeds.454.proj.weight', 'token_embeds.454.proj.bias', 'token_embeds.455.proj.weight', 'token_embeds.455.proj.bias', 'token_embeds.456.proj.weight', 'token_embeds.456.proj.bias', 'token_embeds.457.proj.weight', 'token_embeds.457.proj.bias', 'token_embeds.458.proj.weight', 'token_embeds.458.proj.bias', 'token_embeds.459.proj.weight', 'token_embeds.459.proj.bias', 'token_embeds.460.proj.weight', 'token_embeds.460.proj.bias', 'token_embeds.461.proj.weight', 'token_embeds.461.proj.bias', 'token_embeds.462.proj.weight', 'token_embeds.462.proj.bias', 'token_embeds.463.proj.weight', 'token_embeds.463.proj.bias', 'token_embeds.464.proj.weight', 'token_embeds.464.proj.bias', 'token_embeds.465.proj.weight', 'token_embeds.465.proj.bias', 'token_embeds.466.proj.weight', 'token_embeds.466.proj.bias', 'token_embeds.467.proj.weight', 'token_embeds.467.proj.bias', 'token_embeds.468.proj.weight', 'token_embeds.468.proj.bias', 'token_embeds.469.proj.weight', 'token_embeds.469.proj.bias', 'token_embeds.470.proj.weight', 'token_embeds.470.proj.bias', 'token_embeds.471.proj.weight', 'token_embeds.471.proj.bias', 'token_embeds.472.proj.weight', 'token_embeds.472.proj.bias', 'token_embeds.473.proj.weight', 'token_embeds.473.proj.bias', 'token_embeds.474.proj.weight', 'token_embeds.474.proj.bias', 'token_embeds.475.proj.weight', 'token_embeds.475.proj.bias', 'token_embeds.476.proj.weight', 'token_embeds.476.proj.bias', 'token_embeds.477.proj.weight', 'token_embeds.477.proj.bias', 'token_embeds.478.proj.weight', 'token_embeds.478.proj.bias', 'token_embeds.479.proj.weight', 'token_embeds.479.proj.bias', 'token_embeds.480.proj.weight', 'token_embeds.480.proj.bias', 'token_embeds.481.proj.weight', 'token_embeds.481.proj.bias', 'token_embeds.482.proj.weight', 'token_embeds.482.proj.bias', 'token_embeds.483.proj.weight', 'token_embeds.483.proj.bias', 'token_embeds.484.proj.weight', 'token_embeds.484.proj.bias', 'token_embeds.485.proj.weight', 'token_embeds.485.proj.bias', 'token_embeds.486.proj.weight', 'token_embeds.486.proj.bias', 'token_embeds.487.proj.weight', 'token_embeds.487.proj.bias', 'token_embeds.488.proj.weight', 'token_embeds.488.proj.bias', 'token_embeds.489.proj.weight', 'token_embeds.489.proj.bias', 'token_embeds.490.proj.weight', 'token_embeds.490.proj.bias', 'token_embeds.491.proj.weight', 'token_embeds.491.proj.bias', 'token_embeds.492.proj.weight', 'token_embeds.492.proj.bias', 'token_embeds.493.proj.weight', 'token_embeds.493.proj.bias', 'token_embeds.494.proj.weight', 'token_embeds.494.proj.bias', 'token_embeds.495.proj.weight', 'token_embeds.495.proj.bias', 'lead_time_embed.weight', 'lead_time_embed.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.2.norm1.weight', 'blocks.2.norm1.bias', 'blocks.2.norm2.weight', 'blocks.2.norm2.bias', 'blocks.3.norm1.weight', 'blocks.3.norm1.bias', 'blocks.3.norm2.weight', 'blocks.3.norm2.bias', 'blocks.4.norm1.weight', 'blocks.4.norm1.bias', 'blocks.4.norm2.weight', 'blocks.4.norm2.bias', 'blocks.5.norm1.weight', 'blocks.5.norm1.bias', 'blocks.5.norm2.weight', 'blocks.5.norm2.bias', 'blocks.6.norm1.weight', 'blocks.6.norm1.bias', 'blocks.6.norm2.weight', 'blocks.6.norm2.bias', 'blocks.7.norm1.weight', 'blocks.7.norm1.bias', 'blocks.7.norm2.weight', 'blocks.7.norm2.bias', 'blocks.8.norm1.weight', 'blocks.8.norm1.bias', 'blocks.8.norm2.weight', 'blocks.8.norm2.bias', 'blocks.9.norm1.weight', 'blocks.9.norm1.bias', 'blocks.9.norm2.weight', 'blocks.9.norm2.bias', 'blocks.10.norm1.weight', 'blocks.10.norm1.bias', 'blocks.10.norm2.weight', 'blocks.10.norm2.bias', 'blocks.11.norm1.weight', 'blocks.11.norm1.bias', 'blocks.11.norm2.weight', 'blocks.11.norm2.bias', 'blocks.12.norm1.weight', 'blocks.12.norm1.bias', 'blocks.12.norm2.weight', 'blocks.12.norm2.bias', 'blocks.13.norm1.weight', 'blocks.13.norm1.bias', 'blocks.13.norm2.weight', 'blocks.13.norm2.bias', 'blocks.14.norm1.weight', 'blocks.14.norm1.bias', 'blocks.14.norm2.weight', 'blocks.14.norm2.bias', 'blocks.15.norm1.weight', 'blocks.15.norm1.bias', 'blocks.15.norm2.weight', 'blocks.15.norm2.bias', 'blocks.16.norm1.weight', 'blocks.16.norm1.bias', 'blocks.16.norm2.weight', 'blocks.16.norm2.bias', 'blocks.17.norm1.weight', 'blocks.17.norm1.bias', 'blocks.17.norm2.weight', 'blocks.17.norm2.bias', 'blocks.18.norm1.weight', 'blocks.18.norm1.bias', 'blocks.18.norm2.weight', 'blocks.18.norm2.bias', 'blocks.19.norm1.weight', 'blocks.19.norm1.bias', 'blocks.19.norm2.weight', 'blocks.19.norm2.bias', 'blocks.20.norm1.weight', 'blocks.20.norm1.bias', 'blocks.20.norm2.weight', 'blocks.20.norm2.bias', 'blocks.21.norm1.weight', 'blocks.21.norm1.bias', 'blocks.21.norm2.weight', 'blocks.21.norm2.bias', 'blocks.22.norm1.weight', 'blocks.22.norm1.bias', 'blocks.22.norm2.weight', 'blocks.22.norm2.bias', 'blocks.23.norm1.weight', 'blocks.23.norm1.bias', 'blocks.23.norm2.weight', 'blocks.23.norm2.bias', 'blocks.24.norm1.weight', 'blocks.24.norm1.bias', 'blocks.24.norm2.weight', 'blocks.24.norm2.bias', 'blocks.25.norm1.weight', 'blocks.25.norm1.bias', 'blocks.25.norm2.weight', 'blocks.25.norm2.bias', 'blocks.26.norm1.weight', 'blocks.26.norm1.bias', 'blocks.26.norm2.weight', 'blocks.26.norm2.bias', 'blocks.27.norm1.weight', 'blocks.27.norm1.bias', 'blocks.27.norm2.weight', 'blocks.27.norm2.bias', 'blocks.28.norm1.weight', 'blocks.28.norm1.bias', 'blocks.28.norm2.weight', 'blocks.28.norm2.bias', 'blocks.29.norm1.weight', 'blocks.29.norm1.bias', 'blocks.29.norm2.weight', 'blocks.29.norm2.bias', 'blocks.30.norm1.weight', 'blocks.30.norm1.bias', 'blocks.30.norm2.weight', 'blocks.30.norm2.bias', 'blocks.31.norm1.weight', 'blocks.31.norm1.bias', 'blocks.31.norm2.weight', 'blocks.31.norm2.bias', 'norm.weight', 'norm.bias', 'decoder_embed.weight', 'decoder_embed.bias', 'decoder_blocks.0.norm1.weight', 'decoder_blocks.0.norm1.bias', 'decoder_blocks.0.norm2.weight', 'decoder_blocks.0.norm2.bias', 'decoder_blocks.1.norm1.weight', 'decoder_blocks.1.norm1.bias', 'decoder_blocks.1.norm2.weight', 'decoder_blocks.1.norm2.bias', 'decoder_norm.weight', 'decoder_norm.bias', 'decoder_pred.weight', 'decoder_pred.bias'])
rank 251 After initialize model
rank 251 before dist.barrier(group)
rank 48 After initialize model
rank 48 before dist.barrier(group)
rank 11 After initialize model
rank 11 before dist.barrier(group)
rank 10 After initialize model
rank 10 before dist.barrier(group)
rank 156 After initialize model
rank 156 before dist.barrier(group)
rank 158 After initialize model
rank 158 before dist.barrier(group)
rank 67 After initialize model
rank 67 before dist.barrier(group)
rank 66 After initialize model
rank 66 before dist.barrier(group)
rank 159 After initialize model
rank 159 before dist.barrier(group)
rank 43 After initialize model
rank 43 before dist.barrier(group)
rank 12 After initialize model
rank 12 before dist.barrier(group)
rank 42 After initialize model
rank 42 before dist.barrier(group)
rank 157 After initialize model
rank 157 before dist.barrier(group)
rank 252 After initialize model
rank 252 before dist.barrier(group)
rank 253 After initialize model
rank 253 before dist.barrier(group)
rank 254 After initialize model
rank 254 before dist.barrier(group)
rank 160 After initialize model
rank 160 before dist.barrier(group)
rank 161 After initialize model
rank 161 before dist.barrier(group)
rank 13 After initialize model
rank 13 before dist.barrier(group)
rank 145 After initialize model
rank 145 before dist.barrier(group)
rank 144 After initialize model
rank 144 before dist.barrier(group)
rank 70 After initialize model
rank 70 before dist.barrier(group)
rank 71 After initialize model
rank 71 before dist.barrier(group)
rank 15 After initialize model
rank 15 before dist.barrier(group)
rank 255 After initialize model
rank 255 before dist.barrier(group)
rank 68 After initialize model
rank 68 before dist.barrier(group)
rank 69 After initialize model
rank 69 before dist.barrier(group)
rank 96 After initialize model
rank 96 before dist.barrier(group)
rank 97 After initialize model
rank 97 before dist.barrier(group)
rank 14 After initialize model
rank 14 before dist.barrier(group)
rank 47 After initialize model
rank 47 before dist.barrier(group)
rank 143 After initialize model
rank 143 before dist.barrier(group)
rank 225 After initialize model
rank 225 before dist.barrier(group)
rank 138 After initialize model
rank 138 before dist.barrier(group)
rank 139 After initialize model
rank 139 before dist.barrier(group)
rank 224 After initialize model
rank 224 before dist.barrier(group)
rank 50 After initialize model
rank 50 before dist.barrier(group)
rank 54 After initialize model
rank 54 before dist.barrier(group)
rank 44 After initialize model
rank 44 before dist.barrier(group)
rank 81 After initialize model
rank 81 before dist.barrier(group)
rank 51 After initialize model
rank 51 before dist.barrier(group)
rank 141 After initialize model
rank 141 before dist.barrier(group)
rank 142 After initialize model
rank 140 After initialize model
rank 142 before dist.barrier(group)
rank 140 before dist.barrier(group)
rank 80 After initialize model
rank 80 before dist.barrier(group)
rank 181 After initialize model
rank 181 before dist.barrier(group)
rank 162 After initialize model
rank 162 before dist.barrier(group)
rank 164 After initialize model
rank 164 before dist.barrier(group)
rank 52 After initialize model
rank 52 before dist.barrier(group)
rank 55 After initialize model
rank 55 before dist.barrier(group)
rank 163 After initialize model
rank 163 before dist.barrier(group)
rank 168 After initialize model
rank 168 before dist.barrier(group)
rank 166 After initialize model
rank 166 before dist.barrier(group)
rank 169 After initialize model
rank 169 before dist.barrier(group)
rank 99 After initialize model
rank 99 before dist.barrier(group)
rank 53 After initialize model
rank 53 before dist.barrier(group)
rank 178 After initialize model
rank 178 before dist.barrier(group)
rank 182 After initialize model
rank 182 before dist.barrier(group)
rank 98 After initialize model
rank 98 before dist.barrier(group)
rank 165 After initialize model
rank 165 before dist.barrier(group)
rank 167 After initialize model
rank 167 before dist.barrier(group)
rank 217 After initialize model
rank 217 before dist.barrier(group)
rank 216 After initialize model
rank 216 before dist.barrier(group)
rank 193 After initialize model
rank 193 before dist.barrier(group)
rank 102 After initialize model
rank 102 before dist.barrier(group)
rank 103 After initialize model
rank 103 before dist.barrier(group)
rank 100 After initialize model
rank 100 before dist.barrier(group)
rank 200 After initialize model
rank 200 before dist.barrier(group)
rank 150 After initialize model
rank 150 before dist.barrier(group)
rank 183 After initialize model
rank 183 before dist.barrier(group)
rank 170 After initialize model
rank 170 before dist.barrier(group)
rank 192 After initialize model
rank 192 before dist.barrier(group)
rank 171 After initialize model
rank 171 before dist.barrier(group)
rank 201 After initialize model
rank 201 before dist.barrier(group)
rank 101 After initialize model
rank 101 before dist.barrier(group)
rank 129 After initialize model
rank 129 before dist.barrier(group)
rank 233 After initialize model
rank 233 before dist.barrier(group)
rank 56 After initialize model
rank 56 before dist.barrier(group)
rank 82 After initialize model
rank 82 before dist.barrier(group)
rank 226 After initialize model
rank 226 before dist.barrier(group)
rank 227 After initialize model
rank 227 before dist.barrier(group)
rank 128 After initialize model
rank 128 before dist.barrier(group)
rank 172 After initialize model
rank 172 before dist.barrier(group)
rank 174 After initialize model
rank 174 before dist.barrier(group)
rank 17 After initialize model
rank 17 before dist.barrier(group)
rank 57 After initialize model
rank 57 before dist.barrier(group)
rank 83 After initialize model
rank 83 before dist.barrier(group)
rank 173 After initialize model
rank 16 After initialize model
rank 173 before dist.barrier(group)
rank 16 before dist.barrier(group)
rank 175 After initialize model
rank 175 before dist.barrier(group)
rank 198 After initialize model
rank 198 before dist.barrier(group)
rank 194 After initialize model
rank 194 before dist.barrier(group)
rank 230 After initialize model
rank 230 before dist.barrier(group)
rank 148 After initialize model
rank 148 before dist.barrier(group)
rank 149 After initialize model
rank 149 before dist.barrier(group)
rank 228 After initialize model
rank 228 before dist.barrier(group)
rank 84 After initialize model
rank 84 before dist.barrier(group)
rank 202 After initialize model
rank 202 before dist.barrier(group)
rank 85 After initialize model
rank 85 before dist.barrier(group)
rank 196 After initialize model
rank 196 before dist.barrier(group)
rank 231 After initialize model
rank 231 before dist.barrier(group)
rank 195 After initialize model
rank 195 before dist.barrier(group)
rank 199 After initialize model
rank 199 before dist.barrier(group)
rank 86 After initialize model
rank 86 before dist.barrier(group)
rank 219 After initialize model
rank 219 before dist.barrier(group)
rank 121 After initialize model
rank 121 before dist.barrier(group)
rank 229 After initialize model
rank 229 before dist.barrier(group)
rank 232 After initialize model
rank 232 before dist.barrier(group)
rank 87 After initialize model
rank 87 before dist.barrier(group)
rank 218 After initialize model
rank 218 before dist.barrier(group)
rank 134 After initialize model
rank 134 before dist.barrier(group)
rank 206 After initialize model
rank 206 before dist.barrier(group)
rank 222 After initialize model
rank 222 before dist.barrier(group)
rank 203 After initialize model
rank 203 before dist.barrier(group)
rank 197 After initialize model
rank 197 before dist.barrier(group)
rank 220 After initialize model
rank 220 before dist.barrier(group)
rank 235 After initialize model
rank 235 before dist.barrier(group)
rank 120 After initialize model
rank 120 before dist.barrier(group)
rank 223 After initialize model
rank 223 before dist.barrier(group)
rank 207 After initialize model
rank 207 before dist.barrier(group)
rank 234 After initialize model
rank 234 before dist.barrier(group)
rank 130 After initialize model
rank 130 before dist.barrier(group)
rank 237 After initialize model
rank 237 before dist.barrier(group)
rank 59 After initialize model
rank 59 before dist.barrier(group)
rank 204 After initialize model
rank 204 before dist.barrier(group)
rank 131 After initialize model
rank 131 before dist.barrier(group)
rank 238 After initialize model
rank 238 before dist.barrier(group)
rank 221 After initialize model
rank 221 before dist.barrier(group)
rank 236 After initialize model
rank 236 before dist.barrier(group)
rank 58 After initialize model
rank 58 before dist.barrier(group)
rank 62 After initialize model
rank 62 before dist.barrier(group)
rank 60 After initialize model
rank 60 before dist.barrier(group)
rank 20 After initialize model
rank 20 before dist.barrier(group)
rank 18 After initialize model
rank 18 before dist.barrier(group)
rank 205 After initialize model
rank 205 before dist.barrier(group)
rank 132 After initialize model
rank 132 before dist.barrier(group)
rank 22 After initialize model
rank 22 before dist.barrier(group)
rank 135 After initialize model
rank 135 before dist.barrier(group)
rank 61 After initialize model
rank 61 before dist.barrier(group)
rank 239 After initialize model
rank 239 before dist.barrier(group)
rank 133 After initialize model
rank 133 before dist.barrier(group)
rank 63 After initialize model
rank 63 before dist.barrier(group)
rank 21 After initialize model
rank 21 before dist.barrier(group)
rank 23 After initialize model
rank 23 before dist.barrier(group)
rank 19 After initialize model
rank 19 before dist.barrier(group)
rank 123 After initialize model
rank 123 before dist.barrier(group)
rank 126 After initialize model
rank 126 before dist.barrier(group)
rank 0 after torch.save for initial
rank 0 before dist.barrier(group)
rank 127 After initialize model
rank 127 before dist.barrier(group)
rank 124 After initialize model
rank 124 before dist.barrier(group)
rank 122 After initialize model
rank 122 before dist.barrier(group)
rank 125 After initialize model
rank 125 before dist.barrier(group)
rank 192 after dist.barrier(group)
rank 192 Before the second dist.barrier()
rank 128 after dist.barrier(group)
rank 128 Before the second dist.barrier()
rank 129 after dist.barrier(group)
rank 129 Before the second dist.barrier()
rank 193 after dist.barrier(group)
rank 193 Before the second dist.barrier()
rank 96 after dist.barrier(group)
rank 96 Before the second dist.barrier()
rank 232 after dist.barrier(group)
rank 232 Before the second dist.barrier()
rank 112 after dist.barrier(group)
rank 112 Before the second dist.barrier()
rank 113 after dist.barrier(group)
rank 113 Before the second dist.barrier()
rank 233 after dist.barrier(group)
rank 233 Before the second dist.barrier()
rank 132 after dist.barrier(group)
rank 132 Before the second dist.barrier()
rank 196 after dist.barrier(group)
rank 196 Before the second dist.barrier()
rank 133 after dist.barrier(group)
rank 133 Before the second dist.barrier()
rank 97 after dist.barrier(group)
rank 97 Before the second dist.barrier()
rank 197 after dist.barrier(group)
rank 197 Before the second dist.barrier()
rank 48 after dist.barrier(group)
rank 48 Before the second dist.barrier()
rank 236 after dist.barrier(group)
rank 236 Before the second dist.barrier()
rank 237 after dist.barrier(group)
rank 237 Before the second dist.barrier()
rank 224 after dist.barrier(group)
rank 224 Before the second dist.barrier()
rank 64 after dist.barrier(group)
rank 64 Before the second dist.barrier()
rank 144 after dist.barrier(group)
rank 144 Before the second dist.barrier()
rank 160 after dist.barrier(group)
rank 160 Before the second dist.barrier()
rank 239 after dist.barrier(group)
rank 239 Before the second dist.barrier()
rank 225 after dist.barrier(group)
rank 225 Before the second dist.barrier()
rank 248 after dist.barrier(group)
rank 248 Before the second dist.barrier()
rank 101 after dist.barrier(group)
rank 101 Before the second dist.barrier()
rank 65 after dist.barrier(group)
rank 65 Before the second dist.barrier()
rank 145 after dist.barrier(group)
rank 145 Before the second dist.barrier()
rank 161 after dist.barrier(group)
rank 161 Before the second dist.barrier()
rank 176 after dist.barrier(group)
rank 176 Before the second dist.barrier()
rank 249 after dist.barrier(group)
rank 249 Before the second dist.barrier()
rank 100 after dist.barrier(group)
rank 100 Before the second dist.barrier()
rank 165 after dist.barrier(group)
rank 120 after dist.barrier(group)
rank 120 Before the second dist.barrier()
rank 184 after dist.barrier(group)
rank 184 Before the second dist.barrier()
rank 177 after dist.barrier(group)
rank 177 Before the second dist.barrier()
rank 229 after dist.barrier(group)
rank 229 Before the second dist.barrier()
rank 165 Before the second dist.barrier()
rank 121 after dist.barrier(group)
rank 121 Before the second dist.barrier()
rank 185 after dist.barrier(group)
rank 185 Before the second dist.barrier()
rank 252 after dist.barrier(group)
rank 252 Before the second dist.barrier()
rank 164 after dist.barrier(group)
rank 164 Before the second dist.barrier()
rank 189 after dist.barrier(group)
rank 189 Before the second dist.barrier()
rank 253 after dist.barrier(group)
rank 253 Before the second dist.barrier()
rank 117 after dist.barrier(group)
rank 117 Before the second dist.barrier()
rank 49 after dist.barrier(group)
rank 49 Before the second dist.barrier()
rank 80 after dist.barrier(group)
rank 80 Before the second dist.barrier()
rank 69 after dist.barrier(group)
rank 69 Before the second dist.barrier()
rank 125 after dist.barrier(group)
rank 125 Before the second dist.barrier()
rank 188 after dist.barrier(group)
rank 188 Before the second dist.barrier()
rank 181 after dist.barrier(group)
rank 181 Before the second dist.barrier()
rank 234 after dist.barrier(group)
rank 234 Before the second dist.barrier()
rank 81 after dist.barrier(group)
rank 81 Before the second dist.barrier()
rank 88 after dist.barrier(group)
rank 88 Before the second dist.barrier()
rank 208 after dist.barrier(group)
rank 208 Before the second dist.barrier()
rank 217 after dist.barrier(group)
rank 217 Before the second dist.barrier()
rank 238 after dist.barrier(group)
rank 238 Before the second dist.barrier()
rank 104 after dist.barrier(group)
rank 104 Before the second dist.barrier()
rank 89 after dist.barrier(group)
rank 89 Before the second dist.barrier()
rank 56 after dist.barrier(group)
rank 56 Before the second dist.barrier()
rank 32 after dist.barrier(group)
rank 32 Before the second dist.barrier()
rank 209 after dist.barrier(group)
rank 209 Before the second dist.barrier()
rank 216 after dist.barrier(group)
rank 216 Before the second dist.barrier()
rank 228 after dist.barrier(group)
rank 152 after dist.barrier(group)
rank 152 Before the second dist.barrier()
rank 105 after dist.barrier(group)
rank 105 Before the second dist.barrier()
rank 134 after dist.barrier(group)
rank 134 Before the second dist.barrier()
rank 57 after dist.barrier(group)
rank 57 Before the second dist.barrier()
rank 33 after dist.barrier(group)
rank 33 Before the second dist.barrier()
rank 198 after dist.barrier(group)
rank 198 Before the second dist.barrier()
rank 168 after dist.barrier(group)
rank 168 Before the second dist.barrier()
rank 228 Before the second dist.barrier()
rank 68 after dist.barrier(group)
rank 68 Before the second dist.barrier()
rank 199 after dist.barrier(group)
rank 199 Before the second dist.barrier()
rank 169 after dist.barrier(group)
rank 169 Before the second dist.barrier()
rank 116 after dist.barrier(group)
rank 116 Before the second dist.barrier()
rank 85 after dist.barrier(group)
rank 85 Before the second dist.barrier()
rank 153 after dist.barrier(group)
rank 153 Before the second dist.barrier()
rank 124 after dist.barrier(group)
rank 124 Before the second dist.barrier()
rank 173 after dist.barrier(group)
rank 212 after dist.barrier(group)
rank 212 Before the second dist.barrier()
rank 52 after dist.barrier(group)
rank 52 Before the second dist.barrier()
rank 157 after dist.barrier(group)
rank 157 Before the second dist.barrier()
rank 93 after dist.barrier(group)
rank 93 Before the second dist.barrier()
rank 173 Before the second dist.barrier()
rank 180 after dist.barrier(group)
rank 180 Before the second dist.barrier()
rank 213 after dist.barrier(group)
rank 213 Before the second dist.barrier()
rank 221 after dist.barrier(group)
rank 221 Before the second dist.barrier()
rank 235 after dist.barrier(group)
rank 235 Before the second dist.barrier()
rank 53 after dist.barrier(group)
rank 53 Before the second dist.barrier()
rank 92 after dist.barrier(group)
rank 92 Before the second dist.barrier()
rank 37 after dist.barrier(group)
rank 37 Before the second dist.barrier()
rank 200 after dist.barrier(group)
rank 200 Before the second dist.barrier()
rank 108 after dist.barrier(group)
rank 108 Before the second dist.barrier()
rank 135 after dist.barrier(group)
rank 149 after dist.barrier(group)
rank 149 Before the second dist.barrier()
rank 60 after dist.barrier(group)
rank 60 Before the second dist.barrier()
rank 16 after dist.barrier(group)
rank 16 Before the second dist.barrier()
rank 40 after dist.barrier(group)
rank 40 Before the second dist.barrier()
rank 201 after dist.barrier(group)
rank 201 Before the second dist.barrier()
rank 136 after dist.barrier(group)
rank 136 Before the second dist.barrier()
rank 109 after dist.barrier(group)
rank 109 Before the second dist.barrier()
rank 135 Before the second dist.barrier()
rank 61 after dist.barrier(group)
rank 61 Before the second dist.barrier()
rank 17 after dist.barrier(group)
rank 17 Before the second dist.barrier()
rank 41 after dist.barrier(group)
rank 41 Before the second dist.barrier()
rank 24 after dist.barrier(group)
rank 24 Before the second dist.barrier()
rank 72 after dist.barrier(group)
rank 72 Before the second dist.barrier()
rank 137 after dist.barrier(group)
rank 137 Before the second dist.barrier()
rank 166 after dist.barrier(group)
rank 166 Before the second dist.barrier()
rank 36 after dist.barrier(group)
rank 25 after dist.barrier(group)
rank 25 Before the second dist.barrier()
rank 73 after dist.barrier(group)
rank 73 Before the second dist.barrier()
rank 231 after dist.barrier(group)
rank 231 Before the second dist.barrier()
rank 84 after dist.barrier(group)
rank 84 Before the second dist.barrier()
rank 102 after dist.barrier(group)
rank 102 Before the second dist.barrier()
rank 70 after dist.barrier(group)
rank 70 Before the second dist.barrier()
rank 36 Before the second dist.barrier()
rank 230 after dist.barrier(group)
rank 230 Before the second dist.barrier()
rank 254 after dist.barrier(group)
rank 254 Before the second dist.barrier()
rank 118 after dist.barrier(group)
rank 118 Before the second dist.barrier()
rank 148 after dist.barrier(group)
rank 148 Before the second dist.barrier()
rank 126 after dist.barrier(group)
rank 126 Before the second dist.barrier()
rank 21 after dist.barrier(group)
rank 21 Before the second dist.barrier()
rank 182 after dist.barrier(group)
rank 182 Before the second dist.barrier()
rank 220 after dist.barrier(group)
rank 220 Before the second dist.barrier()
rank 255 after dist.barrier(group)
rank 255 Before the second dist.barrier()
rank 119 after dist.barrier(group)
rank 119 Before the second dist.barrier()
rank 156 after dist.barrier(group)
rank 156 Before the second dist.barrier()
rank 127 after dist.barrier(group)
rank 127 Before the second dist.barrier()
rank 20 after dist.barrier(group)
rank 20 Before the second dist.barrier()
rank 29 after dist.barrier(group)
rank 29 Before the second dist.barrier()
rank 76 after dist.barrier(group)
rank 76 Before the second dist.barrier()
rank 204 after dist.barrier(group)
rank 204 Before the second dist.barrier()
rank 172 after dist.barrier(group)
rank 172 Before the second dist.barrier()
rank 190 after dist.barrier(group)
rank 190 Before the second dist.barrier()
rank 141 after dist.barrier(group)
rank 141 Before the second dist.barrier()
rank 45 after dist.barrier(group)
rank 45 Before the second dist.barrier()
rank 194 after dist.barrier(group)
rank 194 Before the second dist.barrier()
rank 28 after dist.barrier(group)
rank 28 Before the second dist.barrier()
rank 77 after dist.barrier(group)
rank 77 Before the second dist.barrier()
rank 205 after dist.barrier(group)
rank 205 Before the second dist.barrier()
rank 131 after dist.barrier(group)
rank 131 Before the second dist.barrier()
rank 195 after dist.barrier(group)
rank 195 Before the second dist.barrier()
rank 103 after dist.barrier(group)
rank 103 Before the second dist.barrier()
rank 94 after dist.barrier(group)
rank 94 Before the second dist.barrier()
rank 214 after dist.barrier(group)
rank 214 Before the second dist.barrier()
rank 71 after dist.barrier(group)
rank 71 Before the second dist.barrier()
rank 95 after dist.barrier(group)
rank 95 Before the second dist.barrier()
rank 167 after dist.barrier(group)
rank 167 Before the second dist.barrier()
rank 86 after dist.barrier(group)
rank 86 Before the second dist.barrier()
rank 158 after dist.barrier(group)
rank 158 Before the second dist.barrier()
rank 38 after dist.barrier(group)
rank 38 Before the second dist.barrier()
rank 191 after dist.barrier(group)
rank 191 Before the second dist.barrier()
rank 54 after dist.barrier(group)
rank 54 Before the second dist.barrier()
rank 140 after dist.barrier(group)
rank 140 Before the second dist.barrier()
rank 159 after dist.barrier(group)
rank 159 Before the second dist.barrier()
rank 183 after dist.barrier(group)
rank 183 Before the second dist.barrier()
rank 222 after dist.barrier(group)
rank 222 Before the second dist.barrier()
rank 44 after dist.barrier(group)
rank 44 Before the second dist.barrier()
rank 174 after dist.barrier(group)
rank 174 Before the second dist.barrier()
rank 223 after dist.barrier(group)
rank 223 Before the second dist.barrier()
rank 110 after dist.barrier(group)
rank 110 Before the second dist.barrier()
rank 150 after dist.barrier(group)
rank 150 Before the second dist.barrier()
rank 62 after dist.barrier(group)
rank 62 Before the second dist.barrier()
rank 130 after dist.barrier(group)
rank 130 Before the second dist.barrier()
rank 175 after dist.barrier(group)
rank 175 Before the second dist.barrier()
rank 98 after dist.barrier(group)
rank 98 Before the second dist.barrier()
rank 99 after dist.barrier(group)
rank 99 Before the second dist.barrier()
rank 162 after dist.barrier(group)
rank 162 Before the second dist.barrier()
rank 179 after dist.barrier(group)
rank 179 Before the second dist.barrier()
rank 215 after dist.barrier(group)
rank 215 Before the second dist.barrier()
rank 227 after dist.barrier(group)
rank 227 Before the second dist.barrier()
rank 163 after dist.barrier(group)
rank 163 Before the second dist.barrier()
rank 123 after dist.barrier(group)
rank 123 Before the second dist.barrier()
rank 251 after dist.barrier(group)
rank 251 Before the second dist.barrier()
rank 55 after dist.barrier(group)
rank 55 Before the second dist.barrier()
rank 151 after dist.barrier(group)
rank 151 Before the second dist.barrier()
rank 39 after dist.barrier(group)
rank 39 Before the second dist.barrier()
rank 114 after dist.barrier(group)
rank 114 Before the second dist.barrier()
rank 87 after dist.barrier(group)
rank 87 Before the second dist.barrier()
rank 67 after dist.barrier(group)
rank 67 Before the second dist.barrier()
rank 115 after dist.barrier(group)
rank 115 Before the second dist.barrier()
rank 22 after dist.barrier(group)
rank 22 Before the second dist.barrier()
rank 46 after dist.barrier(group)
rank 46 Before the second dist.barrier()
rank 78 after dist.barrier(group)
rank 78 Before the second dist.barrier()
rank 206 after dist.barrier(group)
rank 206 Before the second dist.barrier()
rank 187 after dist.barrier(group)
rank 187 Before the second dist.barrier()
rank 142 after dist.barrier(group)
rank 142 Before the second dist.barrier()
rank 63 after dist.barrier(group)
rank 63 Before the second dist.barrier()
rank 30 after dist.barrier(group)
rank 30 Before the second dist.barrier()
rank 79 after dist.barrier(group)
rank 79 Before the second dist.barrier()
rank 207 after dist.barrier(group)
rank 207 Before the second dist.barrier()
rank 143 after dist.barrier(group)
rank 143 Before the second dist.barrier()
rank 111 after dist.barrier(group)
rank 111 Before the second dist.barrier()
rank 31 after dist.barrier(group)
rank 31 Before the second dist.barrier()
rank 250 after dist.barrier(group)
rank 250 Before the second dist.barrier()
rank 211 after dist.barrier(group)
rank 211 Before the second dist.barrier()
rank 122 after dist.barrier(group)
rank 122 Before the second dist.barrier()
rank 219 after dist.barrier(group)
rank 219 Before the second dist.barrier()
rank 83 after dist.barrier(group)
rank 83 Before the second dist.barrier()
rank 66 after dist.barrier(group)
rank 66 Before the second dist.barrier()
rank 50 after dist.barrier(group)
rank 50 Before the second dist.barrier()
rank 107 after dist.barrier(group)
rank 107 Before the second dist.barrier()
rank 226 after dist.barrier(group)
rank 226 Before the second dist.barrier()
rank 51 after dist.barrier(group)
rank 51 Before the second dist.barrier()
rank 106 after dist.barrier(group)
rank 146 after dist.barrier(group)
rank 146 Before the second dist.barrier()
rank 59 after dist.barrier(group)
rank 59 Before the second dist.barrier()
rank 34 after dist.barrier(group)
rank 34 Before the second dist.barrier()
rank 47 after dist.barrier(group)
rank 47 Before the second dist.barrier()
rank 186 after dist.barrier(group)
rank 186 Before the second dist.barrier()
rank 178 after dist.barrier(group)
rank 178 Before the second dist.barrier()
rank 106 Before the second dist.barrier()
rank 90 after dist.barrier(group)
rank 90 Before the second dist.barrier()
rank 147 after dist.barrier(group)
rank 147 Before the second dist.barrier()
rank 58 after dist.barrier(group)
rank 58 Before the second dist.barrier()
rank 35 after dist.barrier(group)
rank 35 Before the second dist.barrier()
rank 23 after dist.barrier(group)
rank 23 Before the second dist.barrier()
rank 171 after dist.barrier(group)
rank 171 Before the second dist.barrier()
rank 154 after dist.barrier(group)
rank 154 Before the second dist.barrier()
rank 91 after dist.barrier(group)
rank 91 Before the second dist.barrier()
rank 155 after dist.barrier(group)
rank 155 Before the second dist.barrier()
rank 210 after dist.barrier(group)
rank 210 Before the second dist.barrier()
rank 218 after dist.barrier(group)
rank 218 Before the second dist.barrier()
rank 82 after dist.barrier(group)
rank 82 Before the second dist.barrier()
rank 170 after dist.barrier(group)
rank 170 Before the second dist.barrier()
rank 19 after dist.barrier(group)
rank 19 Before the second dist.barrier()
rank 42 after dist.barrier(group)
rank 42 Before the second dist.barrier()
rank 75 after dist.barrier(group)
rank 75 Before the second dist.barrier()
rank 203 after dist.barrier(group)
rank 203 Before the second dist.barrier()
rank 139 after dist.barrier(group)
rank 139 Before the second dist.barrier()
rank 43 after dist.barrier(group)
rank 43 Before the second dist.barrier()
rank 26 after dist.barrier(group)
rank 26 Before the second dist.barrier()
rank 27 after dist.barrier(group)
rank 27 Before the second dist.barrier()
rank 74 after dist.barrier(group)
rank 74 Before the second dist.barrier()
rank 202 after dist.barrier(group)
rank 202 Before the second dist.barrier()
rank 18 after dist.barrier(group)
rank 18 Before the second dist.barrier()
rank 138 after dist.barrier(group)
rank 138 Before the second dist.barrier()
rank 0 after dist.barrier(group)
rank 0 Before the second dist.barrier()
rank 1 after dist.barrier(group)
rank 1 src_rank 0
rank 3 after dist.barrier(group)
rank 3 src_rank 0
rank 5 after dist.barrier(group)
rank 5 src_rank 0
rank 6 after dist.barrier(group)
rank 6 src_rank 0
rank 2 after dist.barrier(group)
rank 2 src_rank 0
rank 4 after dist.barrier(group)
rank 4 src_rank 0
rank 7 after dist.barrier(group)
rank 7 src_rank 0
rank 241 after dist.barrier(group)
rank 241 Before the second dist.barrier()
rank 244 after dist.barrier(group)
rank 244 Before the second dist.barrier()
rank 245 after dist.barrier(group)
rank 245 Before the second dist.barrier()
rank 247 after dist.barrier(group)
rank 247 Before the second dist.barrier()
rank 240 after dist.barrier(group)
rank 240 Before the second dist.barrier()
rank 242 after dist.barrier(group)
rank 242 Before the second dist.barrier()
rank 243 after dist.barrier(group)
rank 243 Before the second dist.barrier()
rank 246 after dist.barrier(group)
rank 246 Before the second dist.barrier()
rank 8 after dist.barrier(group)
rank 8 Before the second dist.barrier()
rank 9 after dist.barrier(group)
rank 9 Before the second dist.barrier()
rank 10 after dist.barrier(group)
rank 10 Before the second dist.barrier()
rank 12 after dist.barrier(group)
rank 12 Before the second dist.barrier()
rank 13 after dist.barrier(group)
rank 13 Before the second dist.barrier()
rank 14 after dist.barrier(group)
rank 14 Before the second dist.barrier()
rank 15 after dist.barrier(group)
rank 15 Before the second dist.barrier()
rank 11 after dist.barrier(group)
rank 11 Before the second dist.barrier()
rank 3 Before the second dist.barrier()
rank 1 Before the second dist.barrier()
rank 4 Before the second dist.barrier()
rank 2 Before the second dist.barrier()
rank 6 Before the second dist.barrier()
rank 5 Before the second dist.barrier()
rank 7 Before the second dist.barrier()
rank 1 After the second dist.barrier()
rank 240 After the second dist.barrier()
rank 234 After the second dist.barrier()
rank 224 After the second dist.barrier()
rank 241 After the second dist.barrier()
rank 235 After the second dist.barrier()
rank 225 After the second dist.barrier()
rank 248 After the second dist.barrier()
rank 237 After the second dist.barrier()
rank 228 After the second dist.barrier()
rank 249 After the second dist.barrier()
rank 238 After the second dist.barrier()
rank 229 After the second dist.barrier()
rank 250 After the second dist.barrier()
rank 239 After the second dist.barrier()
rank 251 After the second dist.barrier()
rank 252 After the second dist.barrier()
rank 253 After the second dist.barrier()
rank 232 After the second dist.barrier()
rank 254 After the second dist.barrier()
rank 233 After the second dist.barrier()
rank 255 After the second dist.barrier()
rank 231 After the second dist.barrier()
rank 246 After the second dist.barrier()
rank 247 After the second dist.barrier()
rank 128 After the second dist.barrier()
rank 4 After the second dist.barrier()
rank 192 After the second dist.barrier()
rank 129 After the second dist.barrier()
rank 193 After the second dist.barrier()
rank 5 After the second dist.barrier()
rank 208 After the second dist.barrier()
rank 0 After the second dist.barrier()
rank 242 After the second dist.barrier()
rank 230 After the second dist.barrier()
rank 244 After the second dist.barrier()
rank 216 After the second dist.barrier()
rank 245 After the second dist.barrier()
rank 236 After the second dist.barrier()
parameter name  var_embed  requires_gradient  True size torch.Size([1, 496, 4096])
rank 209 After the second dist.barrier()
rank 64 After the second dist.barrier()
rank 112 After the second dist.barrier()
rank 96 After the second dist.barrier()
rank 160 After the second dist.barrier()
rank 200 After the second dist.barrier()
rank 176 After the second dist.barrier()
rank 217 After the second dist.barrier()
rank 226 After the second dist.barrier()
rank 243 After the second dist.barrier()
rank 97 After the second dist.barrier()
rank 120 After the second dist.barrier()
rank 184 After the second dist.barrier()
rank 227 After the second dist.barrier()
rank 133 After the second dist.barrier()
rank 65 After the second dist.barrier()
rank 6 After the second dist.barrier()
rank 197 After the second dist.barrier()
rank 168 After the second dist.barrier()
rank 7 After the second dist.barrier()
rank 32 After the second dist.barrier()
rank 177 After the second dist.barrier()
rank 220 After the second dist.barrier()
rank 80 After the second dist.barrier()
rank 152 After the second dist.barrier()
rank 161 After the second dist.barrier()
rank 121 After the second dist.barrier()
rank 113 After the second dist.barrier()
rank 48 After the second dist.barrier()
rank 104 After the second dist.barrier()
rank 213 After the second dist.barrier()
rank 221 After the second dist.barrier()
rank 49 After the second dist.barrier()
rank 153 After the second dist.barrier()
rank 88 After the second dist.barrier()
rank 144 After the second dist.barrier()
rank 56 After the second dist.barrier()
rank 201 After the second dist.barrier()
rank 185 After the second dist.barrier()
rank 89 After the second dist.barrier()
rank 132 After the second dist.barrier()
rank 101 After the second dist.barrier()
rank 69 After the second dist.barrier()
rank 196 After the second dist.barrier()
rank 16 After the second dist.barrier()
rank 105 After the second dist.barrier()
rank 169 After the second dist.barrier()
rank 116 After the second dist.barrier()
rank 125 After the second dist.barrier()
rank 33 After the second dist.barrier()
rank 24 After the second dist.barrier()
rank 180 After the second dist.barrier()
rank 117 After the second dist.barrier()
rank 145 After the second dist.barrier()
rank 165 After the second dist.barrier()
rank 205 After the second dist.barrier()
rank 189 After the second dist.barrier()
rank 181 After the second dist.barrier()
rank 212 After the second dist.barrier()
rank 81 After the second dist.barrier()
rank 136 After the second dist.barrier()
rank 57 After the second dist.barrier()
rank 40 After the second dist.barrier()
rank 72 After the second dist.barrier()
rank 73 After the second dist.barrier()
rank 198 After the second dist.barrier()
rank 2 After the second dist.barrier()
rank 134 After the second dist.barrier()
rank 3 After the second dist.barrier()
rank 135 After the second dist.barrier()
rank 172 After the second dist.barrier()
rank 36 After the second dist.barrier()
rank 222 After the second dist.barrier()
rank 164 After the second dist.barrier()
rank 37 After the second dist.barrier()
rank 17 After the second dist.barrier()
rank 214 After the second dist.barrier()
rank 157 After the second dist.barrier()
rank 100 After the second dist.barrier()
rank 68 After the second dist.barrier()
rank 93 After the second dist.barrier()
rank 173 After the second dist.barrier()
rank 8 After the second dist.barrier()
rank 215 After the second dist.barrier()
rank 53 After the second dist.barrier()
rank 84 After the second dist.barrier()
rank 156 After the second dist.barrier()
rank 61 After the second dist.barrier()
rank 41 After the second dist.barrier()
rank 204 After the second dist.barrier()
rank 9 After the second dist.barrier()
rank 188 After the second dist.barrier()
rank 109 After the second dist.barrier()
rank 148 After the second dist.barrier()
rank 124 After the second dist.barrier()
rank 25 After the second dist.barrier()
rank 85 After the second dist.barrier()
rank 137 After the second dist.barrier()
rank 149 After the second dist.barrier()
rank 199 After the second dist.barrier()
rank 182 After the second dist.barrier()
rank 183 After the second dist.barrier()
rank 102 After the second dist.barrier()
rank 223 After the second dist.barrier()
rank 118 After the second dist.barrier()
rank 70 After the second dist.barrier()
rank 103 After the second dist.barrier()
rank 166 After the second dist.barrier()
rank 126 After the second dist.barrier()
rank 206 After the second dist.barrier()
rank 140 After the second dist.barrier()
rank 71 After the second dist.barrier()
rank 108 After the second dist.barrier()
rank 92 After the second dist.barrier()
rank 60 After the second dist.barrier()
rank 21 After the second dist.barrier()
rank 52 After the second dist.barrier()
rank 141 After the second dist.barrier()
rank 167 After the second dist.barrier()
rank 29 After the second dist.barrier()
rank 190 After the second dist.barrier()
rank 44 After the second dist.barrier()
rank 77 After the second dist.barrier()
rank 191 After the second dist.barrier()
rank 130 After the second dist.barrier()
rank 45 After the second dist.barrier()
rank 195 After the second dist.barrier()
rank 131 After the second dist.barrier()
rank 218 After the second dist.barrier()
rank 174 After the second dist.barrier()
rank 219 After the second dist.barrier()
rank 158 After the second dist.barrier()
rank 20 After the second dist.barrier()
rank 119 After the second dist.barrier()
rank 86 After the second dist.barrier()
rank 127 After the second dist.barrier()
rank 62 After the second dist.barrier()
rank 38 After the second dist.barrier()
rank 94 After the second dist.barrier()
rank 63 After the second dist.barrier()
rank 39 After the second dist.barrier()
rank 210 After the second dist.barrier()
rank 54 After the second dist.barrier()
rank 87 After the second dist.barrier()
rank 110 After the second dist.barrier()
rank 76 After the second dist.barrier()
rank 207 After the second dist.barrier()
rank 211 After the second dist.barrier()
rank 55 After the second dist.barrier()
rank 111 After the second dist.barrier()
rank 150 After the second dist.barrier()
rank 28 After the second dist.barrier()
rank 151 After the second dist.barrier()
rank 13 After the second dist.barrier()
rank 194 After the second dist.barrier()
rank 66 After the second dist.barrier()
rank 163 After the second dist.barrier()
rank 178 After the second dist.barrier()
rank 115 After the second dist.barrier()
rank 99 After the second dist.barrier()
rank 67 After the second dist.barrier()
rank 22 After the second dist.barrier()
rank 202 After the second dist.barrier()
rank 179 After the second dist.barrier()
rank 95 After the second dist.barrier()
rank 122 After the second dist.barrier()
rank 23 After the second dist.barrier()
rank 203 After the second dist.barrier()
rank 175 After the second dist.barrier()
rank 142 After the second dist.barrier()
rank 159 After the second dist.barrier()
rank 123 After the second dist.barrier()
rank 46 After the second dist.barrier()
rank 30 After the second dist.barrier()
rank 12 After the second dist.barrier()
rank 187 After the second dist.barrier()
rank 47 After the second dist.barrier()
rank 78 After the second dist.barrier()
rank 98 After the second dist.barrier()
rank 170 After the second dist.barrier()
rank 155 After the second dist.barrier()
rank 35 After the second dist.barrier()
rank 171 After the second dist.barrier()
rank 162 After the second dist.barrier()
rank 59 After the second dist.barrier()
rank 114 After the second dist.barrier()
rank 143 After the second dist.barrier()
rank 107 After the second dist.barrier()
rank 91 After the second dist.barrier()
rank 82 After the second dist.barrier()
rank 51 After the second dist.barrier()
rank 83 After the second dist.barrier()
rank 31 After the second dist.barrier()
rank 14 After the second dist.barrier()
rank 186 After the second dist.barrier()
rank 147 After the second dist.barrier()
rank 79 After the second dist.barrier()
rank 15 After the second dist.barrier()
rank 18 After the second dist.barrier()
rank 19 After the second dist.barrier()
rank 58 After the second dist.barrier()
rank 146 After the second dist.barrier()
rank 34 After the second dist.barrier()
rank 138 After the second dist.barrier()
rank 154 After the second dist.barrier()
rank 27 After the second dist.barrier()
rank 139 After the second dist.barrier()
rank 90 After the second dist.barrier()
rank 106 After the second dist.barrier()
rank 43 After the second dist.barrier()
rank 50 After the second dist.barrier()
rank 74 After the second dist.barrier()
rank 75 After the second dist.barrier()
rank 42 After the second dist.barrier()
rank 26 After the second dist.barrier()
rank 10 After the second dist.barrier()
rank 11 After the second dist.barrier()
parameter name  var_query  requires_gradient  True size torch.Size([1, 1, 4096])
parameter name  pos_embed  requires_gradient  True size torch.Size([1, 2048, 4096])
parameter name  mask_token  requires_gradient  True size torch.Size([1, 1, 128])
parameter name  decoder_pos_embed  requires_gradient  True size torch.Size([1, 2048, 128])
parameter name  token_embeds.0.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.0.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.1.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.1.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.2.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.2.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.3.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.3.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.4.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.4.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.5.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.5.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.6.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.6.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.7.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.7.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.8.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.8.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.9.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.9.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.10.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.10.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.11.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.11.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.12.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.12.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.13.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.13.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.14.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.14.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.15.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.15.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.16.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.16.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.17.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.17.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.18.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.18.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.19.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.19.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.20.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.20.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.21.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.21.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.22.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.22.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.23.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.23.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.24.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.24.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.25.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.25.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.26.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.26.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.27.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.27.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.28.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.28.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.29.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.29.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.30.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.30.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.31.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.31.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.32.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.32.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.33.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.33.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.34.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.34.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.35.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.35.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.36.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.36.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.37.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.37.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.38.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.38.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.39.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.39.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.40.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.40.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.41.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.41.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.42.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.42.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.43.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.43.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.44.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.44.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.45.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.45.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.46.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.46.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.47.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.47.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.48.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.48.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.49.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.49.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.50.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.50.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.51.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.51.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.52.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.52.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.53.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.53.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.54.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.54.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.55.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.55.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.56.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.56.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.57.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.57.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.58.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.58.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.59.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.59.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.60.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.60.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.61.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.61.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.62.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.62.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.63.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.63.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.64.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.64.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.65.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.65.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.66.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.66.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.67.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.67.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.68.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.68.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.69.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.69.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.70.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.70.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.71.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.71.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.72.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.72.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.73.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.73.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.74.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.74.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.75.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.75.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.76.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.76.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.77.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.77.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.78.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.78.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.79.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.79.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.80.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.80.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.81.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.81.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.82.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.82.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.83.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.83.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.84.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.84.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.85.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.85.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.86.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.86.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.87.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.87.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.88.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.88.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.89.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.89.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.90.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.90.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.91.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.91.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.92.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.92.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.93.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.93.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.94.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.94.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.95.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.95.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.96.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.96.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.97.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.97.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.98.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.98.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.99.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.99.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.100.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.100.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.101.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.101.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.102.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.102.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.103.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.103.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.104.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.104.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.105.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.105.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.106.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.106.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.107.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.107.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.108.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.108.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.109.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.109.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.110.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.110.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.111.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.111.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.112.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.112.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.113.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.113.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.114.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.114.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.115.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.115.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.116.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.116.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.117.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.117.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.118.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.118.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.119.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.119.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.120.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.120.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.121.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.121.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.122.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.122.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.123.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.123.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.124.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.124.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.125.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.125.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.126.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.126.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.127.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.127.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.128.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.128.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.129.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.129.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.130.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.130.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.131.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.131.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.132.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.132.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.133.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.133.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.134.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.134.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.135.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.135.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.136.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.136.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.137.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.137.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.138.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.138.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.139.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.139.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.140.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.140.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.141.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.141.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.142.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.142.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.143.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.143.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.144.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.144.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.145.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.145.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.146.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.146.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.147.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.147.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.148.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.148.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.149.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.149.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.150.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.150.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.151.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.151.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.152.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.152.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.153.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.153.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.154.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.154.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.155.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.155.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.156.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.156.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.157.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.157.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.158.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.158.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.159.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.159.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.160.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.160.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.161.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.161.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.162.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.162.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.163.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.163.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.164.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.164.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.165.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.165.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.166.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.166.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.167.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.167.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.168.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.168.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.169.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.169.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.170.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.170.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.171.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.171.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.172.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.172.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.173.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.173.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.174.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.174.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.175.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.175.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.176.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.176.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.177.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.177.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.178.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.178.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.179.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.179.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.180.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.180.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.181.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.181.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.182.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.182.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.183.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.183.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.184.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.184.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.185.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.185.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.186.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.186.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.187.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.187.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.188.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.188.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.189.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.189.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.190.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.190.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.191.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.191.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.192.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.192.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.193.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.193.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.194.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.194.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.195.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.195.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.196.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.196.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.197.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.197.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.198.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.198.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.199.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.199.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.200.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.200.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.201.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.201.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.202.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.202.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.203.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.203.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.204.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.204.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.205.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.205.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.206.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.206.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.207.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.207.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.208.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.208.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.209.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.209.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.210.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.210.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.211.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.211.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.212.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.212.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.213.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.213.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.214.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.214.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.215.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.215.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.216.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.216.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.217.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.217.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.218.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.218.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.219.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.219.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.220.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.220.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.221.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.221.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.222.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.222.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.223.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.223.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.224.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.224.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.225.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.225.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.226.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.226.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.227.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.227.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.228.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.228.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.229.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.229.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.230.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.230.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.231.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.231.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.232.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.232.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.233.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.233.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.234.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.234.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.235.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.235.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.236.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.236.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.237.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.237.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.238.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.238.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.239.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.239.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.240.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.240.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.241.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.241.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.242.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.242.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.243.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.243.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.244.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.244.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.245.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.245.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.246.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.246.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.247.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.247.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.248.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.248.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.249.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.249.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.250.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.250.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.251.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.251.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.252.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.252.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.253.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.253.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.254.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.254.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.255.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.255.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.256.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.256.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.257.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.257.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.258.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.258.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.259.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.259.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.260.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.260.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.261.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.261.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.262.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.262.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.263.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.263.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.264.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.264.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.265.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.265.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.266.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.266.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.267.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.267.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.268.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.268.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.269.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.269.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.270.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.270.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.271.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.271.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.272.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.272.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.273.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.273.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.274.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.274.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.275.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.275.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.276.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.276.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.277.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.277.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.278.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.278.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.279.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.279.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.280.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.280.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.281.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.281.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.282.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.282.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.283.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.283.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.284.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.284.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.285.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.285.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.286.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.286.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.287.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.287.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.288.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.288.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.289.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.289.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.290.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.290.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.291.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.291.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.292.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.292.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.293.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.293.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.294.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.294.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.295.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.295.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.296.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.296.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.297.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.297.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.298.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.298.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.299.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.299.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.300.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.300.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.301.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.301.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.302.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.302.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.303.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.303.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.304.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.304.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.305.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.305.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.306.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.306.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.307.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.307.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.308.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.308.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.309.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.309.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.310.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.310.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.311.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.311.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.312.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.312.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.313.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.313.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.314.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.314.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.315.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.315.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.316.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.316.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.317.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.317.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.318.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.318.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.319.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.319.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.320.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.320.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.321.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.321.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.322.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.322.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.323.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.323.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.324.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.324.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.325.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.325.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.326.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.326.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.327.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.327.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.328.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.328.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.329.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.329.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.330.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.330.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.331.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.331.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.332.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.332.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.333.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.333.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.334.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.334.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.335.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.335.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.336.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.336.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.337.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.337.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.338.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.338.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.339.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.339.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.340.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.340.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.341.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.341.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.342.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.342.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.343.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.343.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.344.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.344.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.345.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.345.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.346.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.346.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.347.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.347.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.348.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.348.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.349.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.349.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.350.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.350.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.351.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.351.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.352.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.352.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.353.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.353.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.354.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.354.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.355.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.355.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.356.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.356.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.357.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.357.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.358.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.358.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.359.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.359.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.360.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.360.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.361.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.361.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.362.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.362.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.363.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.363.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.364.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.364.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.365.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.365.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.366.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.366.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.367.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.367.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.368.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.368.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.369.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.369.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.370.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.370.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.371.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.371.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.372.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.372.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.373.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.373.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.374.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.374.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.375.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.375.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.376.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.376.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.377.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.377.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.378.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.378.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.379.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.379.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.380.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.380.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.381.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.381.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.382.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.382.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.383.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.383.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.384.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.384.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.385.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.385.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.386.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.386.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.387.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.387.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.388.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.388.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.389.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.389.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.390.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.390.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.391.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.391.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.392.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.392.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.393.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.393.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.394.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.394.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.395.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.395.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.396.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.396.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.397.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.397.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.398.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.398.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.399.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.399.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.400.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.400.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.401.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.401.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.402.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.402.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.403.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.403.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.404.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.404.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.405.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.405.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.406.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.406.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.407.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.407.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.408.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.408.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.409.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.409.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.410.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.410.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.411.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.411.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.412.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.412.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.413.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.413.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.414.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.414.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.415.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.415.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.416.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.416.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.417.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.417.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.418.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.418.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.419.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.419.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.420.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.420.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.421.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.421.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.422.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.422.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.423.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.423.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.424.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.424.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.425.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.425.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.426.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.426.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.427.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.427.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.428.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.428.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.429.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.429.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.430.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.430.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.431.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.431.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.432.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.432.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.433.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.433.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.434.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.434.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.435.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.435.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.436.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.436.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.437.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.437.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.438.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.438.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.439.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.439.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.440.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.440.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.441.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.441.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.442.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.442.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.443.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.443.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.444.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.444.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.445.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.445.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.446.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.446.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.447.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.447.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.448.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.448.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.449.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.449.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.450.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.450.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.451.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.451.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.452.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.452.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.453.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.453.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.454.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.454.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.455.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.455.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.456.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.456.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.457.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.457.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.458.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.458.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.459.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.459.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.460.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.460.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.461.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.461.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.462.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.462.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.463.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.463.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.464.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.464.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.465.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.465.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.466.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.466.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.467.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.467.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.468.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.468.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.469.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.469.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.470.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.470.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.471.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.471.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.472.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.472.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.473.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.473.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.474.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.474.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.475.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.475.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.476.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.476.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.477.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.477.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.478.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.478.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.479.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.479.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.480.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.480.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.481.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.481.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.482.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.482.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.483.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.483.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.484.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.484.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.485.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.485.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.486.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.486.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.487.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.487.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.488.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.488.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.489.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.489.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.490.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.490.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.491.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.491.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.492.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.492.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.493.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.493.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.494.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.494.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  token_embeds.495.proj.weight  requires_gradient  True size torch.Size([4096, 1, 6, 6])
parameter name  token_embeds.495.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  var_agg.q.weight  requires_gradient  True size torch.Size([512, 4096])
parameter name  var_agg.kv.weight  requires_gradient  True size torch.Size([1024, 4096])
parameter name  var_agg.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  var_agg.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  lead_time_embed.weight  requires_gradient  True size torch.Size([4096, 1])
parameter name  lead_time_embed.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.0.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.0.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.0.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.0.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.0.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.0.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.0.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.0.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.0.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.0.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.0.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.0.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.1.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.1.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.1.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.1.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.1.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.1.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.1.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.1.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.1.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.1.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.1.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.1.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.2.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.2.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.2.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.2.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.2.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.2.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.2.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.2.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.2.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.2.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.2.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.2.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.3.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.3.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.3.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.3.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.3.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.3.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.3.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.3.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.3.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.3.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.3.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.3.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.4.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.4.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.4.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.4.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.4.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.4.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.4.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.4.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.4.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.4.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.4.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.4.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.5.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.5.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.5.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.5.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.5.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.5.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.5.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.5.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.5.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.5.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.5.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.5.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.6.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.6.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.6.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.6.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.6.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.6.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.6.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.6.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.6.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.6.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.6.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.6.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.7.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.7.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.7.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.7.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.7.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.7.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.7.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.7.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.7.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.7.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.7.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.7.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.8.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.8.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.8.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.8.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.8.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.8.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.8.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.8.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.8.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.8.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.8.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.8.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.9.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.9.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.9.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.9.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.9.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.9.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.9.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.9.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.9.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.9.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.9.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.9.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.10.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.10.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.10.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.10.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.10.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.10.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.10.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.10.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.10.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.10.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.10.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.10.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.11.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.11.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.11.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.11.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.11.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.11.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.11.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.11.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.11.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.11.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.11.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.11.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.12.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.12.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.12.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.12.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.12.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.12.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.12.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.12.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.12.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.12.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.12.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.12.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.13.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.13.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.13.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.13.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.13.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.13.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.13.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.13.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.13.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.13.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.13.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.13.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.14.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.14.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.14.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.14.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.14.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.14.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.14.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.14.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.14.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.14.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.14.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.14.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.15.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.15.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.15.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.15.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.15.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.15.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.15.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.15.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.15.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.15.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.15.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.15.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.16.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.16.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.16.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.16.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.16.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.16.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.16.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.16.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.16.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.16.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.16.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.16.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.17.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.17.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.17.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.17.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.17.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.17.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.17.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.17.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.17.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.17.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.17.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.17.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.18.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.18.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.18.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.18.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.18.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.18.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.18.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.18.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.18.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.18.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.18.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.18.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.19.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.19.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.19.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.19.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.19.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.19.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.19.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.19.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.19.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.19.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.19.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.19.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.20.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.20.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.20.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.20.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.20.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.20.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.20.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.20.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.20.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.20.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.20.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.20.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.21.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.21.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.21.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.21.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.21.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.21.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.21.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.21.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.21.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.21.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.21.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.21.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.22.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.22.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.22.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.22.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.22.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.22.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.22.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.22.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.22.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.22.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.22.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.22.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.23.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.23.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.23.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.23.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.23.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.23.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.23.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.23.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.23.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.23.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.23.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.23.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.24.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.24.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.24.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.24.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.24.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.24.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.24.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.24.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.24.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.24.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.24.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.24.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.25.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.25.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.25.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.25.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.25.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.25.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.25.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.25.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.25.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.25.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.25.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.25.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.26.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.26.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.26.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.26.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.26.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.26.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.26.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.26.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.26.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.26.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.26.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.26.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.27.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.27.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.27.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.27.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.27.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.27.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.27.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.27.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.27.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.27.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.27.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.27.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.28.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.28.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.28.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.28.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.28.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.28.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.28.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.28.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.28.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.28.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.28.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.28.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.29.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.29.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.29.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.29.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.29.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.29.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.29.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.29.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.29.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.29.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.29.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.29.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.30.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.30.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.30.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.30.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.30.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.30.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.30.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.30.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.30.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.30.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.30.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.30.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.31.norm1.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.31.norm1.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.31.attn.qkv.weight  requires_gradient  True size torch.Size([1536, 4096])
parameter name  blocks.31.attn.qkv.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.31.attn.proj.weight  requires_gradient  True size torch.Size([4096, 512])
parameter name  blocks.31.attn.proj.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.31.norm2.weight  requires_gradient  True size torch.Size([4096])
parameter name  blocks.31.norm2.bias  requires_gradient  True size torch.Size([4096])
parameter name  blocks.31.mlp.fc1.weight  requires_gradient  True size torch.Size([2048, 4096])
parameter name  blocks.31.mlp.fc1.bias  requires_gradient  True size torch.Size([2048])
parameter name  blocks.31.mlp.fc2.weight  requires_gradient  True size torch.Size([4096, 2048])
parameter name  blocks.31.mlp.fc2.bias  requires_gradient  True size torch.Size([4096])
parameter name  norm.weight  requires_gradient  True size torch.Size([4096])
parameter name  norm.bias  requires_gradient  True size torch.Size([4096])
parameter name  decoder_embed.weight  requires_gradient  True size torch.Size([128, 4096])
parameter name  decoder_embed.bias  requires_gradient  True size torch.Size([128])
parameter name  decoder_blocks.0.norm1.weight  requires_gradient  True size torch.Size([128])
parameter name  decoder_blocks.0.norm1.bias  requires_gradient  True size torch.Size([128])
parameter name  decoder_blocks.0.attn.qkv.weight  requires_gradient  True size torch.Size([384, 128])
parameter name  decoder_blocks.0.attn.qkv.bias  requires_gradient  True size torch.Size([384])
parameter name  decoder_blocks.0.attn.proj.weight  requires_gradient  True size torch.Size([128, 128])
parameter name  decoder_blocks.0.attn.proj.bias  requires_gradient  True size torch.Size([128])
parameter name  decoder_blocks.0.norm2.weight  requires_gradient  True size torch.Size([128])
parameter name  decoder_blocks.0.norm2.bias  requires_gradient  True size torch.Size([128])
parameter name  decoder_blocks.0.mlp.fc1.weight  requires_gradient  True size torch.Size([512, 128])
parameter name  decoder_blocks.0.mlp.fc1.bias  requires_gradient  True size torch.Size([512])
parameter name  decoder_blocks.0.mlp.fc2.weight  requires_gradient  True size torch.Size([128, 512])
parameter name  decoder_blocks.0.mlp.fc2.bias  requires_gradient  True size torch.Size([128])
parameter name  decoder_blocks.1.norm1.weight  requires_gradient  True size torch.Size([128])
parameter name  decoder_blocks.1.norm1.bias  requires_gradient  True size torch.Size([128])
parameter name  decoder_blocks.1.attn.qkv.weight  requires_gradient  True size torch.Size([384, 128])
parameter name  decoder_blocks.1.attn.qkv.bias  requires_gradient  True size torch.Size([384])
parameter name  decoder_blocks.1.attn.proj.weight  requires_gradient  True size torch.Size([128, 128])
parameter name  decoder_blocks.1.attn.proj.bias  requires_gradient  True size torch.Size([128])
parameter name  decoder_blocks.1.norm2.weight  requires_gradient  True size torch.Size([128])
parameter name  decoder_blocks.1.norm2.bias  requires_gradient  True size torch.Size([128])
parameter name  decoder_blocks.1.mlp.fc1.weight  requires_gradient  True size torch.Size([512, 128])
parameter name  decoder_blocks.1.mlp.fc1.bias  requires_gradient  True size torch.Size([512])
parameter name  decoder_blocks.1.mlp.fc2.weight  requires_gradient  True size torch.Size([128, 512])
parameter name  decoder_blocks.1.mlp.fc2.bias  requires_gradient  True size torch.Size([128])
parameter name  decoder_norm.weight  requires_gradient  True size torch.Size([128])
parameter name  decoder_norm.bias  requires_gradient  True size torch.Size([128])
parameter name  decoder_pred.weight  requires_gradient  True size torch.Size([17856, 128])
parameter name  decoder_pred.bias  requires_gradient  True size torch.Size([17856])
total_params before FSDP tensor(6604997568) params_per_gpu tensor(903697600)
total_params after FSDP FullyShardedDataParallel(
  (_fsdp_wrapped_module): ClimaX(
    (token_embeds): ModuleList(
      (0-495): 496 x PatchEmbed(
        (proj): Conv2d(1, 4096, kernel_size=(6, 6), stride=(6, 6))
        (norm): Identity()
      )
    )
    (var_agg): FullyShardedDataParallel(
      (_fsdp_wrapped_module): CheckpointWrapper(
        (_checkpoint_wrapped_module): VariableMapping_Attention(
          (q): Linear(in_features=4096, out_features=512, bias=False)
          (kv): Linear(in_features=4096, out_features=1024, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=4096, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (lead_time_embed): Linear(in_features=1, out_features=4096, bias=True)
    (pos_drop): Dropout(p=0.1, inplace=False)
    (blocks): ModuleList(
      (0-31): 32 x FullyShardedDataParallel(
        (_fsdp_wrapped_module): CheckpointWrapper(
          (_checkpoint_wrapped_module): Block(
            (norm1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=4096, out_features=1536, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=512, out_features=4096, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (norm2): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=4096, out_features=2048, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (fc2): Linear(in_features=2048, out_features=4096, bias=True)
            )
            (ls2): Identity()
          )
        )
      )
    )
    (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
    (decoder_embed): Linear(in_features=4096, out_features=128, bias=True)
    (decoder_blocks): ModuleList(
      (0-1): 2 x FullyShardedDataParallel(
        (_fsdp_wrapped_module): CheckpointWrapper(
          (_checkpoint_wrapped_module): Block(
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=128, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=128, out_features=128, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
            (ls1): Identity()
            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=128, out_features=512, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.0, inplace=False)
              (fc2): Linear(in_features=512, out_features=128, bias=True)
            )
            (ls2): Identity()
          )
        )
      )
    )
    (decoder_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (decoder_pred): Linear(in_features=128, out_features=17856, bias=True)
  )
)
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
memory stats reset, ready to track
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
MIOpen(HIP): Warning [PlainTextDb] Unable to create a directory: "/tmp/.config/miopen"
MIOpen(HIP): Warning [PlainTextDb] Unable to create a directory: "/tmp/.config/miopen"
MIOpen(HIP): Warning [PlainTextDb] Unable to create a directory: "/tmp/.config/miopen"
MIOpen(HIP): Warning [PlainTextDb] Unable to create a directory: "/tmp/.config/miopen"
MIOpen(HIP): Warning [PlainTextDb] Unable to create a directory: "/tmp/.config/miopen"
MIOpen(HIP): Warning [PlainTextDb] Unable to create a directory: "/tmp/.config/miopen"
MIOpen(HIP): Warning [PlainTextDb] Unable to create a directory: "/tmp/.config/miopen"
MIOpen(HIP): Warning [PlainTextDb] Unable to create a directory: "/tmp/.config/miopen"
MIOpen(HIP): Warning [PlainTextDb] Unable to create a directory: "/tmp/.config/miopen"
MIOpen(HIP): Warning [PlainTextDb] Unable to create a directory: "/tmp/.config/miopen"
epoch-train:  0 batch_idx 0 step 0  loss  0.06396484375 get_lr 
epoch-train:  0 batch_idx 1 step 1  loss  0.0615234375 get_lr 
epoch-train:  0 batch_idx 2 step 2  loss  0.0595703125 get_lr 
[2025-03-27 14:30:49,712] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,713] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,713] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,713] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,714] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,714] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,714] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,714] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,715] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,715] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,715] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,715] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,715] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,715] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,715] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,715] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,715] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,715] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,715] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,715] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,715] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,715] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,715] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,715] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,715] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,715] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,716] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,716] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,716] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,716] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,716] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,716] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,716] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,716] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,716] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,716] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,716] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,716] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,716] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,716] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,716] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,716] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,717] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,717] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,717] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,717] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,717] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,717] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,717] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,717] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,717] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,717] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,717] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,717] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,717] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,717] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,717] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,717] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,718] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,718] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,718] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,718] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,718] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,718] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,718] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,718] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,718] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,718] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,718] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,718] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,718] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,718] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,718] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,718] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,718] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,718] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,718] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,718] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,718] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,718] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,719] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,719] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,719] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,719] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,719] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,719] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,719] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,719] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,719] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,719] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,719] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,719] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,719] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,719] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,719] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,719] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,719] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,719] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,719] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,719] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,719] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,719] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,719] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,719] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,719] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,719] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,720] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,720] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,720] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,720] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,720] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,720] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,720] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,720] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,720] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,720] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,720] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,720] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,720] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,720] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,720] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,720] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,720] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,720] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,720] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,720] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,720] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,720] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,720] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,720] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,720] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,720] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,720] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,720] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,720] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,720] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,720] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,720] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,720] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,720] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,720] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,720] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,721] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,721] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,721] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,721] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,721] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,721] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,721] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,721] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,721] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,721] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,721] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,721] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,721] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,721] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,721] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,721] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,721] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,721] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,721] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,721] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,721] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,721] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,721] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,721] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,721] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,721] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,721] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,721] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,721] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,721] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,721] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
epoch-train:  0 batch_idx 3 step 3  loss  0.057861328125 get_lr 
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,723] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,722] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,723] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,723] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,723] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,723] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,723] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,723] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,723] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,723] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,723] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,723] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,723] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,723] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,723] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,723] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,723] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,723] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,723] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,723] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,723] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:49,723] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-27 14:30:53,415] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,415] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,415] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,416] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,417] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,417] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,424] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,424] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,424] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,425] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,425] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,426] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,427] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,427] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,429] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,429] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,430] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,430] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,431] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,431] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,431] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,431] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,432] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,432] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,433] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,433] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,434] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,434] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,434] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,434] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,435] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,435] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,436] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,436] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,438] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,438] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,438] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,438] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,439] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,439] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,440] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,440] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,440] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,440] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,440] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,440] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,441] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,441] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,441] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,441] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,441] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,441] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,443] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,443] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,443] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,444] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,445] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,445] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,445] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,445] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,446] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,446] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,447] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,447] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,447] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,447] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,448] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,448] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,449] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,449] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,449] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,449] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,449] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,450] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,450] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,450] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,451] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,451] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,452] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,452] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,453] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,453] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,458] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,458] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,458] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,459] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,460] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,460] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,461] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,461] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,461] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,461] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,461] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,461] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,461] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,461] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,461] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,461] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,462] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,462] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,463] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,463] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,463] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,463] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,464] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,464] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,465] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,465] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,466] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,466] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,467] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,467] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,468] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,468] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,468] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,468] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,468] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,468] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,468] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,468] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,469] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,469] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,469] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,469] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,469] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,469] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,469] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,469] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,469] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,469] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,471] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,471] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,471] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,471] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,473] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,473] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,473] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,473] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,474] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,473] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,474] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,474] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,475] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,475] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,477] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,477] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,477] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,477] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,479] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,479] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,481] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,481] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,483] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,484] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,484] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,484] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,484] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,485] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,485] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,485] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,485] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,485] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,486] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,486] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,487] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,487] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,488] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,488] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,488] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,488] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,490] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,491] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,491] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,491] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,492] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,492] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,492] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,492] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,493] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,493] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,494] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,494] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,494] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,494] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,496] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,496] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,496] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,496] [INFO] [profiler.py:226:end_profile] Flops profiler finished
epoch-train:  0 batch_idx 4 step 4  loss  0.05908203125 get_lr 
[2025-03-27 14:30:53,496] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,496] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,496] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,497] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,497] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,497] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,498] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,498] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,498] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,499] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,499] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,500] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,500] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,500] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,500] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,501] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,501] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,501] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,503] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,503] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,503] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,503] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,504] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,504] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,507] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,508] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,508] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,508] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,508] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,508] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,509] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,509] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,509] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,509] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,509] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,509] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,510] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,510] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,511] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,512] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,512] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,512] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,512] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,512] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,513] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,513] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,513] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,513] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,514] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,514] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,515] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,515] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,517] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,517] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,517] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,517] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,519] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,519] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,520] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,521] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,523] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,523] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,524] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,524] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,524] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,524] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,527] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-27 14:30:53,527] [INFO] [profiler.py:226:end_profile] Flops profiler finished
epoch:  0  epoch_loss  0.037750244140625

--> cuda max reserved memory = 62.7441
--> max reserved percentage = 98.06 %

--> cuda max memory allocated = 60.1521
--> max allocated percentage = 94.01 %

--> peak active memory = 60.1756
--> peak active memory 94.05 %

cudaMalloc retries = 6
cuda OOM = 0

--> Recommend decreasing batch size...cuda retries can greatly degrade perf!

HERE1 Namespace(arch='orbit', batch_size=2, dataset='appl', depth=32, embed_dim=4096, fsdp_size=2, log_dir='./logs/2025-03-27--14:29:28.165823_dataset[appl]_batch_size[2]/', max_epochs=10000, num_heads=32, seq_par_size=1, simple_ddp_size=16, tensor_par_size=8, yaml_config='configs/appl.yaml') 62.7441 7.525911650080676 tensor(6604997568)

/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
0.05user 0.23system 2:14.94elapsed 0%CPU (0avgtext+0avgdata 17408maxresident)k
2966inputs+2224outputs (10major+3449minor)pagefaults 0swaps
