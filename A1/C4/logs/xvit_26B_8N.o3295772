
Lmod is automatically replacing "cce/18.0.1" with "gcc-native/14.2".


Lmod is automatically replacing "PrgEnv-cray/8.6.0" with "PrgEnv-gnu/8.6.0".


Inactive Modules:
  1) darshan-runtime

Due to MODULEPATH changes, the following have been reloaded:
  1) cray-libsci/24.11.0     2) cray-mpich/8.1.31


Lmod is automatically replacing "gcc-native/14.2" with "gcc/12.2.0".


The following have been reloaded with a version change:
  1) cray-libsci/24.11.0 => cray-libsci/23.09.1.1
  2) cray-mpich/8.1.31 => cray-mpich/8.1.27
  3) libfabric/1.22.0 => libfabric/1.20.1

Lmod has detected the following error: These module(s) or extension(s) exist
but cannot be loaded as requested: "darshan-runtime"
   Try: "module spider darshan-runtime" to see how to load the module(s).



for i in {orbit_token_agg,orbit_linear_token_agg,orbit_linear,}; do for j in {2,}; do for k in {256,}; do python train.py \ configs/ERA5-100million-91variables.yaml \ --max_epochs 1 \ --fa2 \ --fsdp_size 1 \ --simple_ddp_size 1 \ --seq_par_size 1 \ --tensor_par_size 64 \ --batch_size $j \ --arch $i \ --channels $k \ --imagex 128 \ --imagey 256 \ --embed_dim 8192 \ --depth 32 \ --num_heads 64 echo "sleeping..." sleep 5 echo "Done" done done done
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,129] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,129] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,129] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,129] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,129] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,129] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,129] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,129] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,130] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,130] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,130] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,130] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,130] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,130] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,131] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,130] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,132] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,132] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,132] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,132] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,132] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,132] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,132] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,132] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,132] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,132] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,132] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,132] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,132] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,132] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,132] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,132] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,132] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,132] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,132] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,132] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,132] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,132] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,132] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,132] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,133] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,133] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,133] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,133] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,133] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,133] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,133] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,133] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,133] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,133] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,133] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,133] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,134] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,133] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,134] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,133] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,134] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,133] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,134] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,134] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,134] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,134] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,134] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:55:16,133] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
HERE0 Namespace(arch='orbit_token_agg', batch_size=2, channels=256, depth=32, embed_dim=8192, fa2=True, fsdp_size=1, imagex=128, imagey=256, max_epochs=1, num_heads=64, real=False, seq_par_size=1, simple_ddp_size=1, tensor_par_size=64, yaml_config='configs/ERA5-100million-91variables.yaml')
[W socket.cpp:464] [c10d] The server socket cannot be initialized on [::]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
rank 7 Before initialize parallelism groups
rank 9 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 12 Before initialize parallelism groups
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
rank 1 Before initialize parallelism groups
rank 40 Before initialize parallelism groups
rank 43 Before initialize parallelism groups
rank 44 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
rank 52 Before initialize parallelism groups
rank 55 Before initialize parallelism groups
rank 50 Before initialize parallelism groups
rank 54 Before initialize parallelism groups
rank 51 Before initialize parallelism groups
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 13 Before initialize parallelism groups
rank 10 Before initialize parallelism groups
Using dist.init_process_group. world_size  64
rank 56 Before initialize parallelism groups
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
rank 27 Before initialize parallelism groups
rank 62 Before initialize parallelism groups
rank 25 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 9 After initialize parallelism groups
rank 41 Before initialize parallelism groups
rank 7 After initialize parallelism groups
rank 60 Before initialize parallelism groups
rank 59 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 5 Before initialize parallelism groups
rank 12 After initialize parallelism groups
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
rank 57 Before initialize parallelism groups
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
rank 49 Before initialize parallelism groups
rank 1 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 11 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 40 After initialize parallelism groups
rank 43 After initialize parallelism groups
rank 44 After initialize parallelism groups
rank 29 Before initialize parallelism groups
rank 50 After initialize parallelism groups
rank 54 After initialize parallelism groups
rank 52 After initialize parallelism groups
rank 51 After initialize parallelism groups
rank 24 Before initialize parallelism groups
rank 55 After initialize parallelism groups
rank 13 After initialize parallelism groups
rank 47 Before initialize parallelism groups
rank 10 After initialize parallelism groups
rank 62 After initialize parallelism groups
rank 56 After initialize parallelism groups
rank 25 After initialize parallelism groups
rank 27 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 42 Before initialize parallelism groups
rank 4 Before initialize parallelism groups
rank 60 After initialize parallelism groups
rank 41 After initialize parallelism groups
rank 15 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 59 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 5 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 57 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 8 Before initialize parallelism groups
rank 3 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 26 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 49 After initialize parallelism groups
rank 48 Before initialize parallelism groups
rank 11 After initialize parallelism groups
rank 33 Before initialize parallelism groups
rank 35 Before initialize parallelism groups
rank 36 Before initialize parallelism groups
rank 14 Before initialize parallelism groups
rank 32 Before initialize parallelism groups
rank 29 After initialize parallelism groups
rank 24 After initialize parallelism groups
rank 47 After initialize parallelism groups
rank 34 Before initialize parallelism groups
rank 16 Before initialize parallelism groups
rank 17 Before initialize parallelism groups
rank 22 Before initialize parallelism groups
rank 19 Before initialize parallelism groups
rank 21 Before initialize parallelism groups
rank 4 After initialize parallelism groups
rank 42 After initialize parallelism groups
rank 15 After initialize parallelism groups
rank 8 After initialize parallelism groups
rank 3 After initialize parallelism groups
rank 26 After initialize parallelism groups
rank 48 After initialize parallelism groups
rank 14 After initialize parallelism groups
rank 33 After initialize parallelism groups
rank 35 After initialize parallelism groups
rank 32 After initialize parallelism groups
rank 36 After initialize parallelism groups
rank 34 After initialize parallelism groups
rank 19 After initialize parallelism groups
rank 21 After initialize parallelism groups
rank 22 After initialize parallelism groups
rank 17 After initialize parallelism groups
rank 16 After initialize parallelism groups
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
config_path  configs/ERA5-100million-91variables.yaml
{'seed_everything': 42, 'trainer': {'checkpoint_path': '/lustre/orion/stf218/world-shared/atsaris/tmp/tmp_checkpoints', 'checkpoint_filename': 'multi_last', 'resume_from_checkpoint': False}, 'parallelism': {'cpu_offloading': False}, 'model': {'lr': 2e-05, 'beta_1': 0.9, 'beta_2': 0.95, 'weight_decay': '1e-5', 'warmup_steps': 1000, 'max_steps': 20000, 'warmup_start_lr': '1e-8', 'eta_min': '1e-8', 'net': {'class_path': 'climax.arch.ClimaX', 'init_args': {'default_vars': ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_70', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000'], 'patch_size': 4, 'decoder_depth': 2, 'mlp_ratio': 4, 'drop_path': 0.1, 'drop_rate': 0.1, 'aggregated_variables': 1}}}, 'data': {'dict_root_dirs': {'mpi-esm': '/lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0'}, 'dict_start_idx': {'mpi-esm': 0}, 'dict_end_idx': {'mpi-esm': 1}, 'dict_in_variables': {'mpi-esm': ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000']}, 'dict_out_variables': {'mpi-esm': ['geopotential_500', 'temperature_850', '2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind']}, 'dict_max_predict_ranges': {'mpi-esm': 720}, 'dict_random_lead_time': {'mpi-esm': True}, 'dict_hrs_each_step': {'mpi-esm': 1}, 'dict_buffer_sizes': {'mpi-esm': 1}, 'num_workers': 1, 'pin_memory': False}}
rank 6 Before initialize parallelism groups
rank 2 Before initialize parallelism groups
rank 38 Before initialize parallelism groups
rank 39 Before initialize parallelism groups
rank 37 Before initialize parallelism groups
rank 30 Before initialize parallelism groups
rank 31 Before initialize parallelism groups
rank 45 Before initialize parallelism groups
rank 28 Before initialize parallelism groups
rank 46 Before initialize parallelism groups
rank 53 Before initialize parallelism groups
rank 61 Before initialize parallelism groups
rank 18 Before initialize parallelism groups
rank 63 Before initialize parallelism groups
rank 20 Before initialize parallelism groups
rank 23 Before initialize parallelism groups
rank 58 Before initialize parallelism groups
max_epochs 1 data_par_size 1 fsdp_size 1 simple_ddp_size 1 tensor_par_size 64 seq_par_size 1 cpu_offloading False
lr  2e-05 beta_1  0.9 beta_2 0.95 weight_decay 1e-05 class_path climax.arch.ClimaX default_vars ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_70', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000']
img_size_x 128 img_size_y 256 patch_size 4 emb_dim 8192 depth 32 decoder_depth 2 num_heads 64 mlp_ratio 4 drop_path 0.1 drop_rate 0.1 aggregated_variables 1
dict_root_dirs {'mpi-esm': '/lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0'} dict_start_idx {'mpi-esm': 0} dict_end_idx {'mpi-esm': 1} batch_size 2 num_workers 1
warmup_steps 1000 max_steps 20000 warmup_start_lr 1e-08 eta_min 1e-08
checkpoint_path /lustre/orion/stf218/world-shared/atsaris/tmp/tmp_checkpoints checkpoint_filename multi_last resume_from_checkpoint False
rank 0 Before initialize parallelism groups
rank 39 After initialize parallelism groups
rank 6 After initialize parallelism groups
rank 38 After initialize parallelism groups
rank 37 After initialize parallelism groups
rank 2 After initialize parallelism groups
rank 53 After initialize parallelism groups
rank 30 After initialize parallelism groups
rank 45 After initialize parallelism groups
rank 61 After initialize parallelism groups
rank 46 After initialize parallelism groups
rank 28 After initialize parallelism groups
rank 31 After initialize parallelism groups
rank 63 After initialize parallelism groups
rank 20 After initialize parallelism groups
rank 18 After initialize parallelism groups
rank 23 After initialize parallelism groups
rank 58 After initialize parallelism groups
rank 0 After initialize parallelism groups
rank 42 After initialize model
rank 42 Before the second dist.barrier()
rank 41 After initialize model
rank 41 Before the second dist.barrier()
rank 45 After initialize model
rank 45 Before the second dist.barrier()
rank 43 After initialize model
rank 43 Before the second dist.barrier()
rank 47 After initialize model
rank 47 Before the second dist.barrier()
rank 46 After initialize model
rank 46 Before the second dist.barrier()
rank 40 After initialize model
rank 40 Before the second dist.barrier()
rank 44 After initialize model
rank 44 Before the second dist.barrier()
rank 26 After initialize model
rank 26 Before the second dist.barrier()
rank 25 After initialize model
rank 25 Before the second dist.barrier()
rank 27 After initialize model
rank 27 Before the second dist.barrier()
rank 34 After initialize model
rank 34 Before the second dist.barrier()
rank 31 After initialize model
rank 31 Before the second dist.barrier()
rank 33 After initialize model
rank 33 Before the second dist.barrier()
rank 28 After initialize model
rank 28 Before the second dist.barrier()
rank 36 After initialize model
rank 36 Before the second dist.barrier()
rank 49 After initialize model
rank 29 After initialize model
rank 49 Before the second dist.barrier()
rank 29 Before the second dist.barrier()
rank 51 After initialize model
rank 51 Before the second dist.barrier()
rank 35 After initialize model
rank 35 Before the second dist.barrier()
rank 50 After initialize model
rank 50 Before the second dist.barrier()
rank 30 After initialize model
rank 30 Before the second dist.barrier()
rank 24 After initialize model
rank 24 Before the second dist.barrier()
rank 55 After initialize model
rank 55 Before the second dist.barrier()
rank 38 After initialize model
rank 38 Before the second dist.barrier()
rank 53 After initialize model
rank 53 Before the second dist.barrier()
rank 39 After initialize model
rank 39 Before the second dist.barrier()
rank 37 After initialize model
rank 37 Before the second dist.barrier()
rank 52 After initialize model
rank 52 Before the second dist.barrier()
rank 32 After initialize model
rank 32 Before the second dist.barrier()
rank 48 After initialize model
rank 48 Before the second dist.barrier()
rank 54 After initialize model
rank 54 Before the second dist.barrier()
rank 18 After initialize model
rank 18 Before the second dist.barrier()
rank 21 After initialize model
rank 21 Before the second dist.barrier()
rank 23 After initialize model
rank 23 Before the second dist.barrier()
rank 20 After initialize model
rank 20 Before the second dist.barrier()
rank 17 After initialize model
rank 17 Before the second dist.barrier()
rank 19 After initialize model
rank 19 Before the second dist.barrier()
rank 22 After initialize model
rank 22 Before the second dist.barrier()
rank 2 After initialize model
rank 2 Before the second dist.barrier()
rank 16 After initialize model
rank 16 Before the second dist.barrier()
rank 4 After initialize model
rank 4 Before the second dist.barrier()
rank 58 After initialize model
rank 58 Before the second dist.barrier()
rank 1 After initialize model
rank 1 Before the second dist.barrier()
rank 3 After initialize model
rank 7 After initialize model
rank 3 Before the second dist.barrier()
rank 7 Before the second dist.barrier()
rank 63 After initialize model
rank 63 Before the second dist.barrier()
rank 59 After initialize model
rank 59 Before the second dist.barrier()
rank 0 After initialize model
rank 0 Before the second dist.barrier()
rank 6 After initialize model
rank 6 Before the second dist.barrier()
rank 9 After initialize model
rank 60 After initialize model
rank 9 Before the second dist.barrier()
rank 60 Before the second dist.barrier()
rank 57 After initialize model
rank 57 Before the second dist.barrier()
rank 5 After initialize model
rank 5 Before the second dist.barrier()
rank 10 After initialize model
rank 10 Before the second dist.barrier()
rank 15 After initialize model
rank 15 Before the second dist.barrier()
rank 62 After initialize model
rank 62 Before the second dist.barrier()
rank 61 After initialize model
rank 61 Before the second dist.barrier()
rank 56 After initialize model
rank 56 Before the second dist.barrier()
rank 12 After initialize model
rank 12 Before the second dist.barrier()
rank 11 After initialize model
rank 11 Before the second dist.barrier()
rank 13 After initialize model
rank 13 Before the second dist.barrier()
rank 8 After initialize model
rank 8 Before the second dist.barrier()
rank 14 After initialize model
rank 14 Before the second dist.barrier()
rank 35 After the second dist.barrier()
rank 36 After the second dist.barrier()
rank 37 After the second dist.barrier()
rank 8 After the second dist.barrier()
rank 0 After the second dist.barrier()
rank 16 After the second dist.barrier()
rank 40 After the second dist.barrier()
rank 48 After the second dist.barrier()
rank 24 After the second dist.barrier()
rank 56 After the second dist.barrier()
rank 39 After the second dist.barrier()
rank 9 After the second dist.barrier()
rank 1 After the second dist.barrier()
rank 17 After the second dist.barrier()
rank 41 After the second dist.barrier()
rank 49 After the second dist.barrier()
rank 25 After the second dist.barrier()
rank 57 After the second dist.barrier()
rank 32 After the second dist.barrier()
rank 13 After the second dist.barrier()
rank 2 After the second dist.barrier()
rank 21 After the second dist.barrier()
rank 45 After the second dist.barrier()
rank 52 After the second dist.barrier()
rank 28 After the second dist.barrier()
rank 61 After the second dist.barrier()
rank 33 After the second dist.barrier()
rank 12 After the second dist.barrier()
rank 3 After the second dist.barrier()
rank 20 After the second dist.barrier()
rank 44 After the second dist.barrier()
rank 53 After the second dist.barrier()
rank 29 After the second dist.barrier()
rank 60 After the second dist.barrier()
rank 34 After the second dist.barrier()
rank 14 After the second dist.barrier()
rank 4 After the second dist.barrier()
rank 22 After the second dist.barrier()
rank 46 After the second dist.barrier()
rank 54 After the second dist.barrier()
rank 30 After the second dist.barrier()
rank 62 After the second dist.barrier()
rank 38 After the second dist.barrier()
rank 15 After the second dist.barrier()
rank 6 After the second dist.barrier()
rank 23 After the second dist.barrier()
rank 47 After the second dist.barrier()
rank 55 After the second dist.barrier()
rank 31 After the second dist.barrier()
rank 63 After the second dist.barrier()
rank 11 After the second dist.barrier()
rank 7 After the second dist.barrier()
rank 18 After the second dist.barrier()
rank 42 After the second dist.barrier()
rank 50 After the second dist.barrier()
rank 27 After the second dist.barrier()
rank 59 After the second dist.barrier()
rank 10 After the second dist.barrier()
rank 5 After the second dist.barrier()
rank 19 After the second dist.barrier()
rank 43 After the second dist.barrier()
rank 51 After the second dist.barrier()
rank 26 After the second dist.barrier()
rank 58 After the second dist.barrier()
parameter name  var_embed  requires_gradient  True size torch.Size([1, 256, 8192])
parameter name  var_query  requires_gradient  True size torch.Size([1, 1, 8192])
parameter name  pos_embed  requires_gradient  True size torch.Size([1, 2048, 8192])
parameter name  token_embeds.0.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.0.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.1.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.1.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.2.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.2.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.3.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.3.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.4.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.4.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.5.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.5.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.6.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.6.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.7.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.7.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.8.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.8.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.9.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.9.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.10.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.10.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.11.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.11.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.12.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.12.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.13.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.13.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.14.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.14.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.15.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.15.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.16.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.16.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.17.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.17.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.18.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.18.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.19.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.19.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.20.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.20.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.21.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.21.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.22.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.22.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.23.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.23.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.24.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.24.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.25.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.25.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.26.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.26.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.27.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.27.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.28.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.28.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.29.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.29.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.30.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.30.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.31.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.31.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.32.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.32.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.33.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.33.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.34.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.34.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.35.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.35.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.36.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.36.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.37.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.37.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.38.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.38.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.39.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.39.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.40.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.40.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.41.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.41.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.42.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.42.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.43.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.43.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.44.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.44.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.45.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.45.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.46.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.46.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.47.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.47.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.48.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.48.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.49.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.49.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.50.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.50.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.51.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.51.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.52.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.52.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.53.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.53.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.54.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.54.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.55.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.55.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.56.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.56.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.57.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.57.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.58.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.58.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.59.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.59.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.60.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.60.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.61.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.61.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.62.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.62.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.63.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.63.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.64.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.64.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.65.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.65.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.66.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.66.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.67.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.67.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.68.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.68.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.69.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.69.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.70.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.70.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.71.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.71.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.72.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.72.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.73.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.73.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.74.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.74.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.75.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.75.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.76.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.76.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.77.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.77.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.78.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.78.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.79.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.79.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.80.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.80.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.81.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.81.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.82.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.82.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.83.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.83.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.84.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.84.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.85.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.85.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.86.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.86.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.87.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.87.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.88.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.88.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.89.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.89.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.90.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.90.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.91.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.91.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.92.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.92.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.93.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.93.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.94.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.94.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.95.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.95.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.96.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.96.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.97.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.97.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.98.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.98.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.99.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.99.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.100.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.100.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.101.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.101.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.102.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.102.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.103.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.103.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.104.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.104.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.105.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.105.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.106.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.106.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.107.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.107.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.108.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.108.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.109.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.109.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.110.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.110.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.111.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.111.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.112.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.112.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.113.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.113.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.114.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.114.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.115.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.115.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.116.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.116.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.117.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.117.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.118.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.118.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.119.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.119.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.120.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.120.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.121.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.121.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.122.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.122.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.123.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.123.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.124.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.124.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.125.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.125.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.126.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.126.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.127.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.127.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.128.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.128.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.129.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.129.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.130.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.130.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.131.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.131.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.132.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.132.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.133.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.133.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.134.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.134.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.135.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.135.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.136.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.136.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.137.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.137.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.138.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.138.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.139.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.139.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.140.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.140.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.141.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.141.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.142.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.142.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.143.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.143.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.144.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.144.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.145.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.145.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.146.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.146.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.147.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.147.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.148.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.148.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.149.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.149.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.150.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.150.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.151.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.151.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.152.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.152.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.153.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.153.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.154.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.154.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.155.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.155.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.156.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.156.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.157.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.157.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.158.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.158.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.159.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.159.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.160.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.160.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.161.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.161.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.162.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.162.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.163.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.163.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.164.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.164.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.165.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.165.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.166.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.166.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.167.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.167.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.168.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.168.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.169.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.169.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.170.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.170.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.171.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.171.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.172.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.172.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.173.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.173.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.174.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.174.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.175.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.175.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.176.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.176.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.177.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.177.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.178.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.178.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.179.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.179.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.180.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.180.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.181.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.181.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.182.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.182.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.183.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.183.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.184.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.184.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.185.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.185.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.186.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.186.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.187.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.187.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.188.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.188.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.189.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.189.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.190.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.190.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.191.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.191.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.192.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.192.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.193.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.193.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.194.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.194.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.195.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.195.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.196.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.196.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.197.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.197.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.198.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.198.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.199.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.199.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.200.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.200.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.201.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.201.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.202.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.202.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.203.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.203.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.204.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.204.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.205.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.205.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.206.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.206.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.207.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.207.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.208.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.208.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.209.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.209.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.210.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.210.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.211.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.211.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.212.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.212.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.213.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.213.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.214.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.214.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.215.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.215.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.216.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.216.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.217.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.217.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.218.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.218.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.219.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.219.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.220.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.220.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.221.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.221.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.222.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.222.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.223.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.223.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.224.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.224.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.225.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.225.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.226.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.226.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.227.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.227.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.228.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.228.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.229.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.229.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.230.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.230.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.231.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.231.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.232.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.232.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.233.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.233.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.234.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.234.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.235.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.235.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.236.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.236.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.237.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.237.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.238.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.238.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.239.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.239.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.240.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.240.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.241.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.241.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.242.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.242.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.243.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.243.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.244.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.244.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.245.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.245.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.246.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.246.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.247.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.247.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.248.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.248.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.249.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.249.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.250.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.250.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.251.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.251.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.252.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.252.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.253.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.253.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.254.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.254.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.255.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.255.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  var_agg.q.weight  requires_gradient  True size torch.Size([128, 8192])
parameter name  var_agg.kv.weight  requires_gradient  True size torch.Size([256, 8192])
parameter name  var_agg.proj.weight  requires_gradient  True size torch.Size([8192, 128])
parameter name  var_agg.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  lead_time_embed.weight  requires_gradient  True size torch.Size([8192, 1])
parameter name  lead_time_embed.bias  requires_gradient  True size torch.Size([8192])
parameter name  head.0.weight  requires_gradient  True size torch.Size([8192, 8192])
parameter name  head.0.bias  requires_gradient  True size torch.Size([8192])
parameter name  head.2.weight  requires_gradient  True size torch.Size([8192, 8192])
parameter name  head.2.bias  requires_gradient  True size torch.Size([8192])
parameter name  head.5.weight  requires_gradient  True size torch.Size([4096, 8192])
parameter name  head.5.bias  requires_gradient  True size torch.Size([4096])
total_params before FSDP tensor(491302912) params_per_gpu tensor(226545664)
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
rank  63 lister_train reset: mpi-esm 9 9 9
rank  63 lister_test reset: mpi-esm 9 9 9
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
total_params after FSDP FullyShardedDataParallel(
  (_fsdp_wrapped_module): ClimaX(
    (token_embeds): ModuleList(
      (0-255): 256 x PatchEmbed(
        (proj): Conv2d(1, 8192, kernel_size=(4, 4), stride=(4, 4))
        (norm): Identity()
      )
    )
    (var_agg): FullyShardedDataParallel(
      (_fsdp_wrapped_module): CheckpointWrapper(
        (_checkpoint_wrapped_module): VariableMapping_Attention(
          (q): Linear(in_features=8192, out_features=128, bias=False)
          (kv): Linear(in_features=8192, out_features=256, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=8192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (lead_time_embed): Linear(in_features=1, out_features=8192, bias=True)
    (pos_drop): Dropout(p=0.1, inplace=False)
    (head): Sequential(
      (0): Linear(in_features=8192, out_features=8192, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=8192, out_features=8192, bias=True)
      (3): GELU(approximate='none')
      (4): Pred_Rearrange()
      (5): Linear(in_features=8192, out_features=4096, bias=True)
    )
  )
)
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
len(dict_root_dirs) 1
rank  0 lister_train reset: mpi-esm 9 9 9
rank  0 lister_test reset: mpi-esm 9 9 9
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
memory stats reset, ready to track
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
MIOpen(HIP): Warning [PlainTextDb] Unable to create a directory: "/tmp/.config/miopen"
MIOpen(HIP): Warning [PlainTextDb] Unable to create a directory: "/tmp/.config/miopen"
MIOpen(HIP): Warning [PlainTextDb] Unable to create a directory: "/tmp/.config/miopen"
MIOpen(HIP): Warning [PlainTextDb] Unable to create a directory: "/tmp/.config/miopen"
MIOpen(HIP): Warning [PlainTextDb] Unable to create a directory: "/tmp/.config/miopen"
MIOpen(HIP): Warning [PlainTextDb] Unable to create a directory: "/tmp/.config/miopen"
MIOpen(HIP): Warning [PlainTextDb] Unable to create a directory: "/tmp/.config/miopen"
MIOpen(HIP): Warning [PlainTextDb] Unable to create a directory: "/tmp/.config/miopen"
MIOpen(HIP): Warning [PlainTextDb] Unable to create a directory: "/tmp/.config/miopen"
MIOpen(HIP): Warning [PlainTextDb] Unable to create a directory: "/tmp/.config/miopen"
MIOpen(HIP): Warning [PlainTextDb] Unable to create a directory: "/tmp/.config/miopen"
MIOpen(HIP): Warning [PlainTextDb] Unable to create a directory: "/tmp/.config/miopen"
MIOpen(HIP): Warning [PlainTextDb] Unable to create a directory: "/tmp/.config/miopen"
MIOpen(HIP): Warning [PlainTextDb] Unable to create a directory: "/tmp/.config/miopen"
MIOpen(HIP): Warning [PlainTextDb] Unable to create a directory: "/tmp/.config/miopen"
MIOpen(HIP): Warning [PlainTextDb] Unable to create a directory: "/tmp/.config/miopen"
MIOpen(HIP): Warning [PlainTextDb] Unable to create a directory: "/tmp/.config/miopen"
epoch-train:  0 batch_idx 0 world_rank 0  loss  77.5
epoch-train:  0 batch_idx 1 world_rank 0  loss  77.0
epoch-train:  0 batch_idx 2 world_rank 0  loss  77.0
[2025-04-09 13:56:08,337] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,337] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,338] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,338] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,338] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,338] [INFO] [profiler.py:80:start_profile] Flops profiler started
epoch-train:  0 batch_idx 3 world_rank 0  loss  77.0
[2025-04-09 13:56:08,339] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,339] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,339] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,340] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,340] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,340] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,340] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,340] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,340] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,340] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,341] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,341] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,341] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,341] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,341] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,341] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,341] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,341] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,341] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,341] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,341] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,341] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,341] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,341] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,341] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,342] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,342] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,342] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,342] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,342] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,342] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,342] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,342] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,342] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,343] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,343] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,343] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,343] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,343] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,343] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,343] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,343] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,343] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,343] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,344] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,344] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,344] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,344] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,344] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,344] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,344] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,345] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,345] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,346] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,347] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,347] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,347] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:08,348] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:09,222] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,223] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,223] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,224] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,224] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,224] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,224] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,224] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,224] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,224] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,224] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,224] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,224] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,224] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,224] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,224] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,225] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,225] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,225] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,225] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,225] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,226] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,226] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,226] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,226] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,226] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,226] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,226] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,226] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,226] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,227] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,227] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,227] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,227] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,227] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,227] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,227] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,227] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,227] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,227] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,227] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,227] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,228] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,228] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,228] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,228] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,228] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,228] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,228] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,229] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,229] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,230] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,230] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,230] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,230] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,230] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,231] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,231] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,231] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,232] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,232] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,233] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,234] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:09,235] [INFO] [profiler.py:226:end_profile] Flops profiler finished
global rank 21: ddp rank 0 iter_start,iter_end = 0 8
global rank 21: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 23: ddp rank 0 iter_start,iter_end = 0 8
global rank 23: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 11: ddp rank 0 iter_start,iter_end = 0 8
global rank 11: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 37: ddp rank 0 iter_start,iter_end = 0 8
global rank 37: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 16: ddp rank 0 iter_start,iter_end = 0 8
global rank 16: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 49: ddp rank 0 iter_start,iter_end = 0 8
global rank 49: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 35: ddp rank 0 iter_start,iter_end = 0 8
global rank 35: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 22: ddp rank 0 iter_start,iter_end = 0 8
global rank 22: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 50: ddp rank 0 iter_start,iter_end = 0 8
global rank 50: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 38: ddp rank 0 iter_start,iter_end = 0 8
global rank 38: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 53: ddp rank 0 iter_start,iter_end = 0 8
global rank 53: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 56: ddp rank 0 iter_start,iter_end = 0 8
global rank 56: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 51: ddp rank 0 iter_start,iter_end = 0 8
global rank 51: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 55: ddp rank 0 iter_start,iter_end = 0 8
global rank 55: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 63: ddp rank 0 iter_start,iter_end = 0 8
global rank 63: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 57: ddp rank 0 iter_start,iter_end = 0 8
global rank 57: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 58: ddp rank 0 iter_start,iter_end = 0 8
global rank 58: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 61: ddp rank 0 iter_start,iter_end = 0 8
global rank 61: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 25: ddp rank 0 iter_start,iter_end = 0 8
global rank 25: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 9: ddp rank 0 iter_start,iter_end = 0 8
global rank 9: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 0: ddp rank 0 iter_start,iter_end = 0 8
global rank 0: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 14: ddp rank 0 iter_start,iter_end = 0 8
global rank 14: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 13: ddp rank 0 iter_start,iter_end = 0 8
global rank 13: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 47: ddp rank 0 iter_start,iter_end = 0 8
global rank 47: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 5: ddp rank 0 iter_start,iter_end = 0 8
global rank 5: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 27: ddp rank 0 iter_start,iter_end = 0 8
global rank 27: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 40: ddp rank 0 iter_start,iter_end = 0 8
global rank 40: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 28: ddp rank 0 iter_start,iter_end = 0 8
global rank 28: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 8: ddp rank 0 iter_start,iter_end = 0 8
global rank 8: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 19: ddp rank 0 iter_start,iter_end = 0 8
global rank 19: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 6: ddp rank 0 iter_start,iter_end = 0 8
global rank 6: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 41: ddp rank 0 iter_start,iter_end = 0 8
global rank 41: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 44: ddp rank 0 iter_start,iter_end = 0 8
global rank 44: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 17: ddp rank 0 iter_start,iter_end = 0 8
global rank 17: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 20: ddp rank 0 iter_start,iter_end = 0 8
global rank 20: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 34: ddp rank 0 iter_start,iter_end = 0 8
global rank 34: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 32: ddp rank 0 iter_start,iter_end = 0 8
global rank 32: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 39: ddp rank 0 iter_start,iter_end = 0 8
global rank 39: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 62: ddp rank 0 iter_start,iter_end = 0 8
global rank 62: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 60: ddp rank 0 iter_start,iter_end = 0 8
global rank 60: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 59: ddp rank 0 iter_start,iter_end = 0 8
global rank 59: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 36: ddp rank 0 iter_start,iter_end = 0 8
global rank 36: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 48: ddp rank 0 iter_start,iter_end = 0 8
global rank 48: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 54: ddp rank 0 iter_start,iter_end = 0 8
global rank 54: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 52: ddp rank 0 iter_start,iter_end = 0 8
global rank 52: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 1: ddp rank 0 iter_start,iter_end = 0 8
global rank 1: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 18: ddp rank 0 iter_start,iter_end = 0 8
global rank 18: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 30: ddp rank 0 iter_start,iter_end = 0 8
global rank 30: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 33: ddp rank 0 iter_start,iter_end = 0 8
global rank 33: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 3: ddp rank 0 iter_start,iter_end = 0 8
global rank 3: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 43: ddp rank 0 iter_start,iter_end = 0 8
global rank 43: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 7: ddp rank 0 iter_start,iter_end = 0 8
global rank 7: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 12: ddp rank 0 iter_start,iter_end = 0 8
global rank 12: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 24: ddp rank 0 iter_start,iter_end = 0 8
global rank 24: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 29: ddp rank 0 iter_start,iter_end = 0 8
global rank 29: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 15: ddp rank 0 iter_start,iter_end = 0 8
global rank 15: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 4: ddp rank 0 iter_start,iter_end = 0 8
global rank 4: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 26: ddp rank 0 iter_start,iter_end = 0 8
global rank 26: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 45: ddp rank 0 iter_start,iter_end = 0 8
global rank 45: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 10: ddp rank 0 iter_start,iter_end = 0 8
global rank 10: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 2: ddp rank 0 iter_start,iter_end = 0 8
global rank 2: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 46: ddp rank 0 iter_start,iter_end = 0 8
global rank 46: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 42: ddp rank 0 iter_start,iter_end = 0 8
global rank 42: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 31: ddp rank 0 iter_start,iter_end = 0 8
global rank 31: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
MUST: Model 226.545664 M TFLOPS: 6.218261705355411

--> cuda max reserved memory = 53.5781
--> max reserved percentage = 83.74 %

--> cuda max memory allocated = 52.5267
--> max allocated percentage = 82.09 %

--> peak active memory = 52.5267
--> peak active memory 82.09 %

cudaMalloc retries = 1
cuda OOM = 0

--> Recommend decreasing batch size...cuda retries can greatly degrade perf!
HERE1 Namespace(arch='orbit_token_agg', batch_size=2, channels=256, depth=32, embed_dim=8192, fa2=True, fsdp_size=1, imagex=128, imagey=256, max_epochs=1, num_heads=64, real=False, seq_par_size=1, simple_ddp_size=1, tensor_par_size=64, yaml_config='configs/ERA5-100million-91variables.yaml') 53.5781 6.218261705355411 tensor(491302912)
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,260] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,260] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,260] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,260] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,260] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,260] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,260] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,260] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,263] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,263] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,263] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,263] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,263] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,263] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,263] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,263] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,263] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,263] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,263] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,263] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,263] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,263] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,263] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,263] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,264] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,264] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,264] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,264] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,264] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,264] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,264] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,264] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,264] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,264] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,264] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,264] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,264] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,264] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,264] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,264] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,265] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,265] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,265] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,265] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,265] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,265] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,265] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,265] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,266] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,266] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,266] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,266] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,266] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,266] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,266] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,266] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,395] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,395] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,395] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,395] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,395] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,395] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,395] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:56:23,395] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
HERE0 Namespace(arch='orbit_linear_token_agg', batch_size=2, channels=256, depth=32, embed_dim=8192, fa2=True, fsdp_size=1, imagex=128, imagey=256, max_epochs=1, num_heads=64, real=False, seq_par_size=1, simple_ddp_size=1, tensor_par_size=64, yaml_config='configs/ERA5-100million-91variables.yaml')
[W socket.cpp:464] [c10d] The server socket cannot be initialized on [::]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
rank 5 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
rank 48 Before initialize parallelism groups
rank 49 Before initialize parallelism groups
rank 52 Before initialize parallelism groups
rank 53 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 54 Before initialize parallelism groups
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
rank 62 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
rank 43 Before initialize parallelism groups
rank 50 Before initialize parallelism groups
rank 20 Before initialize parallelism groups
rank 16 Before initialize parallelism groups
rank 57 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 63 Before initialize parallelism groups
rank 3 Before initialize parallelism groups
rank 24 Before initialize parallelism groups
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 6 Before initialize parallelism groups
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
rank 40 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
rank 41 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 5 After initialize parallelism groups
Using dist.init_process_group. world_size  64
rank 32 Before initialize parallelism groups
rank 49 After initialize parallelism groups
rank 52 After initialize parallelism groups
rank 48 After initialize parallelism groups
rank 54 After initialize parallelism groups
rank 30 Before initialize parallelism groups
rank 53 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
rank 62 After initialize parallelism groups
rank 18 Before initialize parallelism groups
rank 22 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 27 Before initialize parallelism groups
rank 42 Before initialize parallelism groups
rank 29 Before initialize parallelism groups
rank 46 Before initialize parallelism groups
rank 50 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
rank 31 Before initialize parallelism groups
rank 58 Before initialize parallelism groups
Using dist.init_process_group. world_size  64
rank 57 After initialize parallelism groups
rank 20 After initialize parallelism groups
rank 43 After initialize parallelism groups
rank 16 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 63 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 38 Before initialize parallelism groups
rank 3 After initialize parallelism groups
rank 24 After initialize parallelism groups
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
rank 2 Before initialize parallelism groups
rank 6 After initialize parallelism groups
rank 17 Before initialize parallelism groups
rank 40 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 41 After initialize parallelism groups
rank 59 Before initialize parallelism groups
rank 34 Before initialize parallelism groups
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 1 Before initialize parallelism groups
rank 25 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 21 Before initialize parallelism groups
rank 30 After initialize parallelism groups
rank 32 After initialize parallelism groups
rank 47 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 45 Before initialize parallelism groups
rank 22 After initialize parallelism groups
rank 18 After initialize parallelism groups
rank 27 After initialize parallelism groups
rank 42 After initialize parallelism groups
rank 29 After initialize parallelism groups
rank 31 After initialize parallelism groups
rank 46 After initialize parallelism groups
rank 58 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 33 Before initialize parallelism groups
rank 38 After initialize parallelism groups
rank 2 After initialize parallelism groups
rank 17 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 61 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 35 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 59 After initialize parallelism groups
rank 60 Before initialize parallelism groups
rank 34 After initialize parallelism groups
rank 26 Before initialize parallelism groups
rank 1 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 25 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 21 After initialize parallelism groups
rank 47 After initialize parallelism groups
rank 56 Before initialize parallelism groups
rank 45 After initialize parallelism groups
rank 33 After initialize parallelism groups
rank 61 After initialize parallelism groups
rank 35 After initialize parallelism groups
rank 60 After initialize parallelism groups
rank 26 After initialize parallelism groups
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
rank 56 After initialize parallelism groups
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
rank 19 Before initialize parallelism groups
rank 28 Before initialize parallelism groups
rank 23 Before initialize parallelism groups
rank 36 Before initialize parallelism groups
rank 4 Before initialize parallelism groups
rank 37 Before initialize parallelism groups
rank 7 Before initialize parallelism groups
rank 39 Before initialize parallelism groups
rank 44 Before initialize parallelism groups
rank 51 Before initialize parallelism groups
rank 55 Before initialize parallelism groups
rank 19 After initialize parallelism groups
rank 23 After initialize parallelism groups
rank 28 After initialize parallelism groups
rank 36 After initialize parallelism groups
rank 37 After initialize parallelism groups
rank 4 After initialize parallelism groups
rank 7 After initialize parallelism groups
rank 44 After initialize parallelism groups
rank 39 After initialize parallelism groups
rank 14 Before initialize parallelism groups
rank 51 After initialize parallelism groups
rank 55 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
rank 14 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
rank 8 Before initialize parallelism groups
rank 10 Before initialize parallelism groups
rank 13 Before initialize parallelism groups
rank 9 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
rank 15 Before initialize parallelism groups
rank 10 After initialize parallelism groups
rank 8 After initialize parallelism groups
rank 13 After initialize parallelism groups
Using dist.init_process_group. world_size  64
config_path  configs/ERA5-100million-91variables.yaml
rank 11 Before initialize parallelism groups
rank 9 After initialize parallelism groups
rank 15 After initialize parallelism groups
rank 12 Before initialize parallelism groups
{'seed_everything': 42, 'trainer': {'checkpoint_path': '/lustre/orion/stf218/world-shared/atsaris/tmp/tmp_checkpoints', 'checkpoint_filename': 'multi_last', 'resume_from_checkpoint': False}, 'parallelism': {'cpu_offloading': False}, 'model': {'lr': 2e-05, 'beta_1': 0.9, 'beta_2': 0.95, 'weight_decay': '1e-5', 'warmup_steps': 1000, 'max_steps': 20000, 'warmup_start_lr': '1e-8', 'eta_min': '1e-8', 'net': {'class_path': 'climax.arch.ClimaX', 'init_args': {'default_vars': ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_70', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000'], 'patch_size': 4, 'decoder_depth': 2, 'mlp_ratio': 4, 'drop_path': 0.1, 'drop_rate': 0.1, 'aggregated_variables': 1}}}, 'data': {'dict_root_dirs': {'mpi-esm': '/lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0'}, 'dict_start_idx': {'mpi-esm': 0}, 'dict_end_idx': {'mpi-esm': 1}, 'dict_in_variables': {'mpi-esm': ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000']}, 'dict_out_variables': {'mpi-esm': ['geopotential_500', 'temperature_850', '2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind']}, 'dict_max_predict_ranges': {'mpi-esm': 720}, 'dict_random_lead_time': {'mpi-esm': True}, 'dict_hrs_each_step': {'mpi-esm': 1}, 'dict_buffer_sizes': {'mpi-esm': 1}, 'num_workers': 1, 'pin_memory': False}}
rank 11 After initialize parallelism groups
max_epochs 1 data_par_size 1 fsdp_size 1 simple_ddp_size 1 tensor_par_size 64 seq_par_size 1 cpu_offloading False
lr  2e-05 beta_1  0.9 beta_2 0.95 weight_decay 1e-05 class_path climax.arch.ClimaX default_vars ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_70', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000']
img_size_x 128 img_size_y 256 patch_size 4 emb_dim 8192 depth 32 decoder_depth 2 num_heads 64 mlp_ratio 4 drop_path 0.1 drop_rate 0.1 aggregated_variables 1
dict_root_dirs {'mpi-esm': '/lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0'} dict_start_idx {'mpi-esm': 0} dict_end_idx {'mpi-esm': 1} batch_size 2 num_workers 1
warmup_steps 1000 max_steps 20000 warmup_start_lr 1e-08 eta_min 1e-08
checkpoint_path /lustre/orion/stf218/world-shared/atsaris/tmp/tmp_checkpoints checkpoint_filename multi_last resume_from_checkpoint False
rank 0 Before initialize parallelism groups
rank 12 After initialize parallelism groups
rank 0 After initialize parallelism groups
rank 26 After initialize model
rank 26 Before the second dist.barrier()
rank 25 After initialize model
rank 25 Before the second dist.barrier()
rank 18 After initialize model
rank 18 Before the second dist.barrier()
rank 28 After initialize model
rank 28 Before the second dist.barrier()
rank 16 After initialize model
rank 16 Before the second dist.barrier()
rank 24 After initialize model
rank 24 Before the second dist.barrier()
rank 4 After initialize model
rank 4 Before the second dist.barrier()
rank 34 After initialize model
rank 34 Before the second dist.barrier()
rank 58 After initialize model
rank 58 Before the second dist.barrier()
rank 6 After initialize model
rank 6 Before the second dist.barrier()
rank 63 After initialize model
rank 63 Before the second dist.barrier()
rank 61 After initialize model
rank 61 Before the second dist.barrier()
rank 20 After initialize model
rank 20 Before the second dist.barrier()
rank 22 After initialize model
rank 22 Before the second dist.barrier()
rank 50 After initialize model
rank 50 Before the second dist.barrier()
rank 57 After initialize model
rank 57 Before the second dist.barrier()
rank 36 After initialize model
rank 36 Before the second dist.barrier()
rank 49 After initialize model
rank 49 Before the second dist.barrier()
rank 10 After initialize model
rank 10 Before the second dist.barrier()
rank 59 After initialize model
rank 59 Before the second dist.barrier()
rank 62 After initialize model
rank 62 Before the second dist.barrier()
rank 33 After initialize model
rank 33 Before the second dist.barrier()
rank 2 After initialize model
rank 2 Before the second dist.barrier()
rank 42 After initialize model
rank 42 Before the second dist.barrier()
rank 9 After initialize model
rank 9 Before the second dist.barrier()
rank 27 After initialize model
rank 27 Before the second dist.barrier()
rank 21 After initialize model
rank 21 Before the second dist.barrier()
rank 0 After initialize model
rank 0 Before the second dist.barrier()
rank 41 After initialize model
rank 41 Before the second dist.barrier()
rank 35 After initialize model
rank 35 Before the second dist.barrier()
rank 56 After initialize model
rank 56 Before the second dist.barrier()
rank 11 After initialize model
rank 11 Before the second dist.barrier()
rank 39 After initialize model
rank 39 Before the second dist.barrier()
rank 1 After initialize model
rank 1 Before the second dist.barrier()
rank 13 After initialize model
rank 13 Before the second dist.barrier()
rank 51 After initialize model
rank 51 Before the second dist.barrier()
rank 53 After initialize model
rank 52 After initialize model
rank 53 Before the second dist.barrier()
rank 52 Before the second dist.barrier()
rank 3 After initialize model
rank 3 Before the second dist.barrier()
rank 5 After initialize model
rank 5 Before the second dist.barrier()
rank 43 After initialize model
rank 43 Before the second dist.barrier()
rank 38 After initialize model
rank 38 Before the second dist.barrier()
rank 48 After initialize model
rank 48 Before the second dist.barrier()
rank 15 After initialize model
rank 15 Before the second dist.barrier()
rank 31 After initialize model
rank 31 Before the second dist.barrier()
rank 29 After initialize model
rank 29 Before the second dist.barrier()
rank 54 After initialize model
rank 54 Before the second dist.barrier()
rank 40 After initialize model
rank 40 Before the second dist.barrier()
rank 8 After initialize model
rank 47 After initialize model
rank 47 Before the second dist.barrier()
rank 8 Before the second dist.barrier()
rank 45 After initialize model
rank 45 Before the second dist.barrier()
rank 32 After initialize model
rank 32 Before the second dist.barrier()
rank 37 After initialize model
rank 37 Before the second dist.barrier()
rank 12 After initialize model
rank 12 Before the second dist.barrier()
rank 30 After initialize model
rank 30 Before the second dist.barrier()
rank 14 After initialize model
rank 14 Before the second dist.barrier()
rank 44 After initialize model
rank 44 Before the second dist.barrier()
rank 46 After initialize model
rank 46 Before the second dist.barrier()
rank 55 After initialize model
rank 55 Before the second dist.barrier()
rank 60 After initialize model
rank 60 Before the second dist.barrier()
rank 17 After initialize model
rank 17 Before the second dist.barrier()
rank 7 After initialize model
rank 7 Before the second dist.barrier()
rank 19 After initialize model
rank 19 Before the second dist.barrier()
rank 23 After initialize model
rank 23 Before the second dist.barrier()
rank 33 After the second dist.barrier()
rank 0 After the second dist.barrier()
rank 32 After the second dist.barrier()
rank 48 After the second dist.barrier()
rank 1 After the second dist.barrier()
rank 49 After the second dist.barrier()
rank 56 After the second dist.barrier()
rank 8 After the second dist.barrier()
parameter name  var_embed  requires_gradient  True size torch.Size([1, 256, 8192])
rank 9 After the second dist.barrier()
rank 37 After the second dist.barrier()
rank 57 After the second dist.barrier()
rank 4 After the second dist.barrier()
rank 11 After the second dist.barrier()
rank 5 After the second dist.barrier()
rank 16 After the second dist.barrier()
rank 40 After the second dist.barrier()
rank 24 After the second dist.barrier()
rank 14 After the second dist.barrier()
rank 17 After the second dist.barrier()
rank 41 After the second dist.barrier()
rank 36 After the second dist.barrier()
rank 15 After the second dist.barrier()
rank 10 After the second dist.barrier()
parameter name  var_query  requires_gradient  True size torch.Size([1, 1, 8192])
rank 52 After the second dist.barrier()
parameter name  var_query_per_rank  requires_gradient  True size torch.Size([1, 1, 8192])
rank 20 After the second dist.barrier()
rank 53 After the second dist.barrier()
rank 25 After the second dist.barrier()
rank 61 After the second dist.barrier()
rank 12 After the second dist.barrier()
parameter name  pos_embed  requires_gradient  True size torch.Size([1, 2048, 8192])
rank 21 After the second dist.barrier()
rank 13 After the second dist.barrier()
parameter name  token_embeds.0.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.0.proj.bias  requires_gradient  True size torch.Size([8192])
rank 38 After the second dist.barrier()
rank 60 After the second dist.barrier()
parameter name  token_embeds.1.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.1.proj.bias  requires_gradient  True size torch.Size([8192])
rank 28 After the second dist.barrier()
rank 6 After the second dist.barrier()
rank 44 After the second dist.barrier()
rank 29 After the second dist.barrier()
parameter name  token_embeds.2.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
rank 45 After the second dist.barrier()
parameter name  token_embeds.2.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.3.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
rank 54 After the second dist.barrier()
rank 39 After the second dist.barrier()
rank 7 After the second dist.barrier()
rank 22 After the second dist.barrier()
rank 55 After the second dist.barrier()
rank 62 After the second dist.barrier()
parameter name  token_embeds.3.proj.bias  requires_gradient  True size torch.Size([8192])
rank 63 After the second dist.barrier()
parameter name  token_embeds.4.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.4.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.5.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
rank 35 After the second dist.barrier()
parameter name  token_embeds.5.proj.bias  requires_gradient  True size torch.Size([8192])
rank 34 After the second dist.barrier()
parameter name  token_embeds.6.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
rank 23 After the second dist.barrier()
rank 46 After the second dist.barrier()
rank 30 After the second dist.barrier()
rank 2 After the second dist.barrier()
rank 47 After the second dist.barrier()
rank 3 After the second dist.barrier()
parameter name  token_embeds.6.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.7.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
rank 50 After the second dist.barrier()
parameter name  token_embeds.7.proj.bias  requires_gradient  True size torch.Size([8192])
rank 18 After the second dist.barrier()
rank 51 After the second dist.barrier()
rank 59 After the second dist.barrier()
parameter name  token_embeds.8.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
rank 19 After the second dist.barrier()
rank 31 After the second dist.barrier()
parameter name  token_embeds.8.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.9.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.9.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.10.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.10.proj.bias  requires_gradient  True size torch.Size([8192])
rank 43 After the second dist.barrier()
rank 26 After the second dist.barrier()
rank 58 After the second dist.barrier()
parameter name  token_embeds.11.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
rank 27 After the second dist.barrier()
parameter name  token_embeds.11.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.12.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.12.proj.bias  requires_gradient  True size torch.Size([8192])
rank 42 After the second dist.barrier()
parameter name  token_embeds.13.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.13.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.14.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.14.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.15.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.15.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.16.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.16.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.17.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.17.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.18.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.18.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.19.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.19.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.20.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.20.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.21.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.21.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.22.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.22.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.23.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.23.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.24.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.24.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.25.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.25.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.26.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.26.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.27.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.27.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.28.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.28.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.29.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.29.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.30.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.30.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.31.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.31.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.32.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.32.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.33.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.33.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.34.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.34.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.35.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.35.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.36.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.36.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.37.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.37.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.38.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.38.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.39.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.39.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.40.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.40.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.41.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.41.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.42.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.42.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.43.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.43.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.44.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.44.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.45.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.45.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.46.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.46.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.47.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.47.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.48.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.48.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.49.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.49.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.50.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.50.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.51.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.51.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.52.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.52.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.53.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.53.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.54.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.54.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.55.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.55.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.56.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.56.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.57.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.57.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.58.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.58.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.59.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.59.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.60.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.60.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.61.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.61.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.62.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.62.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.63.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.63.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.64.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.64.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.65.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.65.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.66.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.66.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.67.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.67.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.68.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.68.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.69.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.69.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.70.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.70.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.71.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.71.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.72.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.72.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.73.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.73.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.74.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.74.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.75.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.75.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.76.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.76.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.77.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.77.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.78.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.78.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.79.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.79.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.80.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.80.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.81.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.81.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.82.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.82.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.83.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.83.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.84.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.84.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.85.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.85.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.86.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.86.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.87.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.87.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.88.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.88.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.89.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.89.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.90.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.90.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.91.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.91.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.92.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.92.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.93.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.93.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.94.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.94.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.95.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.95.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.96.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.96.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.97.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.97.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.98.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.98.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.99.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.99.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.100.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.100.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.101.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.101.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.102.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.102.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.103.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.103.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.104.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.104.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.105.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.105.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.106.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.106.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.107.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.107.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.108.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.108.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.109.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.109.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.110.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.110.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.111.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.111.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.112.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.112.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.113.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.113.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.114.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.114.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.115.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.115.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.116.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.116.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.117.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.117.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.118.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.118.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.119.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.119.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.120.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.120.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.121.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.121.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.122.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.122.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.123.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.123.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.124.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.124.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.125.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.125.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.126.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.126.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.127.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.127.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.128.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.128.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.129.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.129.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.130.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.130.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.131.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.131.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.132.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.132.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.133.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.133.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.134.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.134.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.135.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.135.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.136.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.136.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.137.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.137.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.138.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.138.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.139.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.139.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.140.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.140.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.141.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.141.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.142.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.142.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.143.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.143.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.144.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.144.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.145.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.145.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.146.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.146.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.147.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.147.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.148.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.148.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.149.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.149.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.150.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.150.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.151.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.151.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.152.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.152.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.153.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.153.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.154.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.154.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.155.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.155.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.156.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.156.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.157.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.157.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.158.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.158.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.159.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.159.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.160.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.160.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.161.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.161.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.162.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.162.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.163.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.163.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.164.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.164.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.165.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.165.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.166.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.166.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.167.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.167.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.168.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.168.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.169.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.169.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.170.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.170.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.171.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.171.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.172.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.172.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.173.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.173.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.174.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.174.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.175.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.175.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.176.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.176.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.177.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.177.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.178.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.178.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.179.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.179.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.180.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.180.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.181.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.181.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.182.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.182.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.183.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.183.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.184.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.184.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.185.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.185.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.186.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.186.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.187.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.187.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.188.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.188.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.189.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.189.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.190.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.190.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.191.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.191.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.192.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.192.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.193.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.193.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.194.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.194.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.195.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.195.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.196.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.196.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.197.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.197.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.198.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.198.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.199.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.199.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.200.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.200.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.201.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.201.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.202.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.202.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.203.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.203.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.204.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.204.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.205.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.205.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.206.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.206.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.207.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.207.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.208.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.208.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.209.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.209.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.210.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.210.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.211.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.211.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.212.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.212.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.213.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.213.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.214.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.214.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.215.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.215.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.216.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.216.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.217.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.217.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.218.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.218.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.219.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.219.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.220.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.220.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.221.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.221.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.222.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.222.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.223.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.223.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.224.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.224.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.225.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.225.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.226.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.226.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.227.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.227.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.228.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.228.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.229.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.229.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.230.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.230.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.231.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.231.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.232.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.232.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.233.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.233.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.234.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.234.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.235.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.235.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.236.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.236.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.237.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.237.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.238.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.238.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.239.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.239.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.240.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.240.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.241.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.241.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.242.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.242.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.243.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.243.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.244.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.244.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.245.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.245.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.246.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.246.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.247.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.247.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.248.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.248.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.249.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.249.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.250.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.250.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.251.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.251.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.252.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.252.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.253.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.253.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.254.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.254.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.255.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.255.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  var_agg.q.weight  requires_gradient  True size torch.Size([128, 8192])
parameter name  var_agg.kv.weight  requires_gradient  True size torch.Size([256, 8192])
parameter name  var_agg.proj.weight  requires_gradient  True size torch.Size([8192, 128])
parameter name  var_agg.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  var_linear_per_rank.weight  requires_gradient  True size torch.Size([1, 4])
parameter name  var_linear_per_rank.bias  requires_gradient  True size torch.Size([1])
parameter name  lead_time_embed.weight  requires_gradient  True size torch.Size([8192, 1])
parameter name  lead_time_embed.bias  requires_gradient  True size torch.Size([8192])
parameter name  head.0.weight  requires_gradient  True size torch.Size([8192, 8192])
parameter name  head.0.bias  requires_gradient  True size torch.Size([8192])
parameter name  head.2.weight  requires_gradient  True size torch.Size([8192, 8192])
parameter name  head.2.bias  requires_gradient  True size torch.Size([8192])
parameter name  head.5.weight  requires_gradient  True size torch.Size([4096, 8192])
parameter name  head.5.bias  requires_gradient  True size torch.Size([4096])
total_params before FSDP tensor(491311109) params_per_gpu tensor(226553861)
total_params after FSDP FullyShardedDataParallel(
  (_fsdp_wrapped_module): ClimaX(
    (token_embeds): ModuleList(
      (0-255): 256 x PatchEmbed(
        (proj): Conv2d(1, 8192, kernel_size=(4, 4), stride=(4, 4))
        (norm): Identity()
      )
    )
    (var_agg): FullyShardedDataParallel(
      (_fsdp_wrapped_module): CheckpointWrapper(
        (_checkpoint_wrapped_module): VariableMapping_Attention(
          (q): Linear(in_features=8192, out_features=128, bias=False)
          (kv): Linear(in_features=8192, out_features=256, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=8192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (var_linear_per_rank): Linear(in_features=4, out_features=1, bias=True)
    (lead_time_embed): Linear(in_features=1, out_features=8192, bias=True)
    (pos_drop): Dropout(p=0.1, inplace=False)
    (head): Sequential(
      (0): Linear(in_features=8192, out_features=8192, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=8192, out_features=8192, bias=True)
      (3): GELU(approximate='none')
      (4): Pred_Rearrange()
      (5): Linear(in_features=8192, out_features=4096, bias=True)
    )
  )
)
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
rank  63 lister_train reset: mpi-esm 9 9 9
rank  63 lister_test reset: mpi-esm 9 9 9
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
rank  0 lister_train reset: mpi-esm 9 9 9
rank  0 lister_test reset: mpi-esm 9 9 9
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
memory stats reset, ready to track
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
epoch-train:  0 batch_idx 0 world_rank 0  loss  5.46875
epoch-train:  0 batch_idx 1 world_rank 0  loss  5.4375
epoch-train:  0 batch_idx 2 world_rank 0  loss  5.4375
[2025-04-09 13:56:53,701] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,701] [INFO] [profiler.py:80:start_profile] Flops profiler started
epoch-train:  0 batch_idx 3 world_rank 0  loss  5.4375
[2025-04-09 13:56:53,702] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,702] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,702] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,702] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,702] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,702] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,702] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,702] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,702] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,702] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,702] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,702] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,702] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,702] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,703] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,703] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,703] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,703] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,703] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,703] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,703] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,703] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,703] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,704] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,704] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,704] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,704] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,704] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,704] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,704] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,704] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,704] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,704] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,704] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,704] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,704] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,704] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,704] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,704] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,705] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,705] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,705] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,705] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,705] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,705] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,705] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,706] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,706] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,706] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,706] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,706] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,706] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,706] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,706] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,706] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,706] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,706] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,706] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,707] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,707] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,707] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:53,708] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:56:54,035] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,036] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,036] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,036] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,036] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,036] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,036] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,037] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,037] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,037] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,037] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,037] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,037] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,037] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,038] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,038] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,038] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,038] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,038] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,038] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,038] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,039] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,039] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,039] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,039] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,039] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,039] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,039] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,040] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,040] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,040] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,040] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,040] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,040] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,040] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,040] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,041] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,041] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,041] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,041] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,041] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,041] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,041] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,042] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,041] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,042] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,041] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,042] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,042] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,042] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,042] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,042] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,042] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,042] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,042] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,042] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,042] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,042] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,043] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,043] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,043] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,044] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,046] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:56:54,048] [INFO] [profiler.py:226:end_profile] Flops profiler finished
global rank 12: ddp rank 0 iter_start,iter_end = 0 8
global rank 12: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 26: ddp rank 0 iter_start,iter_end = 0 8
global rank 26: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 17: ddp rank 0 iter_start,iter_end = 0 8
global rank 17: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 13: ddp rank 0 iter_start,iter_end = 0 8
global rank 13: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 6: ddp rank 0 iter_start,iter_end = 0 8
global rank 6: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 8: ddp rank 0 iter_start,iter_end = 0 8
global rank 8: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 50: ddp rank 0 iter_start,iter_end = 0 8
global rank 50: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 35: ddp rank 0 iter_start,iter_end = 0 8
global rank 35: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 22: ddp rank 0 iter_start,iter_end = 0 8
global rank 22: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 16: ddp rank 0 iter_start,iter_end = 0 8
global rank 16: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 31: ddp rank 0 iter_start,iter_end = 0 8
global rank 31: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 20: ddp rank 0 iter_start,iter_end = 0 8
global rank 20: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 30: ddp rank 0 iter_start,iter_end = 0 8
global rank 30: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 21: ddp rank 0 iter_start,iter_end = 0 8
global rank 21: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 5: ddp rank 0 iter_start,iter_end = 0 8
global rank 5: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 23: ddp rank 0 iter_start,iter_end = 0 8
global rank 23: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 25: ddp rank 0 iter_start,iter_end = 0 8
global rank 25: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 28: ddp rank 0 iter_start,iter_end = 0 8
global rank 28: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 62: ddp rank 0 iter_start,iter_end = 0 8
global rank 62: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 33: ddp rank 0 iter_start,iter_end = 0 8
global rank 33: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 38: ddp rank 0 iter_start,iter_end = 0 8
global rank 38: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 32: ddp rank 0 iter_start,iter_end = 0 8
global rank 32: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 36: ddp rank 0 iter_start,iter_end = 0 8
global rank 36: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 61: ddp rank 0 iter_start,iter_end = 0 8
global rank 61: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 24: ddp rank 0 iter_start,iter_end = 0 8
global rank 24: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 52: ddp rank 0 iter_start,iter_end = 0 8
global rank 52: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 0: ddp rank 0 iter_start,iter_end = 0 8
global rank 0: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 54: ddp rank 0 iter_start,iter_end = 0 8
global rank 54: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 51: ddp rank 0 iter_start,iter_end = 0 8
global rank 51: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 10: ddp rank 0 iter_start,iter_end = 0 8
global rank 10: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 9: ddp rank 0 iter_start,iter_end = 0 8
global rank 9: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 15: ddp rank 0 iter_start,iter_end = 0 8
global rank 15: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 19: ddp rank 0 iter_start,iter_end = 0 8
global rank 19: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 2: ddp rank 0 iter_start,iter_end = 0 8
global rank 2: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 7: ddp rank 0 iter_start,iter_end = 0 8
global rank 7: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 58: ddp rank 0 iter_start,iter_end = 0 8
global rank 58: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 49: ddp rank 0 iter_start,iter_end = 0 8
global rank 49: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 60: ddp rank 0 iter_start,iter_end = 0 8
global rank 60: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 4: ddp rank 0 iter_start,iter_end = 0 8
global rank 4: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 42: ddp rank 0 iter_start,iter_end = 0 8
global rank 42: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 63: ddp rank 0 iter_start,iter_end = 0 8
global rank 63: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 37: ddp rank 0 iter_start,iter_end = 0 8
global rank 37: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 39: ddp rank 0 iter_start,iter_end = 0 8
global rank 39: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 29: ddp rank 0 iter_start,iter_end = 0 8
global rank 29: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 46: ddp rank 0 iter_start,iter_end = 0 8
global rank 46: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 40: ddp rank 0 iter_start,iter_end = 0 8
global rank 40: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 34: ddp rank 0 iter_start,iter_end = 0 8
global rank 34: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 48: ddp rank 0 iter_start,iter_end = 0 8
global rank 48: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 1: ddp rank 0 iter_start,iter_end = 0 8
global rank 1: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 14: ddp rank 0 iter_start,iter_end = 0 8
global rank 14: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 57: ddp rank 0 iter_start,iter_end = 0 8
global rank 57: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 3: ddp rank 0 iter_start,iter_end = 0 8
global rank 3: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 18: ddp rank 0 iter_start,iter_end = 0 8
global rank 18: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 11: ddp rank 0 iter_start,iter_end = 0 8
global rank 11: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 55: ddp rank 0 iter_start,iter_end = 0 8
global rank 55: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 53: ddp rank 0 iter_start,iter_end = 0 8
global rank 53: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 45: ddp rank 0 iter_start,iter_end = 0 8
global rank 45: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 47: ddp rank 0 iter_start,iter_end = 0 8
global rank 47: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 59: ddp rank 0 iter_start,iter_end = 0 8
global rank 59: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 56: ddp rank 0 iter_start,iter_end = 0 8
global rank 56: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 43: ddp rank 0 iter_start,iter_end = 0 8
global rank 43: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 44: ddp rank 0 iter_start,iter_end = 0 8
global rank 44: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 27: ddp rank 0 iter_start,iter_end = 0 8
global rank 27: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 41: ddp rank 0 iter_start,iter_end = 0 8
global rank 41: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
I am here
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
MUST: Model 226.553861 M TFLOPS: 10.834589356734137

--> cuda max reserved memory = 21.3047
--> max reserved percentage = 33.3 %

--> cuda max memory allocated = 16.6465
--> max allocated percentage = 26.02 %

--> peak active memory = 20.709
--> peak active memory 32.37 %

cudaMalloc retries = 0
cuda OOM = 0

HERE1 Namespace(arch='orbit_linear_token_agg', batch_size=2, channels=256, depth=32, embed_dim=8192, fa2=True, fsdp_size=1, imagex=128, imagey=256, max_epochs=1, num_heads=64, real=False, seq_par_size=1, simple_ddp_size=1, tensor_par_size=64, yaml_config='configs/ERA5-100million-91variables.yaml') 21.3047 10.834589356734137 tensor(491311109)
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,841] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,841] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,841] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,841] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,841] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,841] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,841] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,841] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,842] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,842] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,842] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,842] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,842] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,842] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,842] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,842] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,842] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,842] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,842] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,842] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,842] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,842] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,842] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,842] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,844] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,844] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,844] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,844] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,844] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,844] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,844] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,844] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,846] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,846] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,846] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,846] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,846] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,846] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,846] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,846] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,846] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,846] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,846] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,846] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,846] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,846] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,846] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,846] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,847] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,847] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,847] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,847] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,847] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,847] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,847] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,847] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,847] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,847] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,847] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,847] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,847] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,847] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,847] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-04-09 13:57:07,847] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
HERE0 Namespace(arch='orbit_linear', batch_size=2, channels=256, depth=32, embed_dim=8192, fa2=True, fsdp_size=1, imagex=128, imagey=256, max_epochs=1, num_heads=64, real=False, seq_par_size=1, simple_ddp_size=1, tensor_par_size=64, yaml_config='configs/ERA5-100million-91variables.yaml')
[W socket.cpp:464] [c10d] The server socket cannot be initialized on [::]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 39 Before initialize parallelism groups
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
rank 29 Before initialize parallelism groups
rank 31 Before initialize parallelism groups
rank 26 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
rank 50 Before initialize parallelism groups
rank 55 Before initialize parallelism groups
rank 54 Before initialize parallelism groups
rank 11 Before initialize parallelism groups
rank 28 Before initialize parallelism groups
rank 35 Before initialize parallelism groups
rank 59 Before initialize parallelism groups
rank 62 Before initialize parallelism groups
rank 60 Before initialize parallelism groups
rank 63 Before initialize parallelism groups
rank 19 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 34 Before initialize parallelism groups
rank 45 Before initialize parallelism groups
Using dist.init_process_group. world_size  64
rank 47 Before initialize parallelism groups
Using dist.init_process_group. world_size  64
rank 53 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 39 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 49 Before initialize parallelism groups
Using dist.init_process_group. world_size  64
rank 16 Before initialize parallelism groups
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 29 After initialize parallelism groups
rank 26 After initialize parallelism groups
rank 31 After initialize parallelism groups
Using dist.init_process_group. world_size  64
rank 43 Before initialize parallelism groups
rank 56 Before initialize parallelism groups
rank 55 After initialize parallelism groups
rank 50 After initialize parallelism groups
rank 54 After initialize parallelism groups
rank 11 After initialize parallelism groups
rank 28 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
rank 35 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 59 After initialize parallelism groups
rank 9 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 6 Before initialize parallelism groups
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
rank 62 After initialize parallelism groups
rank 60 After initialize parallelism groups
rank 63 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
rank 19 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
rank 53 After initialize parallelism groups
rank 34 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
rank 47 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 45 After initialize parallelism groups
Using dist.init_process_group. world_size  64
rank 49 After initialize parallelism groups
rank 16 After initialize parallelism groups
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
Using dist.init_process_group. world_size  64
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 38 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 43 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 56 After initialize parallelism groups
rank 15 Before initialize parallelism groups
rank 2 Before initialize parallelism groups
rank 17 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 24 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 9 After initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 6 After initialize parallelism groups
rank 48 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 18 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 23 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 37 Before initialize parallelism groups
rank 10 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 3 Before initialize parallelism groups
rank 30 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 52 Before initialize parallelism groups
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:697] [c10d] The client socket cannot be initialized to connect to [frontier00123.frontier.olcf.ornl.gov]:29500 (errno: 97 - Address family not supported by protocol).
rank 1 Before initialize parallelism groups
rank 7 Before initialize parallelism groups
rank 38 After initialize parallelism groups
rank 22 Before initialize parallelism groups
rank 15 After initialize parallelism groups
rank 57 Before initialize parallelism groups
rank 2 After initialize parallelism groups
rank 41 Before initialize parallelism groups
rank 17 After initialize parallelism groups
rank 8 Before initialize parallelism groups
rank 24 After initialize parallelism groups
rank 48 After initialize parallelism groups
rank 18 After initialize parallelism groups
rank 37 After initialize parallelism groups
rank 23 After initialize parallelism groups
rank 10 After initialize parallelism groups
rank 3 After initialize parallelism groups
rank 30 After initialize parallelism groups
rank 52 After initialize parallelism groups
rank 1 After initialize parallelism groups
rank 7 After initialize parallelism groups
rank 22 After initialize parallelism groups
rank 57 After initialize parallelism groups
rank 8 After initialize parallelism groups
rank 41 After initialize parallelism groups
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
Using dist.init_process_group. world_size  64
config_path  configs/ERA5-100million-91variables.yaml
rank 21 Before initialize parallelism groups
rank 25 Before initialize parallelism groups
rank 20 Before initialize parallelism groups
rank 4 Before initialize parallelism groups
rank 27 Before initialize parallelism groups
rank 44 Before initialize parallelism groups
rank 5 Before initialize parallelism groups
rank 12 Before initialize parallelism groups
rank 32 Before initialize parallelism groups
rank 40 Before initialize parallelism groups
rank 42 Before initialize parallelism groups
rank 46 Before initialize parallelism groups
rank 51 Before initialize parallelism groups
rank 33 Before initialize parallelism groups
rank 36 Before initialize parallelism groups
rank 14 Before initialize parallelism groups
rank 61 Before initialize parallelism groups
rank 13 Before initialize parallelism groups
rank 58 Before initialize parallelism groups
{'seed_everything': 42, 'trainer': {'checkpoint_path': '/lustre/orion/stf218/world-shared/atsaris/tmp/tmp_checkpoints', 'checkpoint_filename': 'multi_last', 'resume_from_checkpoint': False}, 'parallelism': {'cpu_offloading': False}, 'model': {'lr': 2e-05, 'beta_1': 0.9, 'beta_2': 0.95, 'weight_decay': '1e-5', 'warmup_steps': 1000, 'max_steps': 20000, 'warmup_start_lr': '1e-8', 'eta_min': '1e-8', 'net': {'class_path': 'climax.arch.ClimaX', 'init_args': {'default_vars': ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_70', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000'], 'patch_size': 4, 'decoder_depth': 2, 'mlp_ratio': 4, 'drop_path': 0.1, 'drop_rate': 0.1, 'aggregated_variables': 1}}}, 'data': {'dict_root_dirs': {'mpi-esm': '/lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0'}, 'dict_start_idx': {'mpi-esm': 0}, 'dict_end_idx': {'mpi-esm': 1}, 'dict_in_variables': {'mpi-esm': ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000']}, 'dict_out_variables': {'mpi-esm': ['geopotential_500', 'temperature_850', '2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind']}, 'dict_max_predict_ranges': {'mpi-esm': 720}, 'dict_random_lead_time': {'mpi-esm': True}, 'dict_hrs_each_step': {'mpi-esm': 1}, 'dict_buffer_sizes': {'mpi-esm': 1}, 'num_workers': 1, 'pin_memory': False}}
max_epochs 1 data_par_size 1 fsdp_size 1 simple_ddp_size 1 tensor_par_size 64 seq_par_size 1 cpu_offloading False
lr  2e-05 beta_1  0.9 beta_2 0.95 weight_decay 1e-05 class_path climax.arch.ClimaX default_vars ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_70', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_corank 21 After initialize parallelism groups
mponent_of_wind_925', 'v_component_of_wind_1000']
img_size_x 128 img_size_y 256 patch_size 4 emb_dim 8192 depth 32 decoder_depth 2 num_heads 64 mlp_ratio 4 drop_path 0.1 drop_rate 0.1 aggregated_variables 1
dict_root_dirs {'mpi-esm': '/lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0'} dict_start_idx {'mpi-esm': 0} dict_end_idx {'mpi-esm': 1} batch_size 2 num_workers 1
warmup_steps 1000 max_steps 20000 warmup_start_lr 1e-08 eta_min 1e-08
checkpoint_path /lustre/orion/stf218/world-shared/atsaris/tmp/tmp_checkpoints checkpoint_filename multi_last resume_from_checkpoint False
rank 0 Before initialize parallelism groups
rank 20 After initialize parallelism groups
rank 4 After initialize parallelism groups
rank 25 After initialize parallelism groups
rank 27 After initialize parallelism groups
rank 5 After initialize parallelism groups
rank 12 After initialize parallelism groups
rank 44 After initialize parallelism groups
rank 33 After initialize parallelism groups
rank 40 After initialize parallelism groups
rank 42 After initialize parallelism groups
rank 46 After initialize parallelism groups
rank 51 After initialize parallelism groups
rank 32 After initialize parallelism groups
rank 61 After initialize parallelism groups
rank 36 After initialize parallelism groups
rank 13 After initialize parallelism groups
rank 14 After initialize parallelism groups
rank 58 After initialize parallelism groups
rank 0 After initialize parallelism groups
rank 41 After initialize model
rank 41 Before the second dist.barrier()
rank 56 After initialize model
rank 56 Before the second dist.barrier()
rank 40 After initialize model
rank 40 Before the second dist.barrier()
rank 48 After initialize model
rank 48 Before the second dist.barrier()
rank 25 After initialize model
rank 25 Before the second dist.barrier()
rank 49 After initialize model
rank 49 Before the second dist.barrier()
rank 24 After initialize model
rank 24 Before the second dist.barrier()
rank 1 After initialize model
rank 1 Before the second dist.barrier()
rank 57 After initialize model
rank 57 Before the second dist.barrier()
rank 0 After initialize model
rank 0 Before the second dist.barrier()
rank 9 After initialize model
rank 9 Before the second dist.barrier()
rank 43 After initialize model
rank 43 Before the second dist.barrier()
rank 17 After initialize model
rank 17 Before the second dist.barrier()
rank 8 After initialize model
rank 8 Before the second dist.barrier()
rank 16 After initialize model
rank 16 Before the second dist.barrier()
rank 33 After initialize model
rank 33 Before the second dist.barrier()
rank 32 After initialize model
rank 32 Before the second dist.barrier()
rank 42 After initialize model
rank 42 Before the second dist.barrier()
rank 51 After initialize model
rank 51 Before the second dist.barrier()
rank 50 After initialize model
rank 50 Before the second dist.barrier()
rank 58 After initialize model
rank 58 Before the second dist.barrier()
rank 62 After initialize model
rank 62 Before the second dist.barrier()
rank 27 After initialize model
rank 59 After initialize model
rank 59 Before the second dist.barrier()
rank 27 Before the second dist.barrier()
rank 45 After initialize model
rank 45 Before the second dist.barrier()
rank 52 After initialize model
rank 52 Before the second dist.barrier()
rank 46 After initialize model
rank 46 Before the second dist.barrier()
rank 53 After initialize model
rank 53 Before the second dist.barrier()
rank 26 After initialize model
rank 26 Before the second dist.barrier()
rank 47 After initialize model
rank 47 Before the second dist.barrier()
rank 63 After initialize model
rank 63 Before the second dist.barrier()
rank 55 After initialize model
rank 55 Before the second dist.barrier()
rank 34 After initialize model
rank 34 Before the second dist.barrier()
rank 54 After initialize model
rank 54 Before the second dist.barrier()
rank 2 After initialize model
rank 2 Before the second dist.barrier()
rank 44 After initialize model
rank 44 Before the second dist.barrier()
rank 35 After initialize model
rank 35 Before the second dist.barrier()
rank 61 After initialize model
rank 61 Before the second dist.barrier()
rank 60 After initialize model
rank 60 Before the second dist.barrier()
rank 30 After initialize model
rank 30 Before the second dist.barrier()
rank 29 After initialize model
rank 29 Before the second dist.barrier()
rank 18 After initialize model
rank 18 Before the second dist.barrier()
rank 3 After initialize model
rank 3 Before the second dist.barrier()
rank 31 After initialize model
rank 31 Before the second dist.barrier()
rank 28 After initialize model
rank 28 Before the second dist.barrier()
rank 10 After initialize model
rank 10 Before the second dist.barrier()
rank 39 After initialize model
rank 39 Before the second dist.barrier()
rank 7 After initialize model
rank 7 Before the second dist.barrier()
rank 6 After initialize model
rank 6 Before the second dist.barrier()
rank 37 After initialize model
rank 37 Before the second dist.barrier()
rank 36 After initialize model
rank 36 Before the second dist.barrier()
rank 38 After initialize model
rank 38 Before the second dist.barrier()
rank 23 After initialize model
rank 23 Before the second dist.barrier()
rank 14 After initialize model
rank 14 Before the second dist.barrier()
rank 11 After initialize model
rank 11 Before the second dist.barrier()
rank 5 After initialize model
rank 5 Before the second dist.barrier()
rank 22 After initialize model
rank 22 Before the second dist.barrier()
rank 4 After initialize model
rank 4 Before the second dist.barrier()
rank 15 After initialize model
rank 15 Before the second dist.barrier()
rank 20 After initialize model
rank 20 Before the second dist.barrier()
rank 13 After initialize model
rank 13 Before the second dist.barrier()
rank 19 After initialize model
rank 19 Before the second dist.barrier()
rank 21 After initialize model
rank 21 Before the second dist.barrier()
rank 12 After initialize model
rank 12 Before the second dist.barrier()
rank 48 After the second dist.barrier()
rank 0 After the second dist.barrier()
rank 56 After the second dist.barrier()
rank 1 After the second dist.barrier()
rank 49 After the second dist.barrier()
rank 57 After the second dist.barrier()
rank 58 After the second dist.barrier()
rank 59 After the second dist.barrier()
rank 60 After the second dist.barrier()
rank 61 After the second dist.barrier()
parameter name  var_embed  requires_gradient  True size torch.Size([1, 256, 8192])
rank 40 After the second dist.barrier()
rank 62 After the second dist.barrier()
rank 4 After the second dist.barrier()
rank 63 After the second dist.barrier()
rank 5 After the second dist.barrier()
rank 16 After the second dist.barrier()
rank 41 After the second dist.barrier()
rank 53 After the second dist.barrier()
rank 24 After the second dist.barrier()
rank 32 After the second dist.barrier()
rank 33 After the second dist.barrier()
parameter name  var_query  requires_gradient  True size torch.Size([1, 1, 8192])
parameter name  var_query_per_rank  requires_gradient  True size torch.Size([1, 1, 8192])
rank 37 After the second dist.barrier()
parameter name  pos_embed  requires_gradient  True size torch.Size([1, 2048, 8192])
rank 36 After the second dist.barrier()
rank 6 After the second dist.barrier()
rank 45 After the second dist.barrier()
rank 52 After the second dist.barrier()
parameter name  token_embeds.0.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
rank 17 After the second dist.barrier()
rank 44 After the second dist.barrier()
rank 25 After the second dist.barrier()
rank 8 After the second dist.barrier()
parameter name  token_embeds.0.proj.bias  requires_gradient  True size torch.Size([8192])
rank 9 After the second dist.barrier()
parameter name  token_embeds.1.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.1.proj.bias  requires_gradient  True size torch.Size([8192])
rank 54 After the second dist.barrier()
rank 55 After the second dist.barrier()
rank 29 After the second dist.barrier()
parameter name  token_embeds.2.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
rank 21 After the second dist.barrier()
rank 7 After the second dist.barrier()
parameter name  token_embeds.2.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.3.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.3.proj.bias  requires_gradient  True size torch.Size([8192])
rank 20 After the second dist.barrier()
rank 46 After the second dist.barrier()
rank 38 After the second dist.barrier()
parameter name  token_embeds.4.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
rank 28 After the second dist.barrier()
rank 39 After the second dist.barrier()
rank 12 After the second dist.barrier()
rank 3 After the second dist.barrier()
rank 13 After the second dist.barrier()
parameter name  token_embeds.4.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.5.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.5.proj.bias  requires_gradient  True size torch.Size([8192])
rank 51 After the second dist.barrier()
rank 30 After the second dist.barrier()
parameter name  token_embeds.6.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
rank 22 After the second dist.barrier()
parameter name  token_embeds.6.proj.bias  requires_gradient  True size torch.Size([8192])
rank 23 After the second dist.barrier()
rank 47 After the second dist.barrier()
parameter name  token_embeds.7.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
rank 2 After the second dist.barrier()
parameter name  token_embeds.7.proj.bias  requires_gradient  True size torch.Size([8192])
rank 50 After the second dist.barrier()
parameter name  token_embeds.8.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
rank 43 After the second dist.barrier()
rank 31 After the second dist.barrier()
rank 35 After the second dist.barrier()
rank 14 After the second dist.barrier()
parameter name  token_embeds.8.proj.bias  requires_gradient  True size torch.Size([8192])
rank 15 After the second dist.barrier()
parameter name  token_embeds.9.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.9.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.10.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
rank 34 After the second dist.barrier()
parameter name  token_embeds.10.proj.bias  requires_gradient  True size torch.Size([8192])
rank 19 After the second dist.barrier()
rank 27 After the second dist.barrier()
parameter name  token_embeds.11.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
rank 42 After the second dist.barrier()
parameter name  token_embeds.11.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.12.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.12.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.13.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
rank 18 After the second dist.barrier()
parameter name  token_embeds.13.proj.bias  requires_gradient  True size torch.Size([8192])
rank 26 After the second dist.barrier()
parameter name  token_embeds.14.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
rank 10 After the second dist.barrier()
parameter name  token_embeds.14.proj.bias  requires_gradient  True size torch.Size([8192])
rank 11 After the second dist.barrier()
parameter name  token_embeds.15.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.15.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.16.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.16.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.17.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.17.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.18.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.18.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.19.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.19.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.20.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.20.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.21.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.21.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.22.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.22.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.23.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.23.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.24.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.24.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.25.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.25.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.26.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.26.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.27.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.27.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.28.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.28.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.29.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.29.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.30.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.30.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.31.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.31.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.32.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.32.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.33.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.33.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.34.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.34.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.35.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.35.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.36.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.36.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.37.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.37.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.38.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.38.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.39.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.39.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.40.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.40.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.41.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.41.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.42.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.42.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.43.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.43.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.44.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.44.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.45.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.45.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.46.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.46.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.47.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.47.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.48.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.48.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.49.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.49.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.50.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.50.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.51.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.51.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.52.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.52.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.53.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.53.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.54.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.54.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.55.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.55.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.56.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.56.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.57.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.57.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.58.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.58.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.59.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.59.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.60.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.60.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.61.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.61.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.62.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.62.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.63.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.63.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.64.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.64.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.65.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.65.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.66.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.66.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.67.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.67.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.68.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.68.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.69.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.69.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.70.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.70.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.71.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.71.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.72.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.72.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.73.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.73.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.74.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.74.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.75.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.75.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.76.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.76.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.77.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.77.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.78.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.78.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.79.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.79.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.80.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.80.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.81.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.81.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.82.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.82.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.83.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.83.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.84.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.84.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.85.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.85.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.86.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.86.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.87.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.87.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.88.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.88.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.89.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.89.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.90.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.90.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.91.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.91.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.92.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.92.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.93.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.93.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.94.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.94.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.95.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.95.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.96.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.96.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.97.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.97.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.98.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.98.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.99.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.99.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.100.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.100.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.101.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.101.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.102.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.102.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.103.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.103.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.104.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.104.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.105.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.105.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.106.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.106.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.107.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.107.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.108.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.108.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.109.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.109.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.110.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.110.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.111.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.111.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.112.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.112.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.113.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.113.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.114.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.114.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.115.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.115.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.116.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.116.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.117.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.117.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.118.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.118.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.119.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.119.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.120.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.120.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.121.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.121.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.122.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.122.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.123.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.123.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.124.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.124.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.125.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.125.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.126.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.126.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.127.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.127.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.128.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.128.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.129.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.129.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.130.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.130.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.131.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.131.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.132.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.132.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.133.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.133.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.134.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.134.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.135.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.135.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.136.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.136.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.137.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.137.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.138.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.138.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.139.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.139.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.140.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.140.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.141.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.141.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.142.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.142.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.143.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.143.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.144.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.144.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.145.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.145.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.146.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.146.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.147.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.147.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.148.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.148.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.149.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.149.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.150.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.150.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.151.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.151.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.152.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.152.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.153.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.153.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.154.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.154.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.155.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.155.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.156.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.156.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.157.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.157.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.158.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.158.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.159.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.159.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.160.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.160.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.161.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.161.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.162.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.162.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.163.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.163.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.164.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.164.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.165.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.165.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.166.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.166.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.167.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.167.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.168.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.168.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.169.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.169.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.170.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.170.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.171.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.171.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.172.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.172.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.173.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.173.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.174.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.174.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.175.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.175.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.176.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.176.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.177.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.177.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.178.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.178.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.179.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.179.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.180.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.180.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.181.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.181.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.182.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.182.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.183.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.183.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.184.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.184.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.185.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.185.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.186.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.186.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.187.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.187.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.188.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.188.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.189.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.189.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.190.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.190.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.191.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.191.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.192.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.192.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.193.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.193.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.194.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.194.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.195.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.195.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.196.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.196.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.197.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.197.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.198.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.198.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.199.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.199.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.200.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.200.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.201.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.201.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.202.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.202.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.203.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.203.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.204.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.204.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.205.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.205.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.206.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.206.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.207.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.207.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.208.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.208.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.209.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.209.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.210.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.210.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.211.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.211.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.212.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.212.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.213.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.213.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.214.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.214.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.215.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.215.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.216.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.216.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.217.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.217.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.218.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.218.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.219.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.219.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.220.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.220.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.221.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.221.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.222.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.222.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.223.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.223.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.224.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.224.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.225.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.225.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.226.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.226.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.227.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.227.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.228.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.228.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.229.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.229.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.230.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.230.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.231.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.231.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.232.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.232.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.233.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.233.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.234.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.234.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.235.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.235.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.236.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.236.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.237.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.237.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.238.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.238.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.239.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.239.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.240.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.240.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.241.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.241.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.242.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.242.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.243.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.243.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.244.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.244.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.245.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.245.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.246.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.246.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.247.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.247.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.248.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.248.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.249.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.249.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.250.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.250.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.251.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.251.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.252.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.252.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.253.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.253.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.254.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.254.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.255.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.255.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  var_agg.q.weight  requires_gradient  True size torch.Size([128, 8192])
parameter name  var_agg.kv.weight  requires_gradient  True size torch.Size([256, 8192])
parameter name  var_agg.proj.weight  requires_gradient  True size torch.Size([8192, 128])
parameter name  var_agg.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  var_linear_per_rank.weight  requires_gradient  True size torch.Size([1, 4])
parameter name  var_linear_per_rank.bias  requires_gradient  True size torch.Size([1])
parameter name  lead_time_embed.weight  requires_gradient  True size torch.Size([8192, 1])
parameter name  lead_time_embed.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.attn.qkv.weight  requires_gradient  True size torch.Size([384, 8192])
parameter name  blocks.0.attn.qkv.bias  requires_gradient  True size torch.Size([384])
parameter name  blocks.0.attn.proj.weight  requires_gradient  True size torch.Size([8192, 128])
parameter name  blocks.0.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.mlp.fc1.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  blocks.0.mlp.fc1.bias  requires_gradient  True size torch.Size([512])
parameter name  blocks.0.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 512])
parameter name  blocks.0.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.attn.qkv.weight  requires_gradient  True size torch.Size([384, 8192])
parameter name  blocks.1.attn.qkv.bias  requires_gradient  True size torch.Size([384])
parameter name  blocks.1.attn.proj.weight  requires_gradient  True size torch.Size([8192, 128])
parameter name  blocks.1.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.mlp.fc1.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  blocks.1.mlp.fc1.bias  requires_gradient  True size torch.Size([512])
parameter name  blocks.1.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 512])
parameter name  blocks.1.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.attn.qkv.weight  requires_gradient  True size torch.Size([384, 8192])
parameter name  blocks.2.attn.qkv.bias  requires_gradient  True size torch.Size([384])
parameter name  blocks.2.attn.proj.weight  requires_gradient  True size torch.Size([8192, 128])
parameter name  blocks.2.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.mlp.fc1.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  blocks.2.mlp.fc1.bias  requires_gradient  True size torch.Size([512])
parameter name  blocks.2.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 512])
parameter name  blocks.2.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.attn.qkv.weight  requires_gradient  True size torch.Size([384, 8192])
parameter name  blocks.3.attn.qkv.bias  requires_gradient  True size torch.Size([384])
parameter name  blocks.3.attn.proj.weight  requires_gradient  True size torch.Size([8192, 128])
parameter name  blocks.3.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.mlp.fc1.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  blocks.3.mlp.fc1.bias  requires_gradient  True size torch.Size([512])
parameter name  blocks.3.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 512])
parameter name  blocks.3.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.attn.qkv.weight  requires_gradient  True size torch.Size([384, 8192])
parameter name  blocks.4.attn.qkv.bias  requires_gradient  True size torch.Size([384])
parameter name  blocks.4.attn.proj.weight  requires_gradient  True size torch.Size([8192, 128])
parameter name  blocks.4.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.mlp.fc1.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  blocks.4.mlp.fc1.bias  requires_gradient  True size torch.Size([512])
parameter name  blocks.4.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 512])
parameter name  blocks.4.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.attn.qkv.weight  requires_gradient  True size torch.Size([384, 8192])
parameter name  blocks.5.attn.qkv.bias  requires_gradient  True size torch.Size([384])
parameter name  blocks.5.attn.proj.weight  requires_gradient  True size torch.Size([8192, 128])
parameter name  blocks.5.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.mlp.fc1.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  blocks.5.mlp.fc1.bias  requires_gradient  True size torch.Size([512])
parameter name  blocks.5.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 512])
parameter name  blocks.5.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.attn.qkv.weight  requires_gradient  True size torch.Size([384, 8192])
parameter name  blocks.6.attn.qkv.bias  requires_gradient  True size torch.Size([384])
parameter name  blocks.6.attn.proj.weight  requires_gradient  True size torch.Size([8192, 128])
parameter name  blocks.6.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.mlp.fc1.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  blocks.6.mlp.fc1.bias  requires_gradient  True size torch.Size([512])
parameter name  blocks.6.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 512])
parameter name  blocks.6.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.attn.qkv.weight  requires_gradient  True size torch.Size([384, 8192])
parameter name  blocks.7.attn.qkv.bias  requires_gradient  True size torch.Size([384])
parameter name  blocks.7.attn.proj.weight  requires_gradient  True size torch.Size([8192, 128])
parameter name  blocks.7.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.mlp.fc1.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  blocks.7.mlp.fc1.bias  requires_gradient  True size torch.Size([512])
parameter name  blocks.7.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 512])
parameter name  blocks.7.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.attn.qkv.weight  requires_gradient  True size torch.Size([384, 8192])
parameter name  blocks.8.attn.qkv.bias  requires_gradient  True size torch.Size([384])
parameter name  blocks.8.attn.proj.weight  requires_gradient  True size torch.Size([8192, 128])
parameter name  blocks.8.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.mlp.fc1.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  blocks.8.mlp.fc1.bias  requires_gradient  True size torch.Size([512])
parameter name  blocks.8.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 512])
parameter name  blocks.8.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.attn.qkv.weight  requires_gradient  True size torch.Size([384, 8192])
parameter name  blocks.9.attn.qkv.bias  requires_gradient  True size torch.Size([384])
parameter name  blocks.9.attn.proj.weight  requires_gradient  True size torch.Size([8192, 128])
parameter name  blocks.9.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.mlp.fc1.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  blocks.9.mlp.fc1.bias  requires_gradient  True size torch.Size([512])
parameter name  blocks.9.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 512])
parameter name  blocks.9.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.attn.qkv.weight  requires_gradient  True size torch.Size([384, 8192])
parameter name  blocks.10.attn.qkv.bias  requires_gradient  True size torch.Size([384])
parameter name  blocks.10.attn.proj.weight  requires_gradient  True size torch.Size([8192, 128])
parameter name  blocks.10.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.mlp.fc1.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  blocks.10.mlp.fc1.bias  requires_gradient  True size torch.Size([512])
parameter name  blocks.10.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 512])
parameter name  blocks.10.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.attn.qkv.weight  requires_gradient  True size torch.Size([384, 8192])
parameter name  blocks.11.attn.qkv.bias  requires_gradient  True size torch.Size([384])
parameter name  blocks.11.attn.proj.weight  requires_gradient  True size torch.Size([8192, 128])
parameter name  blocks.11.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.mlp.fc1.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  blocks.11.mlp.fc1.bias  requires_gradient  True size torch.Size([512])
parameter name  blocks.11.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 512])
parameter name  blocks.11.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.attn.qkv.weight  requires_gradient  True size torch.Size([384, 8192])
parameter name  blocks.12.attn.qkv.bias  requires_gradient  True size torch.Size([384])
parameter name  blocks.12.attn.proj.weight  requires_gradient  True size torch.Size([8192, 128])
parameter name  blocks.12.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.mlp.fc1.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  blocks.12.mlp.fc1.bias  requires_gradient  True size torch.Size([512])
parameter name  blocks.12.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 512])
parameter name  blocks.12.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.attn.qkv.weight  requires_gradient  True size torch.Size([384, 8192])
parameter name  blocks.13.attn.qkv.bias  requires_gradient  True size torch.Size([384])
parameter name  blocks.13.attn.proj.weight  requires_gradient  True size torch.Size([8192, 128])
parameter name  blocks.13.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.mlp.fc1.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  blocks.13.mlp.fc1.bias  requires_gradient  True size torch.Size([512])
parameter name  blocks.13.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 512])
parameter name  blocks.13.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.attn.qkv.weight  requires_gradient  True size torch.Size([384, 8192])
parameter name  blocks.14.attn.qkv.bias  requires_gradient  True size torch.Size([384])
parameter name  blocks.14.attn.proj.weight  requires_gradient  True size torch.Size([8192, 128])
parameter name  blocks.14.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.mlp.fc1.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  blocks.14.mlp.fc1.bias  requires_gradient  True size torch.Size([512])
parameter name  blocks.14.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 512])
parameter name  blocks.14.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.attn.qkv.weight  requires_gradient  True size torch.Size([384, 8192])
parameter name  blocks.15.attn.qkv.bias  requires_gradient  True size torch.Size([384])
parameter name  blocks.15.attn.proj.weight  requires_gradient  True size torch.Size([8192, 128])
parameter name  blocks.15.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.mlp.fc1.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  blocks.15.mlp.fc1.bias  requires_gradient  True size torch.Size([512])
parameter name  blocks.15.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 512])
parameter name  blocks.15.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.attn.qkv.weight  requires_gradient  True size torch.Size([384, 8192])
parameter name  blocks.16.attn.qkv.bias  requires_gradient  True size torch.Size([384])
parameter name  blocks.16.attn.proj.weight  requires_gradient  True size torch.Size([8192, 128])
parameter name  blocks.16.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.mlp.fc1.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  blocks.16.mlp.fc1.bias  requires_gradient  True size torch.Size([512])
parameter name  blocks.16.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 512])
parameter name  blocks.16.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.attn.qkv.weight  requires_gradient  True size torch.Size([384, 8192])
parameter name  blocks.17.attn.qkv.bias  requires_gradient  True size torch.Size([384])
parameter name  blocks.17.attn.proj.weight  requires_gradient  True size torch.Size([8192, 128])
parameter name  blocks.17.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.mlp.fc1.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  blocks.17.mlp.fc1.bias  requires_gradient  True size torch.Size([512])
parameter name  blocks.17.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 512])
parameter name  blocks.17.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.attn.qkv.weight  requires_gradient  True size torch.Size([384, 8192])
parameter name  blocks.18.attn.qkv.bias  requires_gradient  True size torch.Size([384])
parameter name  blocks.18.attn.proj.weight  requires_gradient  True size torch.Size([8192, 128])
parameter name  blocks.18.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.mlp.fc1.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  blocks.18.mlp.fc1.bias  requires_gradient  True size torch.Size([512])
parameter name  blocks.18.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 512])
parameter name  blocks.18.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.attn.qkv.weight  requires_gradient  True size torch.Size([384, 8192])
parameter name  blocks.19.attn.qkv.bias  requires_gradient  True size torch.Size([384])
parameter name  blocks.19.attn.proj.weight  requires_gradient  True size torch.Size([8192, 128])
parameter name  blocks.19.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.mlp.fc1.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  blocks.19.mlp.fc1.bias  requires_gradient  True size torch.Size([512])
parameter name  blocks.19.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 512])
parameter name  blocks.19.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.attn.qkv.weight  requires_gradient  True size torch.Size([384, 8192])
parameter name  blocks.20.attn.qkv.bias  requires_gradient  True size torch.Size([384])
parameter name  blocks.20.attn.proj.weight  requires_gradient  True size torch.Size([8192, 128])
parameter name  blocks.20.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.mlp.fc1.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  blocks.20.mlp.fc1.bias  requires_gradient  True size torch.Size([512])
parameter name  blocks.20.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 512])
parameter name  blocks.20.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.attn.qkv.weight  requires_gradient  True size torch.Size([384, 8192])
parameter name  blocks.21.attn.qkv.bias  requires_gradient  True size torch.Size([384])
parameter name  blocks.21.attn.proj.weight  requires_gradient  True size torch.Size([8192, 128])
parameter name  blocks.21.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.mlp.fc1.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  blocks.21.mlp.fc1.bias  requires_gradient  True size torch.Size([512])
parameter name  blocks.21.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 512])
parameter name  blocks.21.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.attn.qkv.weight  requires_gradient  True size torch.Size([384, 8192])
parameter name  blocks.22.attn.qkv.bias  requires_gradient  True size torch.Size([384])
parameter name  blocks.22.attn.proj.weight  requires_gradient  True size torch.Size([8192, 128])
parameter name  blocks.22.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.mlp.fc1.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  blocks.22.mlp.fc1.bias  requires_gradient  True size torch.Size([512])
parameter name  blocks.22.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 512])
parameter name  blocks.22.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.attn.qkv.weight  requires_gradient  True size torch.Size([384, 8192])
parameter name  blocks.23.attn.qkv.bias  requires_gradient  True size torch.Size([384])
parameter name  blocks.23.attn.proj.weight  requires_gradient  True size torch.Size([8192, 128])
parameter name  blocks.23.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.mlp.fc1.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  blocks.23.mlp.fc1.bias  requires_gradient  True size torch.Size([512])
parameter name  blocks.23.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 512])
parameter name  blocks.23.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.attn.qkv.weight  requires_gradient  True size torch.Size([384, 8192])
parameter name  blocks.24.attn.qkv.bias  requires_gradient  True size torch.Size([384])
parameter name  blocks.24.attn.proj.weight  requires_gradient  True size torch.Size([8192, 128])
parameter name  blocks.24.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.mlp.fc1.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  blocks.24.mlp.fc1.bias  requires_gradient  True size torch.Size([512])
parameter name  blocks.24.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 512])
parameter name  blocks.24.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.attn.qkv.weight  requires_gradient  True size torch.Size([384, 8192])
parameter name  blocks.25.attn.qkv.bias  requires_gradient  True size torch.Size([384])
parameter name  blocks.25.attn.proj.weight  requires_gradient  True size torch.Size([8192, 128])
parameter name  blocks.25.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.mlp.fc1.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  blocks.25.mlp.fc1.bias  requires_gradient  True size torch.Size([512])
parameter name  blocks.25.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 512])
parameter name  blocks.25.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.attn.qkv.weight  requires_gradient  True size torch.Size([384, 8192])
parameter name  blocks.26.attn.qkv.bias  requires_gradient  True size torch.Size([384])
parameter name  blocks.26.attn.proj.weight  requires_gradient  True size torch.Size([8192, 128])
parameter name  blocks.26.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.mlp.fc1.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  blocks.26.mlp.fc1.bias  requires_gradient  True size torch.Size([512])
parameter name  blocks.26.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 512])
parameter name  blocks.26.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.attn.qkv.weight  requires_gradient  True size torch.Size([384, 8192])
parameter name  blocks.27.attn.qkv.bias  requires_gradient  True size torch.Size([384])
parameter name  blocks.27.attn.proj.weight  requires_gradient  True size torch.Size([8192, 128])
parameter name  blocks.27.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.mlp.fc1.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  blocks.27.mlp.fc1.bias  requires_gradient  True size torch.Size([512])
parameter name  blocks.27.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 512])
parameter name  blocks.27.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.attn.qkv.weight  requires_gradient  True size torch.Size([384, 8192])
parameter name  blocks.28.attn.qkv.bias  requires_gradient  True size torch.Size([384])
parameter name  blocks.28.attn.proj.weight  requires_gradient  True size torch.Size([8192, 128])
parameter name  blocks.28.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.mlp.fc1.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  blocks.28.mlp.fc1.bias  requires_gradient  True size torch.Size([512])
parameter name  blocks.28.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 512])
parameter name  blocks.28.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.attn.qkv.weight  requires_gradient  True size torch.Size([384, 8192])
parameter name  blocks.29.attn.qkv.bias  requires_gradient  True size torch.Size([384])
parameter name  blocks.29.attn.proj.weight  requires_gradient  True size torch.Size([8192, 128])
parameter name  blocks.29.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.mlp.fc1.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  blocks.29.mlp.fc1.bias  requires_gradient  True size torch.Size([512])
parameter name  blocks.29.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 512])
parameter name  blocks.29.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.attn.qkv.weight  requires_gradient  True size torch.Size([384, 8192])
parameter name  blocks.30.attn.qkv.bias  requires_gradient  True size torch.Size([384])
parameter name  blocks.30.attn.proj.weight  requires_gradient  True size torch.Size([8192, 128])
parameter name  blocks.30.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.mlp.fc1.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  blocks.30.mlp.fc1.bias  requires_gradient  True size torch.Size([512])
parameter name  blocks.30.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 512])
parameter name  blocks.30.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.attn.qkv.weight  requires_gradient  True size torch.Size([384, 8192])
parameter name  blocks.31.attn.qkv.bias  requires_gradient  True size torch.Size([384])
parameter name  blocks.31.attn.proj.weight  requires_gradient  True size torch.Size([8192, 128])
parameter name  blocks.31.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.mlp.fc1.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  blocks.31.mlp.fc1.bias  requires_gradient  True size torch.Size([512])
parameter name  blocks.31.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 512])
parameter name  blocks.31.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  norm.weight  requires_gradient  True size torch.Size([8192])
parameter name  norm.bias  requires_gradient  True size torch.Size([8192])
parameter name  head.0.weight  requires_gradient  True size torch.Size([8192, 8192])
parameter name  head.0.bias  requires_gradient  True size torch.Size([8192])
parameter name  head.2.weight  requires_gradient  True size torch.Size([8192, 8192])
parameter name  head.2.bias  requires_gradient  True size torch.Size([8192])
parameter name  head.5.weight  requires_gradient  True size torch.Size([4096, 8192])
parameter name  head.5.bias  requires_gradient  True size torch.Size([4096])
total_params before FSDP tensor(26297569285) params_per_gpu tensor(630824965)
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
rank  63 lister_train reset: mpi-esm 9 9 9
rank  63 lister_test reset: mpi-esm 9 9 9
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
total_params after FSDP FullyShardedDataParallel(
  (_fsdp_wrapped_module): ClimaX(
    (token_embeds): ModuleList(
      (0-255): 256 x PatchEmbed(
        (proj): Conv2d(1, 8192, kernel_size=(4, 4), stride=(4, 4))
        (norm): Identity()
      )
    )
    (var_agg): FullyShardedDataParallel(
      (_fsdp_wrapped_module): CheckpointWrapper(
        (_checkpoint_wrapped_module): VariableMapping_Attention(
          (q): Linear(in_features=8192, out_features=128, bias=False)
          (kv): Linear(in_features=8192, out_features=256, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=8192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (var_linear_per_rank): Linear(in_features=4, out_features=1, bias=True)
    (lead_time_embed): Linear(in_features=1, out_features=8192, bias=True)
    (pos_drop): Dropout(p=0.1, inplace=False)
    (blocks): ModuleList(
      (0-31): 32 x FullyShardedDataParallel(
        (_fsdp_wrapped_module): CheckpointWrapper(
          (_checkpoint_wrapped_module): Block(
            (norm1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=8192, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=128, out_features=8192, bias=True)
              (proj_drop): Dropout(p=0.1, inplace=False)
            )
            (ls1): Identity()
            (norm2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=8192, out_features=512, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.1, inplace=False)
              (fc2): Linear(in_features=512, out_features=8192, bias=True)
            )
            (ls2): Identity()
          )
        )
      )
    )
    (norm): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
use_ddstore is : 0
    (head): Sequential(
      (0): Linear(in_features=8192, out_features=8192, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=8192, out_features=8192, bias=True)
      (3): GELU(approximate='none')
      (4): Pred_Rearrange()
      (5): Linear(in_features=8192, out_features=4096, bias=True)
    )
  )
)
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
rank  0 lister_train reset: mpi-esm 9 9 9
rank  0 lister_test reset: mpi-esm 9 9 9
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
memory stats reset, ready to track
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
epoch-train:  0 batch_idx 0 world_rank 0  loss  8.0625
epoch-train:  0 batch_idx 1 world_rank 0  loss  nan
epoch-train:  0 batch_idx 2 world_rank 0  loss  nan
[2025-04-09 13:57:50,255] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,256] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,256] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,257] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,257] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,257] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,257] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,257] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,257] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,258] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,258] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,258] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,258] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,258] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,258] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,258] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,258] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,259] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,259] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,259] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,259] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,259] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,259] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,259] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,259] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,259] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,259] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,259] [INFO] [profiler.py:80:start_profile] Flops profiler started
epoch-train:  0 batch_idx 3 world_rank 0  loss  nan
[2025-04-09 13:57:50,259] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,259] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,260] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,260] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,260] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,260] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,260] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,260] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,260] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,260] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,260] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,260] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,261] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,260] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,261] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,261] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,261] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,261] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,262] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,262] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,262] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,263] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,263] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,263] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,263] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,264] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,264] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,264] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,264] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,264] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,264] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,264] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,264] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,265] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,265] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:50,265] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-04-09 13:57:51,456] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,456] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,456] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,457] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,457] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,457] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,457] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,457] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,457] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,457] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,457] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,457] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,457] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,457] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,457] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,457] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,457] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,457] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,457] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,457] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,457] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,458] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,457] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,458] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,457] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,457] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,458] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,458] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,458] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,458] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,458] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,458] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,458] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,458] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,458] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,458] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,458] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,458] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,458] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,458] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,458] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,458] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,458] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,458] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,458] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,458] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,458] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,458] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,458] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,458] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,458] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,459] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,459] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,459] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,459] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,459] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,459] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,459] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,459] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,459] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,459] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,459] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,460] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-04-09 13:57:51,460] [INFO] [profiler.py:226:end_profile] Flops profiler finished
global rank 20: ddp rank 0 iter_start,iter_end = 0 8
global rank 20: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 3: ddp rank 0 iter_start,iter_end = 0 8
global rank 3: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 25: ddp rank 0 iter_start,iter_end = 0 8
global rank 25: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 26: ddp rank 0 iter_start,iter_end = 0 8
global rank 26: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 14: ddp rank 0 iter_start,iter_end = 0 8
global rank 14: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 12: ddp rank 0 iter_start,iter_end = 0 8
global rank 12: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 9: ddp rank 0 iter_start,iter_end = 0 8
global rank 9: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 60: ddp rank 0 iter_start,iter_end = 0 8
global rank 60: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 5: ddp rank 0 iter_start,iter_end = 0 8
global rank 5: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 16: ddp rank 0 iter_start,iter_end = 0 8
global rank 16: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 22: ddp rank 0 iter_start,iter_end = 0 8
global rank 22: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 52: ddp rank 0 iter_start,iter_end = 0 8
global rank 52: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 46: ddp rank 0 iter_start,iter_end = 0 8
global rank 46: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 32: ddp rank 0 iter_start,iter_end = 0 8
global rank 32: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 47: ddp rank 0 iter_start,iter_end = 0 8
global rank 47: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 21: ddp rank 0 iter_start,iter_end = 0 8
global rank 21: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 54: ddp rank 0 iter_start,iter_end = 0 8
global rank 54: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 34: ddp rank 0 iter_start,iter_end = 0 8
global rank 34: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 0: ddp rank 0 iter_start,iter_end = 0 8
global rank 0: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 53: ddp rank 0 iter_start,iter_end = 0 8
global rank 53: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 27: ddp rank 0 iter_start,iter_end = 0 8
global rank 27: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 18: ddp rank 0 iter_start,iter_end = 0 8
global rank 18: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 17: ddp rank 0 iter_start,iter_end = 0 8
global rank 17: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 56: ddp rank 0 iter_start,iter_end = 0 8
global rank 56: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 45: ddp rank 0 iter_start,iter_end = 0 8
global rank 45: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 42: ddp rank 0 iter_start,iter_end = 0 8
global rank 42: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 11: ddp rank 0 iter_start,iter_end = 0 8
global rank 11: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 59: ddp rank 0 iter_start,iter_end = 0 8
global rank 59: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 57: ddp rank 0 iter_start,iter_end = 0 8
global rank 57: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 6: ddp rank 0 iter_start,iter_end = 0 8
global rank 6: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 51: ddp rank 0 iter_start,iter_end = 0 8
global rank 51: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 28: ddp rank 0 iter_start,iter_end = 0 8
global rank 28: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 49: ddp rank 0 iter_start,iter_end = 0 8
global rank 49: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 36: ddp rank 0 iter_start,iter_end = 0 8
global rank 36: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 19: ddp rank 0 iter_start,iter_end = 0 8
global rank 19: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 23: ddp rank 0 iter_start,iter_end = 0 8
global rank 23: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 1: ddp rank 0 iter_start,iter_end = 0 8
global rank 1: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 15: ddp rank 0 iter_start,iter_end = 0 8
global rank 15: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 44: ddp rank 0 iter_start,iter_end = 0 8
global rank 44: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 4: ddp rank 0 iter_start,iter_end = 0 8
global rank 4: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 61: ddp rank 0 iter_start,iter_end = 0 8
global rank 61: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 43: ddp rank 0 iter_start,iter_end = 0 8
global rank 43: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 62: ddp rank 0 iter_start,iter_end = 0 8
global rank 62: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 41: ddp rank 0 iter_start,iter_end = 0 8
global rank 41: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 31: ddp rank 0 iter_start,iter_end = 0 8
global rank 31: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 55: ddp rank 0 iter_start,iter_end = 0 8
global rank 55: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 35: ddp rank 0 iter_start,iter_end = 0 8
global rank 35: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 2: ddp rank 0 iter_start,iter_end = 0 8
global rank 2: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 8: ddp rank 0 iter_start,iter_end = 0 8
global rank 8: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 39: ddp rank 0 iter_start,iter_end = 0 8
global rank 39: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 13: ddp rank 0 iter_start,iter_end = 0 8
global rank 13: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 24: ddp rank 0 iter_start,iter_end = 0 8
global rank 24: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 33: ddp rank 0 iter_start,iter_end = 0 8
global rank 33: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 63: ddp rank 0 iter_start,iter_end = 0 8
global rank 63: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 30: ddp rank 0 iter_start,iter_end = 0 8
global rank 30: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 29: ddp rank 0 iter_start,iter_end = 0 8
global rank 29: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 10: ddp rank 0 iter_start,iter_end = 0 8
global rank 10: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 7: ddp rank 0 iter_start,iter_end = 0 8
global rank 7: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 50: ddp rank 0 iter_start,iter_end = 0 8
global rank 50: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 37: ddp rank 0 iter_start,iter_end = 0 8
global rank 37: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 48: ddp rank 0 iter_start,iter_end = 0 8
global rank 48: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 40: ddp rank 0 iter_start,iter_end = 0 8
global rank 40: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 38: ddp rank 0 iter_start,iter_end = 0 8
global rank 38: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 58: ddp rank 0 iter_start,iter_end = 0 8
global rank 58: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
MUST: Model 630.824965 M TFLOPS: 7.59721134205738

--> cuda max reserved memory = 32.2363
--> max reserved percentage = 50.38 %

--> cuda max memory allocated = 22.6942
--> max allocated percentage = 35.47 %

--> peak active memory = 29.5775
--> peak active memory 46.23 %

cudaMalloc retries = 0
cuda OOM = 0

HERE1 Namespace(arch='orbit_linear', batch_size=2, channels=256, depth=32, embed_dim=8192, fa2=True, fsdp_size=1, imagex=128, imagey=256, max_epochs=1, num_heads=64, real=False, seq_par_size=1, simple_ddp_size=1, tensor_par_size=64, yaml_config='configs/ERA5-100million-91variables.yaml') 32.2363 7.59721134205738 tensor(26297569285)
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
0.04user 0.33system 2:56.23elapsed 0%CPU (0avgtext+0avgdata 18432maxresident)k
3082inputs+1848outputs (10major+3359minor)pagefaults 0swaps
