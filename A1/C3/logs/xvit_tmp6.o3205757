
Lmod is automatically replacing "cce/18.0.1" with "gcc-native/13.2".


Lmod is automatically replacing "PrgEnv-cray/8.6.0" with "PrgEnv-gnu/8.6.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) cray-libsci/24.11.0     2) cray-mpich/8.1.31     3) darshan-runtime/3.4.6-mpi


Lmod is automatically replacing "gcc-native/13.2" with "gcc/12.2.0".


Inactive Modules:
  1) darshan-runtime

The following have been reloaded with a version change:
  1) cray-libsci/24.11.0 => cray-libsci/23.09.1.1
  2) cray-mpich/8.1.31 => cray-mpich/8.1.27
  3) libfabric/1.22.0 => libfabric/1.20.1

Lmod has detected the following error: These module(s) or extension(s) exist
but cannot be loaded as requested: "darshan-runtime"
   Try: "module spider darshan-runtime" to see how to load the module(s).



for i in {orbit,orbit_token,orbit_token_agg,orbit_hier,orbit_linear}; do for j in {2,}; do for k in {256,}; do python train.py \ configs/ERA5-100million-91variables.yaml \ --max_epochs 1 \ --fa2 \ --fsdp_size 1 \ --simple_ddp_size 1 \ --seq_par_size 1 \ --tensor_par_size 16 \ --batch_size $j \ --arch $i \ --channels $k \ --imagex 128 \ --imagey 256 \ --embed_dim 6144 \ --depth 32 \ --num_heads 32 echo "sleeping..." sleep 5 echo "Done" done done done
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:33:43,014] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:33:43,014] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:33:43,014] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:33:43,014] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:33:43,014] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:33:43,014] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:33:43,014] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:33:43,014] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:33:43,016] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:33:43,016] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:33:43,016] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:33:43,016] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:33:43,016] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:33:43,016] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:33:43,016] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:33:43,016] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
HERE0 Namespace(arch='orbit', batch_size=2, channels=256, depth=32, embed_dim=6144, fa2=True, fsdp_size=1, imagex=128, imagey=256, max_epochs=1, num_heads=32, real=False, seq_par_size=1, simple_ddp_size=1, tensor_par_size=16, yaml_config='configs/ERA5-100million-91variables.yaml')
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
config_path  configs/ERA5-100million-91variables.yaml
rank 2 Before initialize parallelism groups
rank 9 Before initialize parallelism groups
rank 7 Before initialize parallelism groups
rank 12 Before initialize parallelism groups
rank 3 Before initialize parallelism groups
rank 14 Before initialize parallelism groups
rank 10 Before initialize parallelism groups
rank 15 Before initialize parallelism groups
rank 11 Before initialize parallelism groups
rank 8 Before initialize parallelism groups
rank 4 Before initialize parallelism groups
rank 6 Before initialize parallelism groups
rank 5 Before initialize parallelism groups
rank 13 Before initialize parallelism groups
rank 1 Before initialize parallelism groups
{'seed_everything': 42, 'trainer': {'checkpoint_path': '/lustre/orion/stf218/world-shared/atsaris/tmp/tmp_checkpoints', 'checkpoint_filename': 'multi_last', 'resume_from_checkpoint': False}, 'parallelism': {'cpu_offloading': False}, 'model': {'lr': 2e-05, 'beta_1': 0.9, 'beta_2': 0.95, 'weight_decay': '1e-5', 'warmup_steps': 1000, 'max_steps': 20000, 'warmup_start_lr': '1e-8', 'eta_min': '1e-8', 'net': {'class_path': 'climax.arch.ClimaX', 'init_args': {'default_vars': ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_70', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000'], 'patch_size': 4, 'decoder_depth': 2, 'mlp_ratio': 4, 'drop_path': 0.1, 'drop_rate': 0.1, 'aggregated_variables': 1}}}, 'data': {'dict_root_dirs': {'mpi-esm': '/lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0'}, 'dict_start_idx': {'mpi-esm': 0}, 'dict_end_idx': {'mpi-esm': 1}, 'dict_in_variables': {'mpi-esm': ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000']}, 'dict_out_variables': {'mpi-esm': ['geopotential_500', 'temperature_850', '2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind']}, 'dict_max_predict_ranges': {'mpi-esm': 720}, 'dict_random_lead_time': {'mpi-esm': True}, 'dict_hrs_each_step': {'mpi-esm': 1}, 'dict_buffer_sizes': {'mpi-esm': 1}, 'num_workers': 1, 'pin_memory': False}}
rank 2 After initialize parallelism groups
rank 12 After initialize parallelism groups
rank 3 After initialize parallelism groups
rank 10 After initialize parallelism groups
rank 14 After initialize parallelism groups
rank 7 After initialize parallelism groups
rank 15 After initialize parallelism groups
rank 9 After initialize parallelism groups
rank 11 After initialize parallelism groups
rank 4 After initialize parallelism groups
rank 8 After initialize parallelism groups
rank 6 After initialize parallelism groups
rank 5 After initialize parallelism groups
rank 13 After initialize parallelism groups
rank 1 After initialize parallelism groups
max_epochs 1 data_par_size 1 fsdp_size 1 simple_ddp_size 1 tensor_par_size 16 seq_par_size 1 cpu_offloading False
lr  2e-05 beta_1  0.9 beta_2 0.95 weight_decay 1e-05 class_path climax.arch.ClimaX default_vars ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_70', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000']
img_size_x 128 img_size_y 256 patch_size 4 emb_dim 6144 depth 32 decoder_depth 2 num_heads 32 mlp_ratio 4 drop_path 0.1 drop_rate 0.1 aggregated_variables 1
dict_root_dirs {'mpi-esm': '/lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0'} dict_start_idx {'mpi-esm': 0} dict_end_idx {'mpi-esm': 1} batch_size 2 num_workers 1
warmup_steps 1000 max_steps 20000 warmup_start_lr 1e-08 eta_min 1e-08
checkpoint_path /lustre/orion/stf218/world-shared/atsaris/tmp/tmp_checkpoints checkpoint_filename multi_last resume_from_checkpoint False
rank 0 Before initialize parallelism groups
rank 0 After initialize parallelism groups
rank 1 After initialize model
rank 1 Before the second dist.barrier()
rank 12 After initialize model
rank 12 Before the second dist.barrier()
rank 2 After initialize model
rank 2 Before the second dist.barrier()
rank 9 After initialize model
rank 9 Before the second dist.barrier()
rank 11 After initialize model
rank 11 Before the second dist.barrier()
rank 6 After initialize model
rank 6 Before the second dist.barrier()
rank 8 After initialize model
rank 8 Before the second dist.barrier()
rank 7 After initialize model
rank 7 Before the second dist.barrier()
rank 4 After initialize model
rank 4 Before the second dist.barrier()
rank 14 After initialize model
rank 14 Before the second dist.barrier()
rank 0 After initialize model
rank 0 Before the second dist.barrier()
rank 13 After initialize model
rank 13 Before the second dist.barrier()
rank 3 After initialize model
rank 3 Before the second dist.barrier()
rank 5 After initialize model
rank 5 Before the second dist.barrier()
rank 15 After initialize model
rank 15 Before the second dist.barrier()
rank 10 After initialize model
rank 10 Before the second dist.barrier()
rank 0 After the second dist.barrier()
rank 4 After the second dist.barrier()
rank 5 After the second dist.barrier()
rank 1 After the second dist.barrier()
rank 7 After the second dist.barrier()
rank 6 After the second dist.barrier()
rank 2 After the second dist.barrier()
rank 3 After the second dist.barrier()
rank 8 After the second dist.barrier()
rank 14 After the second dist.barrier()
rank 15 After the second dist.barrier()
rank 9 After the second dist.barrier()
rank 12 After the second dist.barrier()
rank 13 After the second dist.barrier()
rank 10 After the second dist.barrier()
rank 11 After the second dist.barrier()
parameter name  var_embed  requires_gradient  True size torch.Size([1, 256, 6144])
parameter name  var_query  requires_gradient  True size torch.Size([1, 1, 6144])
parameter name  pos_embed  requires_gradient  True size torch.Size([1, 2048, 6144])
parameter name  token_embeds.0.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.0.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.1.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.1.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.2.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.2.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.3.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.3.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.4.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.4.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.5.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.5.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.6.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.6.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.7.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.7.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.8.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.8.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.9.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.9.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.10.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.10.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.11.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.11.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.12.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.12.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.13.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.13.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.14.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.14.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.15.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.15.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.16.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.16.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.17.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.17.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.18.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.18.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.19.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.19.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.20.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.20.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.21.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.21.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.22.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.22.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.23.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.23.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.24.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.24.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.25.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.25.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.26.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.26.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.27.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.27.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.28.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.28.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.29.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.29.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.30.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.30.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.31.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.31.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.32.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.32.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.33.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.33.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.34.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.34.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.35.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.35.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.36.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.36.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.37.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.37.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.38.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.38.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.39.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.39.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.40.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.40.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.41.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.41.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.42.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.42.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.43.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.43.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.44.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.44.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.45.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.45.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.46.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.46.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.47.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.47.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.48.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.48.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.49.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.49.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.50.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.50.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.51.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.51.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.52.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.52.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.53.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.53.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.54.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.54.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.55.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.55.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.56.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.56.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.57.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.57.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.58.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.58.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.59.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.59.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.60.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.60.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.61.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.61.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.62.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.62.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.63.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.63.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.64.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.64.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.65.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.65.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.66.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.66.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.67.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.67.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.68.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.68.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.69.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.69.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.70.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.70.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.71.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.71.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.72.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.72.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.73.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.73.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.74.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.74.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.75.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.75.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.76.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.76.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.77.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.77.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.78.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.78.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.79.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.79.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.80.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.80.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.81.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.81.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.82.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.82.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.83.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.83.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.84.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.84.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.85.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.85.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.86.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.86.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.87.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.87.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.88.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.88.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.89.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.89.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.90.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.90.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.91.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.91.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.92.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.92.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.93.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.93.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.94.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.94.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.95.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.95.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.96.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.96.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.97.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.97.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.98.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.98.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.99.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.99.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.100.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.100.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.101.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.101.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.102.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.102.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.103.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.103.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.104.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.104.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.105.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.105.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.106.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.106.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.107.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.107.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.108.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.108.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.109.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.109.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.110.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.110.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.111.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.111.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.112.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.112.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.113.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.113.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.114.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.114.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.115.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.115.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.116.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.116.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.117.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.117.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.118.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.118.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.119.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.119.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.120.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.120.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.121.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.121.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.122.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.122.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.123.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.123.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.124.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.124.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.125.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.125.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.126.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.126.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.127.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.127.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.128.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.128.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.129.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.129.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.130.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.130.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.131.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.131.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.132.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.132.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.133.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.133.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.134.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.134.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.135.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.135.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.136.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.136.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.137.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.137.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.138.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.138.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.139.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.139.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.140.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.140.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.141.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.141.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.142.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.142.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.143.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.143.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.144.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.144.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.145.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.145.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.146.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.146.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.147.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.147.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.148.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.148.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.149.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.149.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.150.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.150.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.151.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.151.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.152.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.152.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.153.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.153.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.154.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.154.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.155.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.155.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.156.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.156.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.157.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.157.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.158.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.158.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.159.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.159.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.160.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.160.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.161.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.161.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.162.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.162.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.163.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.163.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.164.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.164.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.165.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.165.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.166.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.166.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.167.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.167.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.168.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.168.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.169.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.169.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.170.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.170.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.171.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.171.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.172.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.172.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.173.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.173.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.174.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.174.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.175.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.175.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.176.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.176.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.177.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.177.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.178.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.178.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.179.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.179.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.180.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.180.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.181.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.181.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.182.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.182.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.183.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.183.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.184.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.184.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.185.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.185.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.186.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.186.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.187.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.187.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.188.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.188.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.189.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.189.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.190.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.190.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.191.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.191.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.192.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.192.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.193.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.193.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.194.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.194.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.195.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.195.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.196.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.196.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.197.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.197.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.198.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.198.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.199.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.199.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.200.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.200.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.201.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.201.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.202.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.202.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.203.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.203.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.204.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.204.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.205.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.205.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.206.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.206.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.207.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.207.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.208.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.208.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.209.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.209.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.210.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.210.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.211.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.211.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.212.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.212.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.213.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.213.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.214.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.214.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.215.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.215.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.216.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.216.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.217.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.217.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.218.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.218.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.219.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.219.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.220.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.220.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.221.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.221.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.222.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.222.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.223.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.223.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.224.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.224.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.225.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.225.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.226.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.226.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.227.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.227.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.228.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.228.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.229.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.229.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.230.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.230.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.231.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.231.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.232.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.232.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.233.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.233.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.234.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.234.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.235.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.235.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.236.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.236.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.237.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.237.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.238.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.238.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.239.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.239.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.240.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.240.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.241.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.241.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.242.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.242.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.243.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.243.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.244.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.244.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.245.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.245.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.246.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.246.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.247.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.247.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.248.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.248.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.249.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.249.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.250.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.250.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.251.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.251.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.252.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.252.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.253.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.253.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.254.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.254.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.255.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.255.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  var_agg.q.weight  requires_gradient  True size torch.Size([384, 6144])
parameter name  var_agg.kv.weight  requires_gradient  True size torch.Size([768, 6144])
parameter name  var_agg.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  var_agg.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  lead_time_embed.weight  requires_gradient  True size torch.Size([6144, 1])
parameter name  lead_time_embed.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.0.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.0.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.0.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.0.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.0.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.0.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.0.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.0.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.0.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.0.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.0.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.0.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.1.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.1.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.1.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.1.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.1.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.1.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.1.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.1.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.1.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.1.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.1.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.1.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.2.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.2.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.2.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.2.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.2.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.2.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.2.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.2.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.2.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.2.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.2.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.2.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.3.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.3.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.3.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.3.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.3.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.3.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.3.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.3.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.3.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.3.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.3.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.3.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.4.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.4.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.4.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.4.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.4.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.4.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.4.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.4.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.4.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.4.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.4.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.4.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.5.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.5.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.5.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.5.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.5.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.5.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.5.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.5.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.5.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.5.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.5.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.5.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.6.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.6.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.6.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.6.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.6.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.6.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.6.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.6.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.6.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.6.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.6.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.6.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.7.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.7.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.7.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.7.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.7.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.7.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.7.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.7.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.7.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.7.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.7.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.7.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.8.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.8.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.8.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.8.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.8.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.8.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.8.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.8.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.8.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.8.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.8.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.8.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.9.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.9.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.9.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.9.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.9.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.9.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.9.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.9.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.9.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.9.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.9.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.9.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.10.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.10.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.10.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.10.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.10.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.10.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.10.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.10.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.10.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.10.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.10.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.10.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.11.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.11.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.11.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.11.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.11.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.11.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.11.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.11.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.11.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.11.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.11.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.11.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.12.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.12.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.12.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.12.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.12.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.12.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.12.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.12.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.12.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.12.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.12.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.12.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.13.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.13.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.13.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.13.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.13.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.13.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.13.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.13.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.13.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.13.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.13.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.13.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.14.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.14.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.14.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.14.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.14.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.14.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.14.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.14.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.14.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.14.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.14.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.14.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.15.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.15.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.15.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.15.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.15.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.15.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.15.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.15.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.15.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.15.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.15.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.15.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.16.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.16.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.16.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.16.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.16.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.16.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.16.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.16.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.16.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.16.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.16.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.16.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.17.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.17.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.17.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.17.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.17.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.17.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.17.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.17.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.17.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.17.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.17.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.17.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.18.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.18.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.18.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.18.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.18.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.18.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.18.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.18.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.18.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.18.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.18.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.18.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.19.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.19.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.19.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.19.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.19.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.19.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.19.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.19.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.19.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.19.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.19.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.19.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.20.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.20.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.20.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.20.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.20.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.20.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.20.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.20.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.20.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.20.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.20.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.20.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.21.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.21.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.21.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.21.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.21.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.21.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.21.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.21.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.21.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.21.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.21.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.21.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.22.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.22.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.22.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.22.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.22.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.22.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.22.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.22.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.22.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.22.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.22.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.22.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.23.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.23.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.23.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.23.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.23.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.23.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.23.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.23.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.23.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.23.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.23.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.23.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.24.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.24.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.24.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.24.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.24.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.24.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.24.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.24.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.24.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.24.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.24.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.24.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.25.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.25.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.25.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.25.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.25.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.25.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.25.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.25.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.25.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.25.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.25.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.25.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.26.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.26.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.26.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.26.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.26.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.26.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.26.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.26.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.26.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.26.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.26.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.26.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.27.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.27.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.27.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.27.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.27.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.27.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.27.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.27.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.27.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.27.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.27.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.27.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.28.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.28.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.28.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.28.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.28.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.28.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.28.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.28.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.28.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.28.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.28.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.28.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.29.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.29.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.29.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.29.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.29.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.29.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.29.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.29.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.29.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.29.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.29.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.29.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.30.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.30.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.30.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.30.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.30.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.30.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.30.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.30.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.30.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.30.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.30.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.30.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.31.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.31.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.31.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.31.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.31.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.31.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.31.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.31.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.31.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.31.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.31.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.31.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  norm.weight  requires_gradient  True size torch.Size([6144])
parameter name  norm.bias  requires_gradient  True size torch.Size([6144])
parameter name  head.0.weight  requires_gradient  True size torch.Size([6144, 6144])
parameter name  head.0.bias  requires_gradient  True size torch.Size([6144])
parameter name  head.2.weight  requires_gradient  True size torch.Size([6144, 6144])
parameter name  head.2.bias  requires_gradient  True size torch.Size([6144])
parameter name  head.5.weight  requires_gradient  True size torch.Size([4096, 6144])
parameter name  head.5.bias  requires_gradient  True size torch.Size([4096])
total_params before FSDP tensor(14796666880) params_per_gpu tensor(1058283520)
total_params after FSDP FullyShardedDataParallel(
  (_fsdp_wrapped_module): ClimaX(
    (token_embeds): ModuleList(
      (0-255): 256 x PatchEmbed(
        (proj): Conv2d(1, 6144, kernel_size=(4, 4), stride=(4, 4))
        (norm): Identity()
      )
    )
    (var_agg): FullyShardedDataParallel(
      (_fsdp_wrapped_module): CheckpointWrapper(
        (_checkpoint_wrapped_module): VariableMapping_Attention(
          (q): Linear(in_features=6144, out_features=384, bias=False)
          (kv): Linear(in_features=6144, out_features=768, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=6144, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (lead_time_embed): Linear(in_features=1, out_features=6144, bias=True)
    (pos_drop): Dropout(p=0.1, inplace=False)
    (blocks): ModuleList(
      (0-31): 32 x FullyShardedDataParallel(
        (_fsdp_wrapped_module): CheckpointWrapper(
          (_checkpoint_wrapped_module): Block(
            (norm1): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=6144, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=6144, bias=True)
              (proj_drop): Dropout(p=0.1, inplace=False)
            )
            (ls1): Identity()
            (norm2): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=6144, out_features=1536, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.1, inplace=False)
              (fc2): Linear(in_features=1536, out_features=6144, bias=True)
            )
            (ls2): Identity()
          )
        )
      )
    )
    (norm): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)
    (head): Sequential(
      (0): Linear(in_features=6144, out_features=6144, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=6144, out_features=6144, bias=True)
      (3): GELU(approximate='none')
      (4): Pred_Rearrange()
      (5): Linear(in_features=6144, out_features=4096, bias=True)
    )
  )
)
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
rank  0 lister_train reset: mpi-esm 9 9 9
rank  0 lister_test reset: mpi-esm 9 9 9
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
rank  15 lister_train reset: mpi-esm 9 9 9
rank  15 lister_test reset: mpi-esm 9 9 9
use_ddstore is : 0
use_ddstore is : 0
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
memory stats reset, ready to track
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
MIOpen(HIP): Warning [PlainTextDb] Unable to create a directory: "/tmp/.config/miopen"
MIOpen(HIP): Warning [PlainTextDb] Unable to create a directory: "/tmp/.config/miopen"
epoch-train:  0 batch_idx 0 world_rank 0  loss  4.09375
epoch-train:  0 batch_idx 1 world_rank 0  loss  nan
epoch-train:  0 batch_idx 2 world_rank 0  loss  nan
epoch-train:  0 batch_idx 3 world_rank 0  loss  nan
[2025-03-16 18:34:47,008] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:34:47,008] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:34:47,009] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:34:47,009] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:34:47,010] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:34:47,010] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:34:47,010] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:34:47,010] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:34:47,011] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:34:47,011] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:34:47,012] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:34:47,013] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:34:47,013] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:34:47,014] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:34:47,014] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:34:47,015] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:34:48,397] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:34:48,398] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:34:48,398] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:34:48,398] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:34:48,398] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:34:48,398] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:34:48,398] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:34:48,398] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:34:48,398] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:34:48,399] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:34:48,399] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:34:48,399] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:34:48,399] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:34:48,399] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:34:48,399] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:34:48,400] [INFO] [profiler.py:226:end_profile] Flops profiler finished
global rank 11: ddp rank 0 iter_start,iter_end = 0 8
global rank 11: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 1: ddp rank 0 iter_start,iter_end = 0 8
global rank 1: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 10: ddp rank 0 iter_start,iter_end = 0 8
global rank 10: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 3: ddp rank 0 iter_start,iter_end = 0 8
global rank 3: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 5: ddp rank 0 iter_start,iter_end = 0 8
global rank 5: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 7: ddp rank 0 iter_start,iter_end = 0 8
global rank 7: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 0: ddp rank 0 iter_start,iter_end = 0 8
global rank 0: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 14: ddp rank 0 iter_start,iter_end = 0 8
global rank 14: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 6: ddp rank 0 iter_start,iter_end = 0 8
global rank 6: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 13: ddp rank 0 iter_start,iter_end = 0 8
global rank 13: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 9: ddp rank 0 iter_start,iter_end = 0 8
global rank 9: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 4: ddp rank 0 iter_start,iter_end = 0 8
global rank 4: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 15: ddp rank 0 iter_start,iter_end = 0 8
global rank 15: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 2: ddp rank 0 iter_start,iter_end = 0 8
global rank 2: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 8: ddp rank 0 iter_start,iter_end = 0 8
global rank 8: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 12: ddp rank 0 iter_start,iter_end = 0 8
global rank 12: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
I am here
MUST: Model 1058.28352 M TFLOPS: 8.701432042692511

--> cuda max reserved memory = 62.8027
--> max reserved percentage = 98.15 %

--> cuda max memory allocated = 54.5112
--> max allocated percentage = 85.19 %

--> peak active memory = 54.5112
--> peak active memory 85.19 %

cudaMalloc retries = 4
cuda OOM = 0

--> Recommend decreasing batch size...cuda retries can greatly degrade perf!
HERE1 Namespace(arch='orbit', batch_size=2, channels=256, depth=32, embed_dim=6144, fa2=True, fsdp_size=1, imagex=128, imagey=256, max_epochs=1, num_heads=32, real=False, seq_par_size=1, simple_ddp_size=1, tensor_par_size=16, yaml_config='configs/ERA5-100million-91variables.yaml') 62.8027 8.701432042692511 tensor(14796666880)
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:35:03,371] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:35:03,371] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:35:03,371] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:35:03,371] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:35:03,371] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:35:03,371] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:35:03,371] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:35:03,371] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:35:03,371] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:35:03,371] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:35:03,371] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:35:03,371] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:35:03,371] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:35:03,371] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:35:03,371] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:35:03,371] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
HERE0 Namespace(arch='orbit_token', batch_size=2, channels=256, depth=32, embed_dim=6144, fa2=True, fsdp_size=1, imagex=128, imagey=256, max_epochs=1, num_heads=32, real=False, seq_par_size=1, simple_ddp_size=1, tensor_par_size=16, yaml_config='configs/ERA5-100million-91variables.yaml')
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
config_path  configs/ERA5-100million-91variables.yaml
rank 1 Before initialize parallelism groups
rank 3 Before initialize parallelism groups
rank 4 Before initialize parallelism groups
rank 2 Before initialize parallelism groups
rank 9 Before initialize parallelism groups
rank 11 Before initialize parallelism groups
rank 15 Before initialize parallelism groups
rank 12 Before initialize parallelism groups
rank 14 Before initialize parallelism groups
rank 13 Before initialize parallelism groups
rank 7 Before initialize parallelism groups
rank 6 Before initialize parallelism groups
rank 3 After initialize parallelism groups
rank 2 After initialize parallelism groups
rank 4 After initialize parallelism groups
rank 1 After initialize parallelism groups
rank 15 After initialize parallelism groups
rank 13 After initialize parallelism groups
rank 14 After initialize parallelism groups
rank 12 After initialize parallelism groups
rank 8 Before initialize parallelism groups
rank 9 After initialize parallelism groups
rank 11 After initialize parallelism groups
rank 10 Before initialize parallelism groups
rank 5 Before initialize parallelism groups
{'seed_everything': 42, 'trainer': {'checkpoint_path': '/lustre/orion/stf218/world-shared/atsaris/tmp/tmp_checkpoints', 'checkpoint_filename': 'multi_last', 'resume_from_checkpoint': False}, 'parallelism': {'cpu_offloading': False}, 'model': {'lr': 2e-05, 'beta_1': 0.9, 'beta_2': 0.95, 'weight_decay': '1e-5', 'warmup_steps': 1000, 'max_steps': 20000, 'warmup_start_lr': '1e-8', 'eta_min': '1e-8', 'net': {'class_path': 'climax.arch.ClimaX', 'init_args': {'default_vars': ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_70', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000'], 'patch_size': 4, 'decoder_depth': 2, 'mlp_ratio': 4, 'drop_path': 0.1, 'drop_rate': 0.1, 'aggregated_variables': 1}}}, 'data': {'dict_root_dirs': {'mpi-esm': '/lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0'}, 'dict_start_idx': {'mpi-esm': 0}, 'dict_end_idx': {'mpi-esm': 1}, 'dict_in_variables': {'mpi-esm': ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000']}, 'dict_out_variables': {'mpi-esm': ['geopotential_500', 'temperature_850', '2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind']}, 'dict_max_predict_ranges': {'mpi-esm': 720}, 'dict_random_lead_time': {'mpi-esm': True}, 'dict_hrs_each_step': {'mpi-esm': 1}, 'dict_buffer_sizes': {'mpi-esm': 1}, 'num_workers': 1, 'pin_memory': False}}
rank 6 After initialize parallelism groups
rank 7 After initialize parallelism groups
rank 8 After initialize parallelism groups
rank 10 After initialize parallelism groups
rank 5 After initialize parallelism groups
max_epochs 1 data_par_size 1 fsdp_size 1 simple_ddp_size 1 tensor_par_size 16 seq_par_size 1 cpu_offloading False
lr  2e-05 beta_1  0.9 beta_2 0.95 weight_decay 1e-05 class_path climax.arch.ClimaX default_vars ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_70', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000']
img_size_x 128 img_size_y 256 patch_size 4 emb_dim 6144 depth 32 decoder_depth 2 num_heads 32 mlp_ratio 4 drop_path 0.1 drop_rate 0.1 aggregated_variables 1
dict_root_dirs {'mpi-esm': '/lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0'} dict_start_idx {'mpi-esm': 0} dict_end_idx {'mpi-esm': 1} batch_size 2 num_workers 1
warmup_steps 1000 max_steps 20000 warmup_start_lr 1e-08 eta_min 1e-08
checkpoint_path /lustre/orion/stf218/world-shared/atsaris/tmp/tmp_checkpoints checkpoint_filename multi_last resume_from_checkpoint False
rank 0 Before initialize parallelism groups
rank 0 After initialize parallelism groups
rank 6 After initialize model
rank 6 Before the second dist.barrier()
rank 2 After initialize model
rank 2 Before the second dist.barrier()
rank 3 After initialize model
rank 3 Before the second dist.barrier()
rank 0 After initialize model
rank 0 Before the second dist.barrier()
rank 4 After initialize model
rank 4 Before the second dist.barrier()
rank 1 After initialize model
rank 1 Before the second dist.barrier()
rank 7 After initialize model
rank 7 Before the second dist.barrier()
rank 5 After initialize model
rank 5 Before the second dist.barrier()
rank 10 After initialize model
rank 10 Before the second dist.barrier()
rank 9 After initialize model
rank 9 Before the second dist.barrier()
rank 8 After initialize model
rank 8 Before the second dist.barrier()
rank 12 After initialize model
rank 12 Before the second dist.barrier()
rank 14 After initialize model
rank 14 Before the second dist.barrier()
rank 15 After initialize model
rank 15 Before the second dist.barrier()
rank 11 After initialize model
rank 11 Before the second dist.barrier()
rank 13 After initialize model
rank 13 Before the second dist.barrier()
rank 0 After the second dist.barrier()
rank 2 After the second dist.barrier()
rank 6 After the second dist.barrier()
rank 9 After the second dist.barrier()
rank 7 After the second dist.barrier()
rank 12 After the second dist.barrier()
rank 3 After the second dist.barrier()
rank 8 After the second dist.barrier()
rank 5 After the second dist.barrier()
rank 11 After the second dist.barrier()
rank 13 After the second dist.barrier()
rank 14 After the second dist.barrier()
rank 15 After the second dist.barrier()
parameter name  var_embed  requires_gradient  True size torch.Size([1, 256, 6144])
rank 1 After the second dist.barrier()
rank 10 After the second dist.barrier()
rank 4 After the second dist.barrier()
parameter name  pos_embed  requires_gradient  True size torch.Size([1, 2048, 6144])
parameter name  token_embeds.0.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.0.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.1.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.1.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.2.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.2.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.3.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.3.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.4.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.4.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.5.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.5.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.6.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.6.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.7.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.7.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.8.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.8.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.9.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.9.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.10.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.10.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.11.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.11.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.12.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.12.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.13.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.13.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.14.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.14.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.15.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.15.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.16.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.16.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.17.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.17.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.18.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.18.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.19.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.19.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.20.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.20.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.21.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.21.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.22.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.22.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.23.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.23.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.24.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.24.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.25.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.25.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.26.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.26.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.27.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.27.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.28.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.28.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.29.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.29.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.30.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.30.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.31.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.31.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.32.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.32.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.33.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.33.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.34.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.34.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.35.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.35.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.36.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.36.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.37.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.37.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.38.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.38.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.39.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.39.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.40.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.40.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.41.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.41.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.42.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.42.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.43.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.43.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.44.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.44.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.45.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.45.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.46.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.46.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.47.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.47.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.48.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.48.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.49.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.49.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.50.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.50.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.51.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.51.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.52.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.52.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.53.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.53.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.54.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.54.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.55.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.55.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.56.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.56.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.57.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.57.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.58.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.58.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.59.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.59.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.60.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.60.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.61.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.61.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.62.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.62.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.63.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.63.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.64.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.64.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.65.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.65.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.66.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.66.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.67.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.67.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.68.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.68.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.69.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.69.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.70.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.70.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.71.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.71.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.72.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.72.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.73.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.73.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.74.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.74.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.75.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.75.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.76.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.76.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.77.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.77.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.78.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.78.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.79.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.79.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.80.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.80.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.81.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.81.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.82.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.82.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.83.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.83.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.84.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.84.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.85.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.85.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.86.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.86.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.87.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.87.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.88.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.88.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.89.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.89.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.90.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.90.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.91.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.91.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.92.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.92.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.93.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.93.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.94.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.94.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.95.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.95.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.96.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.96.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.97.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.97.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.98.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.98.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.99.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.99.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.100.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.100.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.101.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.101.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.102.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.102.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.103.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.103.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.104.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.104.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.105.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.105.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.106.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.106.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.107.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.107.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.108.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.108.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.109.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.109.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.110.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.110.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.111.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.111.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.112.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.112.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.113.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.113.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.114.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.114.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.115.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.115.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.116.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.116.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.117.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.117.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.118.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.118.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.119.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.119.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.120.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.120.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.121.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.121.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.122.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.122.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.123.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.123.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.124.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.124.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.125.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.125.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.126.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.126.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.127.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.127.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.128.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.128.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.129.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.129.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.130.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.130.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.131.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.131.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.132.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.132.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.133.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.133.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.134.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.134.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.135.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.135.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.136.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.136.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.137.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.137.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.138.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.138.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.139.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.139.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.140.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.140.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.141.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.141.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.142.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.142.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.143.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.143.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.144.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.144.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.145.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.145.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.146.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.146.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.147.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.147.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.148.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.148.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.149.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.149.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.150.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.150.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.151.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.151.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.152.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.152.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.153.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.153.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.154.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.154.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.155.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.155.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.156.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.156.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.157.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.157.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.158.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.158.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.159.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.159.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.160.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.160.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.161.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.161.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.162.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.162.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.163.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.163.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.164.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.164.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.165.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.165.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.166.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.166.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.167.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.167.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.168.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.168.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.169.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.169.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.170.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.170.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.171.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.171.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.172.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.172.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.173.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.173.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.174.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.174.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.175.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.175.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.176.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.176.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.177.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.177.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.178.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.178.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.179.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.179.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.180.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.180.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.181.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.181.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.182.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.182.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.183.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.183.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.184.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.184.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.185.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.185.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.186.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.186.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.187.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.187.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.188.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.188.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.189.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.189.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.190.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.190.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.191.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.191.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.192.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.192.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.193.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.193.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.194.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.194.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.195.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.195.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.196.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.196.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.197.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.197.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.198.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.198.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.199.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.199.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.200.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.200.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.201.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.201.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.202.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.202.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.203.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.203.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.204.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.204.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.205.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.205.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.206.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.206.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.207.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.207.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.208.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.208.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.209.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.209.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.210.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.210.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.211.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.211.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.212.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.212.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.213.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.213.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.214.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.214.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.215.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.215.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.216.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.216.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.217.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.217.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.218.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.218.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.219.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.219.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.220.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.220.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.221.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.221.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.222.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.222.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.223.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.223.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.224.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.224.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.225.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.225.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.226.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.226.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.227.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.227.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.228.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.228.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.229.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.229.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.230.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.230.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.231.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.231.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.232.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.232.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.233.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.233.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.234.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.234.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.235.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.235.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.236.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.236.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.237.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.237.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.238.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.238.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.239.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.239.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.240.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.240.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.241.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.241.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.242.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.242.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.243.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.243.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.244.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.244.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.245.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.245.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.246.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.246.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.247.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.247.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.248.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.248.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.249.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.249.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.250.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.250.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.251.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.251.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.252.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.252.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.253.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.253.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.254.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.254.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.255.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.255.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  lead_time_embed.weight  requires_gradient  True size torch.Size([6144, 1])
parameter name  lead_time_embed.bias  requires_gradient  True size torch.Size([6144])
parameter name  head.0.weight  requires_gradient  True size torch.Size([6144, 6144])
parameter name  head.0.bias  requires_gradient  True size torch.Size([6144])
parameter name  head.2.weight  requires_gradient  True size torch.Size([6144, 6144])
parameter name  head.2.bias  requires_gradient  True size torch.Size([6144])
parameter name  head.5.weight  requires_gradient  True size torch.Size([4096, 6144])
parameter name  head.5.bias  requires_gradient  True size torch.Size([4096])
total_params before FSDP tensor(141586432) params_per_gpu tensor(141586432)
total_params after FSDP FullyShardedDataParallel(
  (_fsdp_wrapped_module): ClimaX(
    (token_embeds): ModuleList(
      (0-255): 256 x PatchEmbed(
        (proj): Conv2d(1, 6144, kernel_size=(4, 4), stride=(4, 4))
        (norm): Identity()
      )
    )
    (lead_time_embed): Linear(in_features=1, out_features=6144, bias=True)
    (pos_drop): Dropout(p=0.1, inplace=False)
    (head): Sequential(
      (0): Linear(in_features=6144, out_features=6144, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=6144, out_features=6144, bias=True)
      (3): GELU(approximate='none')
      (4): Pred_Rearrange()
      (5): Linear(in_features=6144, out_features=4096, bias=True)
    )
  )
)
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
rank  15 lister_train reset: mpi-esm 9 9 9
rank  15 lister_test reset: mpi-esm 9 9 9
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
rank  0 lister_train reset: mpi-esm 9 9 9
rank  0 lister_test reset: mpi-esm 9 9 9
use_ddstore is : 0
use_ddstore is : 0
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
memory stats reset, ready to track
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
epoch-train:  0 batch_idx 0 world_rank 0  loss  0.94921875
epoch-train:  0 batch_idx 1 world_rank 0  loss  0.9453125
epoch-train:  0 batch_idx 2 world_rank 0  loss  0.9453125
epoch-train:  0 batch_idx 3 world_rank 0  loss  0.94140625
[2025-03-16 18:35:29,799] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:35:30,101] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:35:30,103] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:35:30,104] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:35:30,104] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:35:30,105] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:35:30,106] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:35:30,106] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:35:30,114] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:35:30,117] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:35:30,117] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:35:30,117] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:35:30,117] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:35:30,117] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:35:30,118] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:35:30,118] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:35:30,222] [INFO] [profiler.py:226:end_profile] Flops profiler finished
global rank 0: ddp rank 0 iter_start,iter_end = 0 8
global rank 0: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
[2025-03-16 18:35:30,545] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:35:30,547] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:35:30,547] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:35:30,548] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:35:30,549] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:35:30,550] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:35:30,550] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:35:30,556] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:35:30,556] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:35:30,556] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:35:30,557] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:35:30,558] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:35:30,559] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:35:30,560] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:35:30,561] [INFO] [profiler.py:226:end_profile] Flops profiler finished
global rank 2: ddp rank 0 iter_start,iter_end = 0 8
global rank 2: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 5: ddp rank 0 iter_start,iter_end = 0 8
global rank 5: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 4: ddp rank 0 iter_start,iter_end = 0 8
global rank 4: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 9: ddp rank 0 iter_start,iter_end = 0 8
global rank 9: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 13: ddp rank 0 iter_start,iter_end = 0 8
global rank 13: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 12: ddp rank 0 iter_start,iter_end = 0 8
global rank 12: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 11: ddp rank 0 iter_start,iter_end = 0 8
global rank 11: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 1: ddp rank 0 iter_start,iter_end = 0 8
global rank 1: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 3: ddp rank 0 iter_start,iter_end = 0 8
global rank 3: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 6: ddp rank 0 iter_start,iter_end = 0 8
global rank 6: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 14: ddp rank 0 iter_start,iter_end = 0 8
global rank 14: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 7: ddp rank 0 iter_start,iter_end = 0 8
global rank 7: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 8: ddp rank 0 iter_start,iter_end = 0 8
global rank 8: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 10: ddp rank 0 iter_start,iter_end = 0 8
global rank 10: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 15: ddp rank 0 iter_start,iter_end = 0 8
global rank 15: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
I am here
I am here
I am here
I am here
I am here
I am here
I am here
I am here
I am here
I am here
I am here
MUST: Model 141.586432 M TFLOPS: 4.860530882925109

--> cuda max reserved memory = 49.9062
--> max reserved percentage = 78.0 %

--> cuda max memory allocated = 38.4727
--> max allocated percentage = 60.13 %

--> peak active memory = 38.4727
--> peak active memory 60.13 %

cudaMalloc retries = 0
cuda OOM = 0

HERE1 Namespace(arch='orbit_token', batch_size=2, channels=256, depth=32, embed_dim=6144, fa2=True, fsdp_size=1, imagex=128, imagey=256, max_epochs=1, num_heads=32, real=False, seq_par_size=1, simple_ddp_size=1, tensor_par_size=16, yaml_config='configs/ERA5-100million-91variables.yaml') 49.9062 4.860530882925109 tensor(141586432)
I am here
I am here
I am here
I am here
I am here
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:35:45,408] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:35:45,408] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:35:45,408] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:35:45,408] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:35:45,408] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:35:45,408] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:35:45,408] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:35:45,408] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:35:45,410] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:35:45,410] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:35:45,410] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:35:45,410] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:35:45,410] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:35:45,410] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:35:45,410] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:35:45,410] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
HERE0 Namespace(arch='orbit_token_agg', batch_size=2, channels=256, depth=32, embed_dim=6144, fa2=True, fsdp_size=1, imagex=128, imagey=256, max_epochs=1, num_heads=32, real=False, seq_par_size=1, simple_ddp_size=1, tensor_par_size=16, yaml_config='configs/ERA5-100million-91variables.yaml')
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
config_path  configs/ERA5-100million-91variables.yaml
rank 2 Before initialize parallelism groups
rank 4 Before initialize parallelism groups
rank 7 Before initialize parallelism groups
rank 1 Before initialize parallelism groups
rank 10 Before initialize parallelism groups
rank 13 Before initialize parallelism groups
rank 15 Before initialize parallelism groups
rank 5 Before initialize parallelism groups
rank 6 Before initialize parallelism groups
rank 9 Before initialize parallelism groups
rank 14 Before initialize parallelism groups
rank 8 Before initialize parallelism groups
rank 7 After initialize parallelism groups
rank 1 After initialize parallelism groups
rank 2 After initialize parallelism groups
rank 4 After initialize parallelism groups
rank 13 After initialize parallelism groups
rank 15 After initialize parallelism groups
rank 5 After initialize parallelism groups
rank 10 After initialize parallelism groups
rank 6 After initialize parallelism groups
rank 9 After initialize parallelism groups
rank 11 Before initialize parallelism groups
rank 14 After initialize parallelism groups
{'seed_everything': 42, 'trainer': {'checkpoint_path': '/lustre/orion/stf218/world-shared/atsaris/tmp/tmp_checkpoints', 'checkpoint_filename': 'multi_last', 'resume_from_checkpoint': False}, 'parallelism': {'cpu_offloading': False}, 'model': {'lr': 2e-05, 'beta_1': 0.9, 'beta_2': 0.95, 'weight_decay': '1e-5', 'warmup_steps': 1000, 'max_steps': 20000, 'warmup_start_lr': '1e-8', 'eta_min': '1e-8', 'net': {'class_path': 'climax.arch.ClimaX', 'init_args': {'default_vars': ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_70', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000'], 'patch_size': 4, 'decoder_depth': 2, 'mlp_ratio': 4, 'drop_path': 0.1, 'drop_rate': 0.1, 'aggregated_variables': 1}}}, 'data': {'dict_root_dirs': {'mpi-esm': '/lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0'}, 'dict_start_idx': {'mpi-esm': 0}, 'dict_end_idx': {'mpi-esm': 1}, 'dict_in_variables': {'mpi-esm': ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000']}, 'dict_out_variables': {'mpi-esm': ['geopotential_500', 'temperature_850', '2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind']}, 'dict_max_predict_ranges': {'mpi-esm': 720}, 'dict_random_lead_time': {'mpi-esm': True}, 'dict_hrs_each_step': {'mpi-esm': 1}, 'dict_buffer_sizes': {'mpi-esm': 1}, 'num_workers': 1, 'pin_memory': False}}
rank 12 Before initialize parallelism groups
rank 3 Before initialize parallelism groups
rank 8 After initialize parallelism groups
rank 11 After initialize parallelism groups
max_epochs 1 data_par_size 1 fsdp_size 1 simple_ddp_size 1 tensor_par_size 16 seq_par_size 1 cpu_offloading False
lr  2e-05 beta_1  0.9 beta_2 0.95 weight_decay 1e-05 class_path climax.arch.ClimaX default_vars ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_70', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000']
img_size_x 128 img_size_y 256 patch_size 4 emb_dim 6144 depth 32 decoder_depth 2 num_heads 32 mlp_ratio 4 drop_path 0.1 drop_rate 0.1 aggregated_variables 1
dict_root_dirs {'mpi-esm': '/lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0'} dict_start_idx {'mpi-esm': 0} dict_end_idx {'mpi-esm': 1} batch_size 2 num_workers 1
warmup_steps 1000 max_steps 20000 warmup_start_lr 1e-08 eta_min 1e-08
checkpoint_path /lustre/orion/stf218/world-shared/atsaris/tmp/tmp_checkpoints checkpoint_filename multi_last resume_from_checkpoint False
rank 0 Before initialize parallelism groups
rank 3 After initialize parallelism groups
rank 12 After initialize parallelism groups
rank 0 After initialize parallelism groups
rank 0 After initialize model
rank 0 Before the second dist.barrier()
rank 8 After initialize model
rank 8 Before the second dist.barrier()
rank 12 After initialize model
rank 12 Before the second dist.barrier()
rank 14 After initialize model
rank 14 Before the second dist.barrier()
rank 9 After initialize model
rank 9 Before the second dist.barrier()
rank 13 After initialize model
rank 13 Before the second dist.barrier()
rank 11 After initialize model
rank 11 Before the second dist.barrier()
rank 6 After initialize model
rank 6 Before the second dist.barrier()
rank 15 After initialize model
rank 15 Before the second dist.barrier()
rank 10 After initialize model
rank 10 Before the second dist.barrier()
rank 2 After initialize model
rank 2 Before the second dist.barrier()
rank 4 After initialize model
rank 4 Before the second dist.barrier()
rank 3 After initialize model
rank 3 Before the second dist.barrier()
rank 7 After initialize model
rank 1 After initialize model
rank 7 Before the second dist.barrier()
rank 1 Before the second dist.barrier()
rank 5 After initialize model
rank 5 Before the second dist.barrier()
rank 8 After the second dist.barrier()
rank 10 After the second dist.barrier()
rank 12 After the second dist.barrier()
rank 13 After the second dist.barrier()
rank 14 After the second dist.barrier()
rank 9 After the second dist.barrier()
rank 11 After the second dist.barrier()
rank 15 After the second dist.barrier()
rank 0 After the second dist.barrier()
rank 1 After the second dist.barrier()
parameter name  var_embed  requires_gradient  True size torch.Size([1, 256, 6144])
rank 5 After the second dist.barrier()
rank 4 After the second dist.barrier()
parameter name  var_query  requires_gradient  True size torch.Size([1, 1, 6144])
parameter name  pos_embed  requires_gradient  True size torch.Size([1, 2048, 6144])
rank 6 After the second dist.barrier()
rank 7 After the second dist.barrier()
parameter name  token_embeds.0.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.0.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.1.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.1.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.2.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.2.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.3.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.3.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.4.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
rank 3 After the second dist.barrier()
parameter name  token_embeds.4.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.5.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.5.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.6.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.6.proj.bias  requires_gradient  True size torch.Size([6144])
rank 2 After the second dist.barrier()
parameter name  token_embeds.7.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.7.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.8.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.8.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.9.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.9.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.10.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.10.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.11.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.11.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.12.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.12.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.13.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.13.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.14.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.14.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.15.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.15.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.16.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.16.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.17.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.17.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.18.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.18.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.19.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.19.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.20.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.20.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.21.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.21.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.22.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.22.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.23.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.23.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.24.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.24.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.25.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.25.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.26.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.26.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.27.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.27.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.28.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.28.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.29.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.29.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.30.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.30.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.31.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.31.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.32.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.32.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.33.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.33.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.34.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.34.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.35.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.35.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.36.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.36.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.37.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.37.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.38.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.38.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.39.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.39.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.40.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.40.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.41.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.41.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.42.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.42.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.43.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.43.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.44.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.44.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.45.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.45.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.46.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.46.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.47.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.47.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.48.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.48.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.49.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.49.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.50.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.50.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.51.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.51.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.52.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.52.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.53.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.53.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.54.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.54.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.55.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.55.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.56.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.56.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.57.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.57.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.58.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.58.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.59.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.59.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.60.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.60.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.61.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.61.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.62.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.62.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.63.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.63.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.64.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.64.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.65.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.65.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.66.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.66.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.67.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.67.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.68.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.68.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.69.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.69.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.70.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.70.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.71.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.71.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.72.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.72.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.73.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.73.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.74.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.74.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.75.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.75.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.76.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.76.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.77.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.77.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.78.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.78.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.79.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.79.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.80.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.80.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.81.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.81.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.82.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.82.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.83.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.83.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.84.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.84.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.85.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.85.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.86.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.86.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.87.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.87.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.88.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.88.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.89.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.89.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.90.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.90.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.91.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.91.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.92.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.92.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.93.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.93.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.94.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.94.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.95.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.95.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.96.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.96.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.97.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.97.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.98.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.98.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.99.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.99.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.100.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.100.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.101.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.101.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.102.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.102.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.103.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.103.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.104.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.104.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.105.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.105.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.106.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.106.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.107.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.107.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.108.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.108.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.109.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.109.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.110.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.110.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.111.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.111.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.112.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.112.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.113.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.113.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.114.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.114.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.115.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.115.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.116.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.116.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.117.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.117.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.118.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.118.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.119.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.119.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.120.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.120.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.121.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.121.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.122.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.122.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.123.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.123.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.124.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.124.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.125.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.125.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.126.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.126.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.127.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.127.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.128.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.128.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.129.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.129.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.130.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.130.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.131.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.131.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.132.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.132.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.133.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.133.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.134.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.134.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.135.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.135.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.136.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.136.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.137.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.137.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.138.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.138.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.139.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.139.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.140.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.140.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.141.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.141.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.142.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.142.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.143.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.143.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.144.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.144.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.145.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.145.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.146.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.146.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.147.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.147.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.148.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.148.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.149.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.149.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.150.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.150.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.151.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.151.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.152.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.152.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.153.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.153.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.154.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.154.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.155.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.155.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.156.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.156.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.157.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.157.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.158.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.158.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.159.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.159.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.160.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.160.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.161.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.161.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.162.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.162.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.163.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.163.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.164.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.164.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.165.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.165.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.166.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.166.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.167.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.167.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.168.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.168.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.169.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.169.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.170.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.170.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.171.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.171.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.172.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.172.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.173.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.173.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.174.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.174.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.175.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.175.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.176.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.176.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.177.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.177.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.178.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.178.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.179.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.179.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.180.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.180.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.181.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.181.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.182.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.182.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.183.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.183.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.184.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.184.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.185.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.185.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.186.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.186.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.187.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.187.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.188.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.188.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.189.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.189.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.190.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.190.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.191.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.191.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.192.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.192.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.193.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.193.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.194.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.194.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.195.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.195.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.196.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.196.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.197.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.197.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.198.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.198.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.199.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.199.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.200.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.200.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.201.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.201.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.202.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.202.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.203.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.203.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.204.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.204.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.205.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.205.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.206.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.206.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.207.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.207.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.208.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.208.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.209.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.209.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.210.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.210.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.211.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.211.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.212.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.212.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.213.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.213.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.214.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.214.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.215.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.215.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.216.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.216.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.217.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.217.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.218.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.218.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.219.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.219.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.220.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.220.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.221.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.221.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.222.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.222.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.223.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.223.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.224.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.224.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.225.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.225.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.226.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.226.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.227.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.227.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.228.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.228.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.229.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.229.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.230.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.230.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.231.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.231.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.232.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.232.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.233.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.233.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.234.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.234.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.235.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.235.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.236.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.236.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.237.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.237.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.238.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.238.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.239.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.239.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.240.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.240.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.241.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.241.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.242.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.242.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.243.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.243.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.244.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.244.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.245.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.245.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.246.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.246.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.247.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.247.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.248.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.248.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.249.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.249.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.250.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.250.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.251.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.251.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.252.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.252.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.253.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.253.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.254.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.254.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.255.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.255.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  var_agg.q.weight  requires_gradient  True size torch.Size([384, 6144])
parameter name  var_agg.kv.weight  requires_gradient  True size torch.Size([768, 6144])
parameter name  var_agg.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  var_agg.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  lead_time_embed.weight  requires_gradient  True size torch.Size([6144, 1])
parameter name  lead_time_embed.bias  requires_gradient  True size torch.Size([6144])
parameter name  head.0.weight  requires_gradient  True size torch.Size([6144, 6144])
parameter name  head.0.bias  requires_gradient  True size torch.Size([6144])
parameter name  head.2.weight  requires_gradient  True size torch.Size([6144, 6144])
parameter name  head.2.bias  requires_gradient  True size torch.Size([6144])
parameter name  head.5.weight  requires_gradient  True size torch.Size([4096, 6144])
parameter name  head.5.bias  requires_gradient  True size torch.Size([4096])
total_params before FSDP tensor(292685824) params_per_gpu tensor(151035904)
total_params after FSDP FullyShardedDataParallel(
  (_fsdp_wrapped_module): ClimaX(
    (token_embeds): ModuleList(
      (0-255): 256 x PatchEmbed(
        (proj): Conv2d(1, 6144, kernel_size=(4, 4), stride=(4, 4))
        (norm): Identity()
      )
    )
    (var_agg): FullyShardedDataParallel(
      (_fsdp_wrapped_module): CheckpointWrapper(
        (_checkpoint_wrapped_module): VariableMapping_Attention(
          (q): Linear(in_features=6144, out_features=384, bias=False)
          (kv): Linear(in_features=6144, out_features=768, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=6144, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (lead_time_embed): Linear(in_features=1, out_features=6144, bias=True)
    (pos_drop): Dropout(p=0.1, inplace=False)
    (head): Sequential(
      (0): Linear(in_features=6144, out_features=6144, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=6144, out_features=6144, bias=True)
      (3): GELU(approximate='none')
      (4): Pred_Rearrange()
      (5): Linear(in_features=6144, out_features=4096, bias=True)
    )
  )
)
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
rank  0 lister_train reset: mpi-esm 9 9 9
rank  0 lister_test reset: mpi-esm 9 9 9
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
rank  15 lister_train reset: mpi-esm 9 9 9
rank  15 lister_test reset: mpi-esm 9 9 9
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
memory stats reset, ready to track
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
epoch-train:  0 batch_idx 0 world_rank 0  loss  13.0625
epoch-train:  0 batch_idx 1 world_rank 0  loss  13.0
epoch-train:  0 batch_idx 2 world_rank 0  loss  13.0
epoch-train:  0 batch_idx 3 world_rank 0  loss  12.9375
[2025-03-16 18:36:19,494] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:36:19,495] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:36:19,495] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:36:19,496] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:36:19,496] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:36:19,496] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:36:19,496] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:36:19,496] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:36:19,497] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:36:19,498] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:36:19,499] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:36:19,499] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:36:19,499] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:36:19,499] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:36:19,500] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:36:19,502] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:36:20,334] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:36:20,335] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:36:20,335] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:36:20,335] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:36:20,335] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:36:20,335] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:36:20,335] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:36:20,335] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:36:20,335] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:36:20,336] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:36:20,336] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:36:20,336] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:36:20,337] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:36:20,338] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:36:20,338] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:36:20,339] [INFO] [profiler.py:226:end_profile] Flops profiler finished
global rank 13: ddp rank 0 iter_start,iter_end = 0 8
global rank 13: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 12: ddp rank 0 iter_start,iter_end = 0 8
global rank 12: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 5: ddp rank 0 iter_start,iter_end = 0 8
global rank 5: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 7: ddp rank 0 iter_start,iter_end = 0 8
global rank 7: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 3: ddp rank 0 iter_start,iter_end = 0 8
global rank 3: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 2: ddp rank 0 iter_start,iter_end = 0 8
global rank 2: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 15: ddp rank 0 iter_start,iter_end = 0 8
global rank 15: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 10: ddp rank 0 iter_start,iter_end = 0 8
global rank 10: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 9: ddp rank 0 iter_start,iter_end = 0 8
global rank 9: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 8: ddp rank 0 iter_start,iter_end = 0 8
global rank 8: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 14: ddp rank 0 iter_start,iter_end = 0 8
global rank 14: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 4: ddp rank 0 iter_start,iter_end = 0 8
global rank 4: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 11: ddp rank 0 iter_start,iter_end = 0 8
global rank 11: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 6: ddp rank 0 iter_start,iter_end = 0 8
global rank 6: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 1: ddp rank 0 iter_start,iter_end = 0 8
global rank 1: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 0: ddp rank 0 iter_start,iter_end = 0 8
global rank 0: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
MUST: Model 151.035904 M TFLOPS: 13.247322175514466

--> cuda max reserved memory = 41.6895
--> max reserved percentage = 65.16 %

--> cuda max memory allocated = 40.9386
--> max allocated percentage = 63.98 %

--> peak active memory = 40.9386
--> peak active memory 63.98 %

cudaMalloc retries = 0
cuda OOM = 0

HERE1 Namespace(arch='orbit_token_agg', batch_size=2, channels=256, depth=32, embed_dim=6144, fa2=True, fsdp_size=1, imagex=128, imagey=256, max_epochs=1, num_heads=32, real=False, seq_par_size=1, simple_ddp_size=1, tensor_par_size=16, yaml_config='configs/ERA5-100million-91variables.yaml') 41.6895 13.247322175514466 tensor(292685824)
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:36:35,145] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:36:35,145] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:36:35,145] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:36:35,145] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:36:35,145] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:36:35,145] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:36:35,145] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:36:35,145] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:36:35,147] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:36:35,147] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:36:35,147] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:36:35,147] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:36:35,147] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:36:35,147] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:36:35,147] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:36:35,147] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
HERE0 Namespace(arch='orbit_hier', batch_size=2, channels=256, depth=32, embed_dim=6144, fa2=True, fsdp_size=1, imagex=128, imagey=256, max_epochs=1, num_heads=32, real=False, seq_par_size=1, simple_ddp_size=1, tensor_par_size=16, yaml_config='configs/ERA5-100million-91variables.yaml')
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
config_path  configs/ERA5-100million-91variables.yaml
rank 11 Before initialize parallelism groups
rank 14 Before initialize parallelism groups
rank 15 Before initialize parallelism groups
rank 12 Before initialize parallelism groups
rank 13 Before initialize parallelism groups
rank 9 Before initialize parallelism groups
rank 2 Before initialize parallelism groups
rank 3 Before initialize parallelism groups
rank 5 Before initialize parallelism groups
rank 7 Before initialize parallelism groups
rank 8 Before initialize parallelism groups
{'seed_everything': 42, 'trainer': {'checkpoint_path': '/lustre/orion/stf218/world-shared/atsaris/tmp/tmp_checkpoints', 'checkpoint_filename': 'multi_last', 'resume_from_checkpoint': False}, 'parallelism': {'cpu_offloading': False}, 'model': {'lr': 2e-05, 'beta_1': 0.9, 'beta_2': 0.95, 'weight_decay': '1e-5', 'warmup_steps': 1000, 'max_steps': 20000, 'warmup_start_lr': '1e-8', 'eta_min': '1e-8', 'net': {'class_path': 'climax.arch.ClimaX', 'init_args': {'default_vars': ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_70', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000'], 'patch_size': 4, 'decoder_depth': 2, 'mlp_ratio': 4, 'drop_path': 0.1, 'drop_rate': 0.1, 'aggregated_variables': 1}}}, 'data': {'dict_root_dirs': {'mpi-esm': '/lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0'}, 'dict_start_idx': {'mpi-esm': 0}, 'dict_end_idx': {'mpi-esm': 1}, 'dict_in_variables': {'mpi-esm': ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000']}, 'dict_out_variables': {'mpi-esm': ['geopotential_500', 'temperature_850', '2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind']}, 'dict_max_predict_ranges': {'mpi-esm': 720}, 'dict_random_lead_time': {'mpi-esm': True}, 'dict_hrs_each_step': {'mpi-esm': 1}, 'dict_buffer_sizes': {'mpi-esm': 1}, 'num_workers': 1, 'pin_memory': False}}
rank 4 Before initialize parallelism groups
rank 14 After initialize parallelism groups
rank 15 After initialize parallelism groups
rank 12 After initialize parallelism groups
rank 11 After initialize parallelism groups
rank 9 After initialize parallelism groups
rank 13 After initialize parallelism groups
rank 10 Before initialize parallelism groups
rank 1 Before initialize parallelism groups
rank 6 Before initialize parallelism groups
max_epochs 1 data_par_size 1 fsdp_size 1 simple_ddp_size 1 tensor_par_size 16 seq_par_size 1 cpu_offloading False
lr  2e-05 beta_1  0.9 beta_2 0.95 weight_decay 1e-05 class_path climax.arch.ClimaX default_vars ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_70', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000']
img_size_x 128 img_size_y 256 patch_size 4 emb_dim 6144 depth 32 decoder_depth 2 num_heads 32 mlp_ratio 4 drop_path 0.1 drop_rate 0.1 aggregated_variables 1
dict_root_dirs {'mpi-esm': '/lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0'} dict_start_idx {'mpi-esm': 0} dict_end_idx {'mpi-esm': 1} batch_size 2 num_workers 1
warmup_steps 1000 max_steps 20000 warmup_start_lr 1e-08 eta_min 1e-08
checkpoint_path /lustre/orion/stf218/world-shared/atsaris/tmp/tmp_checkpoints checkpoint_filename multi_last resume_from_checkpoint False
rank 0 Before initialize parallelism groups
rank 5 After initialize parallelism groups
rank 7 After initialize parallelism groups
rank 3 After initialize parallelism groups
rank 8 After initialize parallelism groups
rank 4 After initialize parallelism groups
rank 2 After initialize parallelism groups
rank 10 After initialize parallelism groups
rank 6 After initialize parallelism groups
rank 1 After initialize parallelism groups
rank 0 After initialize parallelism groups
rank 13 After initialize model
rank 13 Before the second dist.barrier()
rank 4 After initialize model
rank 4 Before the second dist.barrier()
rank 6 After initialize model
rank 6 Before the second dist.barrier()
rank 0 After initialize model
rank 0 Before the second dist.barrier()
rank 3 After initialize model
rank 3 Before the second dist.barrier()
rank 9 After initialize model
rank 9 Before the second dist.barrier()
rank 14 After initialize model
rank 14 Before the second dist.barrier()
rank 1 After initialize model
rank 1 Before the second dist.barrier()
rank 8 After initialize model
rank 8 Before the second dist.barrier()
rank 12 After initialize model
rank 12 Before the second dist.barrier()
rank 10 After initialize model
rank 10 Before the second dist.barrier()
rank 15 After initialize model
rank 15 Before the second dist.barrier()
rank 11 After initialize model
rank 11 Before the second dist.barrier()
rank 7 After initialize model
rank 7 Before the second dist.barrier()
rank 5 After initialize model
rank 5 Before the second dist.barrier()
rank 2 After initialize model
rank 2 Before the second dist.barrier()
rank 8 After the second dist.barrier()
rank 9 After the second dist.barrier()
rank 1 After the second dist.barrier()
rank 10 After the second dist.barrier()
rank 11 After the second dist.barrier()
rank 14 After the second dist.barrier()
rank 15 After the second dist.barrier()
rank 13 After the second dist.barrier()
rank 4 After the second dist.barrier()
rank 12 After the second dist.barrier()
rank 6 After the second dist.barrier()
rank 0 After the second dist.barrier()
rank 7 After the second dist.barrier()
rank 2 After the second dist.barrier()
rank 5 After the second dist.barrier()
parameter name  var_embed  requires_gradient  True size torch.Size([1, 256, 6144])
rank 3 After the second dist.barrier()
parameter name  var_query  requires_gradient  True size torch.Size([1, 1, 6144])
parameter name  var_query_per_rank  requires_gradient  True size torch.Size([1, 1, 6144])
parameter name  pos_embed  requires_gradient  True size torch.Size([1, 2048, 6144])
parameter name  token_embeds.0.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.0.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.1.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.1.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.2.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.2.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.3.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.3.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.4.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.4.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.5.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.5.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.6.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.6.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.7.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.7.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.8.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.8.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.9.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.9.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.10.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.10.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.11.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.11.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.12.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.12.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.13.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.13.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.14.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.14.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.15.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.15.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.16.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.16.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.17.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.17.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.18.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.18.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.19.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.19.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.20.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.20.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.21.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.21.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.22.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.22.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.23.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.23.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.24.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.24.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.25.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.25.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.26.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.26.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.27.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.27.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.28.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.28.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.29.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.29.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.30.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.30.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.31.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.31.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.32.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.32.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.33.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.33.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.34.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.34.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.35.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.35.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.36.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.36.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.37.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.37.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.38.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.38.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.39.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.39.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.40.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.40.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.41.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.41.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.42.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.42.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.43.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.43.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.44.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.44.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.45.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.45.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.46.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.46.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.47.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.47.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.48.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.48.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.49.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.49.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.50.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.50.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.51.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.51.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.52.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.52.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.53.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.53.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.54.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.54.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.55.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.55.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.56.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.56.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.57.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.57.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.58.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.58.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.59.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.59.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.60.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.60.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.61.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.61.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.62.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.62.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.63.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.63.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.64.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.64.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.65.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.65.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.66.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.66.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.67.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.67.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.68.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.68.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.69.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.69.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.70.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.70.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.71.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.71.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.72.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.72.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.73.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.73.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.74.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.74.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.75.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.75.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.76.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.76.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.77.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.77.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.78.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.78.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.79.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.79.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.80.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.80.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.81.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.81.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.82.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.82.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.83.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.83.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.84.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.84.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.85.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.85.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.86.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.86.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.87.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.87.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.88.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.88.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.89.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.89.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.90.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.90.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.91.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.91.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.92.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.92.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.93.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.93.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.94.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.94.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.95.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.95.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.96.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.96.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.97.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.97.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.98.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.98.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.99.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.99.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.100.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.100.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.101.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.101.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.102.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.102.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.103.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.103.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.104.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.104.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.105.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.105.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.106.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.106.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.107.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.107.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.108.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.108.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.109.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.109.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.110.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.110.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.111.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.111.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.112.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.112.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.113.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.113.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.114.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.114.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.115.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.115.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.116.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.116.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.117.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.117.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.118.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.118.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.119.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.119.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.120.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.120.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.121.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.121.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.122.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.122.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.123.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.123.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.124.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.124.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.125.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.125.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.126.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.126.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.127.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.127.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.128.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.128.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.129.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.129.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.130.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.130.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.131.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.131.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.132.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.132.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.133.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.133.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.134.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.134.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.135.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.135.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.136.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.136.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.137.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.137.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.138.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.138.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.139.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.139.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.140.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.140.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.141.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.141.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.142.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.142.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.143.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.143.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.144.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.144.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.145.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.145.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.146.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.146.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.147.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.147.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.148.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.148.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.149.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.149.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.150.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.150.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.151.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.151.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.152.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.152.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.153.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.153.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.154.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.154.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.155.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.155.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.156.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.156.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.157.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.157.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.158.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.158.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.159.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.159.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.160.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.160.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.161.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.161.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.162.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.162.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.163.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.163.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.164.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.164.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.165.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.165.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.166.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.166.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.167.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.167.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.168.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.168.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.169.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.169.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.170.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.170.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.171.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.171.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.172.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.172.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.173.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.173.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.174.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.174.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.175.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.175.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.176.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.176.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.177.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.177.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.178.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.178.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.179.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.179.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.180.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.180.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.181.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.181.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.182.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.182.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.183.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.183.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.184.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.184.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.185.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.185.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.186.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.186.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.187.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.187.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.188.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.188.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.189.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.189.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.190.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.190.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.191.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.191.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.192.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.192.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.193.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.193.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.194.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.194.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.195.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.195.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.196.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.196.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.197.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.197.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.198.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.198.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.199.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.199.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.200.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.200.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.201.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.201.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.202.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.202.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.203.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.203.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.204.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.204.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.205.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.205.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.206.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.206.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.207.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.207.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.208.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.208.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.209.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.209.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.210.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.210.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.211.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.211.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.212.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.212.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.213.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.213.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.214.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.214.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.215.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.215.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.216.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.216.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.217.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.217.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.218.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.218.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.219.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.219.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.220.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.220.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.221.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.221.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.222.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.222.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.223.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.223.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.224.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.224.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.225.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.225.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.226.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.226.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.227.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.227.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.228.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.228.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.229.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.229.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.230.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.230.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.231.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.231.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.232.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.232.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.233.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.233.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.234.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.234.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.235.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.235.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.236.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.236.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.237.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.237.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.238.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.238.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.239.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.239.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.240.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.240.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.241.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.241.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.242.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.242.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.243.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.243.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.244.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.244.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.245.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.245.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.246.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.246.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.247.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.247.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.248.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.248.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.249.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.249.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.250.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.250.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.251.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.251.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.252.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.252.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.253.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.253.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.254.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.254.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.255.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.255.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  var_agg.q.weight  requires_gradient  True size torch.Size([384, 6144])
parameter name  var_agg.kv.weight  requires_gradient  True size torch.Size([768, 6144])
parameter name  var_agg.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  var_agg.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  var_agg_per_rank.q.weight  requires_gradient  True size torch.Size([6144, 6144])
parameter name  var_agg_per_rank.kv.weight  requires_gradient  True size torch.Size([12288, 6144])
parameter name  var_agg_per_rank.proj.weight  requires_gradient  True size torch.Size([6144, 6144])
parameter name  var_agg_per_rank.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  lead_time_embed.weight  requires_gradient  True size torch.Size([6144, 1])
parameter name  lead_time_embed.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.0.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.0.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.0.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.0.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.0.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.0.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.0.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.0.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.0.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.0.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.0.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.0.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.1.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.1.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.1.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.1.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.1.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.1.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.1.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.1.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.1.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.1.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.1.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.1.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.2.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.2.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.2.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.2.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.2.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.2.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.2.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.2.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.2.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.2.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.2.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.2.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.3.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.3.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.3.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.3.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.3.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.3.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.3.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.3.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.3.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.3.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.3.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.3.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.4.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.4.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.4.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.4.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.4.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.4.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.4.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.4.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.4.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.4.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.4.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.4.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.5.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.5.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.5.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.5.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.5.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.5.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.5.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.5.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.5.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.5.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.5.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.5.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.6.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.6.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.6.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.6.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.6.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.6.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.6.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.6.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.6.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.6.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.6.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.6.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.7.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.7.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.7.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.7.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.7.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.7.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.7.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.7.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.7.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.7.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.7.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.7.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.8.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.8.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.8.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.8.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.8.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.8.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.8.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.8.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.8.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.8.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.8.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.8.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.9.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.9.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.9.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.9.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.9.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.9.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.9.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.9.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.9.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.9.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.9.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.9.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.10.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.10.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.10.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.10.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.10.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.10.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.10.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.10.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.10.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.10.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.10.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.10.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.11.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.11.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.11.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.11.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.11.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.11.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.11.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.11.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.11.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.11.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.11.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.11.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.12.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.12.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.12.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.12.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.12.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.12.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.12.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.12.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.12.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.12.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.12.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.12.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.13.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.13.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.13.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.13.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.13.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.13.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.13.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.13.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.13.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.13.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.13.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.13.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.14.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.14.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.14.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.14.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.14.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.14.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.14.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.14.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.14.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.14.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.14.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.14.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.15.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.15.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.15.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.15.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.15.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.15.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.15.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.15.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.15.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.15.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.15.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.15.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.16.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.16.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.16.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.16.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.16.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.16.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.16.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.16.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.16.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.16.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.16.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.16.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.17.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.17.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.17.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.17.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.17.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.17.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.17.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.17.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.17.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.17.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.17.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.17.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.18.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.18.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.18.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.18.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.18.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.18.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.18.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.18.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.18.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.18.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.18.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.18.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.19.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.19.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.19.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.19.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.19.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.19.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.19.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.19.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.19.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.19.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.19.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.19.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.20.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.20.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.20.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.20.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.20.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.20.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.20.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.20.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.20.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.20.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.20.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.20.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.21.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.21.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.21.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.21.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.21.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.21.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.21.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.21.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.21.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.21.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.21.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.21.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.22.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.22.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.22.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.22.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.22.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.22.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.22.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.22.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.22.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.22.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.22.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.22.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.23.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.23.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.23.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.23.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.23.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.23.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.23.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.23.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.23.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.23.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.23.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.23.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.24.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.24.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.24.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.24.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.24.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.24.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.24.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.24.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.24.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.24.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.24.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.24.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.25.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.25.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.25.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.25.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.25.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.25.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.25.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.25.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.25.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.25.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.25.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.25.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.26.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.26.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.26.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.26.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.26.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.26.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.26.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.26.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.26.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.26.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.26.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.26.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.27.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.27.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.27.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.27.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.27.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.27.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.27.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.27.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.27.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.27.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.27.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.27.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.28.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.28.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.28.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.28.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.28.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.28.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.28.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.28.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.28.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.28.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.28.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.28.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.29.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.29.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.29.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.29.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.29.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.29.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.29.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.29.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.29.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.29.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.29.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.29.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.30.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.30.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.30.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.30.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.30.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.30.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.30.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.30.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.30.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.30.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.30.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.30.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.31.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.31.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.31.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.31.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.31.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.31.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.31.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.31.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.31.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.31.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.31.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.31.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  norm.weight  requires_gradient  True size torch.Size([6144])
parameter name  norm.bias  requires_gradient  True size torch.Size([6144])
parameter name  head.0.weight  requires_gradient  True size torch.Size([6144, 6144])
parameter name  head.0.bias  requires_gradient  True size torch.Size([6144])
parameter name  head.2.weight  requires_gradient  True size torch.Size([6144, 6144])
parameter name  head.2.bias  requires_gradient  True size torch.Size([6144])
parameter name  head.5.weight  requires_gradient  True size torch.Size([4096, 6144])
parameter name  head.5.bias  requires_gradient  True size torch.Size([4096])
total_params before FSDP tensor(17212690432) params_per_gpu tensor(1209290752)
total_params after FSDP FullyShardedDataParallel(
  (_fsdp_wrapped_module): ClimaX(
    (token_embeds): ModuleList(
      (0-255): 256 x PatchEmbed(
        (proj): Conv2d(1, 6144, kernel_size=(4, 4), stride=(4, 4))
        (norm): Identity()
      )
    )
    (var_agg): FullyShardedDataParallel(
      (_fsdp_wrapped_module): CheckpointWrapper(
        (_checkpoint_wrapped_module): VariableMapping_Attention(
          (q): Linear(in_features=6144, out_features=384, bias=False)
          (kv): Linear(in_features=6144, out_features=768, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=6144, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (var_agg_per_rank): CustomCrossAttention(
      (q): Linear(in_features=6144, out_features=6144, bias=False)
      (kv): Linear(in_features=6144, out_features=12288, bias=False)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=6144, out_features=6144, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (lead_time_embed): Linear(in_features=1, out_features=6144, bias=True)
    (pos_drop): Dropout(p=0.1, inplace=False)
    (blocks): ModuleList(
      (0-31): 32 x FullyShardedDataParallel(
        (_fsdp_wrapped_module): CheckpointWrapper(
          (_checkpoint_wrapped_module): Block(
            (norm1): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=6144, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=6144, bias=True)
              (proj_drop): Dropout(p=0.1, inplace=False)
            )
            (ls1): Identity()
            (norm2): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=6144, out_features=1536, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.1, inplace=False)
              (fc2): Linear(in_features=1536, out_features=6144, bias=True)
            )
            (ls2): Identity()
          )
        )
      )
    )
    (norm): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)
    (head): Sequential(
      (0): Linear(in_features=6144, out_features=6144, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=6144, out_features=6144, bias=True)
      (3): GELU(approximate='none')
      (4): Pred_Rearrange()
      (5): Linear(in_features=6144, out_features=4096, bias=True)
    )
  )
)
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
rank  15 lister_train reset: mpi-esm 9 9 9
rank  15 lister_test reset: mpi-esm 9 9 9
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
rank  0 lister_train reset: mpi-esm 9 9 9
rank  0 lister_test reset: mpi-esm 9 9 9
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
memory stats reset, ready to track
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
epoch-train:  0 batch_idx 0 world_rank 0  loss  4.28125
epoch-train:  0 batch_idx 1 world_rank 0  loss  nan
epoch-train:  0 batch_idx 2 world_rank 0  loss  nan
[2025-03-16 18:37:21,547] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:37:21,548] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:37:21,549] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:37:21,550] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:37:21,550] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:37:21,551] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:37:21,551] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:37:21,552] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:37:21,553] [INFO] [profiler.py:80:start_profile] Flops profiler started
epoch-train:  0 batch_idx 3 world_rank 0  loss  nan
[2025-03-16 18:37:21,554] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:37:21,556] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:37:21,557] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:37:21,558] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:37:21,558] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:37:21,559] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:37:21,561] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:37:22,505] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:37:22,505] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:37:22,505] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:37:22,505] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:37:22,505] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:37:22,505] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:37:22,505] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:37:22,506] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:37:22,506] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:37:22,506] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:37:22,506] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:37:22,506] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:37:22,507] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:37:22,507] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:37:22,507] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:37:22,508] [INFO] [profiler.py:226:end_profile] Flops profiler finished
global rank 14: ddp rank 0 iter_start,iter_end = 0 8
global rank 14: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 12: ddp rank 0 iter_start,iter_end = 0 8
global rank 12: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 13: ddp rank 0 iter_start,iter_end = 0 8
global rank 13: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 9: ddp rank 0 iter_start,iter_end = 0 8
global rank 9: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 8: ddp rank 0 iter_start,iter_end = 0 8
global rank 8: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 4: ddp rank 0 iter_start,iter_end = 0 8
global rank 4: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 3: ddp rank 0 iter_start,iter_end = 0 8
global rank 3: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 1: ddp rank 0 iter_start,iter_end = 0 8
global rank 1: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 2: ddp rank 0 iter_start,iter_end = 0 8
global rank 2: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 5: ddp rank 0 iter_start,iter_end = 0 8
global rank 5: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 6: ddp rank 0 iter_start,iter_end = 0 8
global rank 6: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 11: ddp rank 0 iter_start,iter_end = 0 8
global rank 11: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 10: ddp rank 0 iter_start,iter_end = 0 8
global rank 10: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 0: ddp rank 0 iter_start,iter_end = 0 8
global rank 0: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 15: ddp rank 0 iter_start,iter_end = 0 8
global rank 15: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 7: ddp rank 0 iter_start,iter_end = 0 8
global rank 7: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
MUST: Model 1209.290752 M TFLOPS: 36.4805289141113

--> cuda max reserved memory = 35.4941
--> max reserved percentage = 55.47 %

--> cuda max memory allocated = 25.1223
--> max allocated percentage = 39.26 %

--> peak active memory = 30.5679
--> peak active memory 47.77 %

cudaMalloc retries = 0
cuda OOM = 0

HERE1 Namespace(arch='orbit_hier', batch_size=2, channels=256, depth=32, embed_dim=6144, fa2=True, fsdp_size=1, imagex=128, imagey=256, max_epochs=1, num_heads=32, real=False, seq_par_size=1, simple_ddp_size=1, tensor_par_size=16, yaml_config='configs/ERA5-100million-91variables.yaml') 35.4941 36.4805289141113 tensor(17212690432)
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:37:37,256] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:37:37,256] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:37:37,256] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:37:37,256] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:37:37,256] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:37:37,256] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:37:37,256] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:37:37,256] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:37:37,261] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:37:37,261] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:37:37,261] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:37:37,261] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:37:37,261] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:37:37,261] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:37:37,261] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-16 18:37:37,261] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
HERE0 Namespace(arch='orbit_linear', batch_size=2, channels=256, depth=32, embed_dim=6144, fa2=True, fsdp_size=1, imagex=128, imagey=256, max_epochs=1, num_heads=32, real=False, seq_par_size=1, simple_ddp_size=1, tensor_par_size=16, yaml_config='configs/ERA5-100million-91variables.yaml')
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
Using dist.init_process_group. world_size  16
config_path  configs/ERA5-100million-91variables.yaml
Using dist.init_process_group. world_size  16
rank 9 Before initialize parallelism groups
rank 13 Before initialize parallelism groups
rank 14 Before initialize parallelism groups
rank 11 Before initialize parallelism groups
rank 12 Before initialize parallelism groups
rank 8 Before initialize parallelism groups
rank 2 Before initialize parallelism groups
rank 6 Before initialize parallelism groups
rank 15 Before initialize parallelism groups
rank 5 Before initialize parallelism groups
rank 1 Before initialize parallelism groups
rank 7 Before initialize parallelism groups
{'seed_everything': 42, 'trainer': {'checkpoint_path': '/lustre/orion/stf218/world-shared/atsaris/tmp/tmp_checkpoints', 'checkpoint_filename': 'multi_last', 'resume_from_checkpoint': False}, 'parallelism': {'cpu_offloading': False}, 'model': {'lr': 2e-05, 'beta_1': 0.9, 'beta_2': 0.95, 'weight_decay': '1e-5', 'warmup_steps': 1000, 'max_steps': 20000, 'warmup_start_lr': '1e-8', 'eta_min': '1e-8', 'net': {'class_path': 'climax.arch.ClimaX', 'init_args': {'default_vars': ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_70', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000'], 'patch_size': 4, 'decoder_depth': 2, 'mlp_ratio': 4, 'drop_path': 0.1, 'drop_rate': 0.1, 'aggregated_variables': 1}}}, 'data': {'dict_root_dirs': {'mpi-esm': '/lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0'}, 'dict_start_idx': {'mpi-esm': 0}, 'dict_end_idx': {'mpi-esm': 1}, 'dict_in_variables': {'mpi-esm': ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000']}, 'dict_out_variables': {'mpi-esm': ['geopotential_500', 'temperature_850', '2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind']}, 'dict_max_predict_ranges': {'mpi-esm': 720}, 'dict_random_lead_time': {'mpi-esm': True}, 'dict_hrs_each_step': {'mpi-esm': 1}, 'dict_buffer_sizes': {'mpi-esm': 1}, 'num_workers': 1, 'pin_memory': False}}
rank 4 Before initialize parallelism groups
rank 9 After initialize parallelism groups
rank 14 After initialize parallelism groups
rank 12 After initialize parallelism groups
rank 11 After initialize parallelism groups
rank 13 After initialize parallelism groups
rank 8 After initialize parallelism groups
rank 3 Before initialize parallelism groups
rank 15 After initialize parallelism groups
max_epochs 1 data_par_size 1 fsdp_size 1 simple_ddp_size 1 tensor_par_size 16 seq_par_size 1 cpu_offloading False
lr  2e-05 beta_1  0.9 beta_2 0.95 weight_decay 1e-05 class_path climax.arch.ClimaX default_vars ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_70', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000']
img_size_x 128 img_size_y 256 patch_size 4 emb_dim 6144 depth 32 decoder_depth 2 num_heads 32 mlp_ratio 4 drop_path 0.1 drop_rate 0.1 aggregated_variables 1
rank 2 After initialize parallelism groups
dict_root_dirs {'mpi-esm': '/lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0'} dict_start_idx {'mpi-esm': 0} dict_end_idx {'mpi-esm': 1} batch_size 2 num_workers 1
warmup_steps 1000 max_steps 20000 warmup_start_lr 1e-08 eta_min 1e-08
checkpoint_path /lustre/orion/stf218/world-shared/atsaris/tmp/tmp_checkpoints checkpoint_filename multi_last resume_from_checkpoint False
rank 0 Before initialize parallelism groups
rank 6 After initialize parallelism groups
rank 5 After initialize parallelism groups
rank 10 Before initialize parallelism groups
rank 7 After initialize parallelism groups
rank 1 After initialize parallelism groups
rank 4 After initialize parallelism groups
rank 3 After initialize parallelism groups
rank 0 After initialize parallelism groups
rank 10 After initialize parallelism groups
rank 4 After initialize model
rank 4 Before the second dist.barrier()
rank 2 After initialize model
rank 2 Before the second dist.barrier()
rank 10 After initialize model
rank 10 Before the second dist.barrier()
rank 14 After initialize model
rank 14 Before the second dist.barrier()
rank 1 After initialize model
rank 1 Before the second dist.barrier()
rank 6 After initialize model
rank 6 Before the second dist.barrier()
rank 3 After initialize model
rank 3 Before the second dist.barrier()
rank 5 After initialize model
rank 5 Before the second dist.barrier()
rank 0 After initialize model
rank 0 Before the second dist.barrier()
rank 7 After initialize model
rank 7 Before the second dist.barrier()
rank 12 After initialize model
rank 12 Before the second dist.barrier()
rank 9 After initialize model
rank 9 Before the second dist.barrier()
rank 8 After initialize model
rank 8 Before the second dist.barrier()
rank 11 After initialize model
rank 11 Before the second dist.barrier()
rank 15 After initialize model
rank 15 Before the second dist.barrier()
rank 13 After initialize model
rank 13 Before the second dist.barrier()
rank 9 After the second dist.barrier()
rank 12 After the second dist.barrier()
rank 1 After the second dist.barrier()
rank 13 After the second dist.barrier()
rank 14 After the second dist.barrier()
rank 15 After the second dist.barrier()
rank 8 After the second dist.barrier()
rank 0 After the second dist.barrier()
rank 10 After the second dist.barrier()
rank 3 After the second dist.barrier()
rank 11 After the second dist.barrier()
rank 6 After the second dist.barrier()
rank 7 After the second dist.barrier()
rank 4 After the second dist.barrier()
rank 5 After the second dist.barrier()
parameter name  var_embed  requires_gradient  True size torch.Size([1, 256, 6144])
rank 2 After the second dist.barrier()
parameter name  var_query  requires_gradient  True size torch.Size([1, 1, 6144])
parameter name  var_query_per_rank  requires_gradient  True size torch.Size([1, 1, 6144])
parameter name  pos_embed  requires_gradient  True size torch.Size([1, 2048, 6144])
parameter name  token_embeds.0.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.0.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.1.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.1.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.2.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.2.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.3.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.3.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.4.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.4.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.5.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.5.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.6.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.6.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.7.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.7.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.8.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.8.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.9.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.9.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.10.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.10.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.11.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.11.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.12.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.12.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.13.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.13.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.14.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.14.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.15.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.15.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.16.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.16.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.17.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.17.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.18.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.18.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.19.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.19.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.20.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.20.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.21.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.21.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.22.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.22.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.23.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.23.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.24.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.24.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.25.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.25.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.26.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.26.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.27.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.27.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.28.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.28.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.29.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.29.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.30.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.30.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.31.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.31.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.32.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.32.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.33.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.33.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.34.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.34.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.35.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.35.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.36.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.36.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.37.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.37.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.38.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.38.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.39.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.39.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.40.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.40.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.41.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.41.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.42.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.42.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.43.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.43.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.44.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.44.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.45.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.45.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.46.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.46.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.47.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.47.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.48.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.48.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.49.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.49.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.50.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.50.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.51.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.51.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.52.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.52.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.53.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.53.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.54.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.54.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.55.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.55.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.56.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.56.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.57.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.57.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.58.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.58.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.59.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.59.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.60.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.60.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.61.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.61.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.62.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.62.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.63.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.63.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.64.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.64.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.65.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.65.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.66.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.66.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.67.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.67.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.68.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.68.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.69.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.69.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.70.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.70.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.71.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.71.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.72.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.72.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.73.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.73.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.74.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.74.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.75.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.75.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.76.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.76.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.77.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.77.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.78.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.78.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.79.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.79.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.80.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.80.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.81.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.81.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.82.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.82.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.83.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.83.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.84.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.84.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.85.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.85.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.86.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.86.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.87.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.87.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.88.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.88.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.89.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.89.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.90.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.90.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.91.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.91.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.92.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.92.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.93.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.93.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.94.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.94.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.95.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.95.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.96.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.96.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.97.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.97.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.98.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.98.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.99.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.99.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.100.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.100.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.101.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.101.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.102.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.102.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.103.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.103.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.104.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.104.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.105.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.105.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.106.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.106.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.107.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.107.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.108.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.108.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.109.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.109.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.110.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.110.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.111.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.111.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.112.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.112.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.113.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.113.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.114.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.114.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.115.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.115.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.116.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.116.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.117.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.117.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.118.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.118.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.119.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.119.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.120.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.120.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.121.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.121.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.122.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.122.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.123.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.123.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.124.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.124.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.125.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.125.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.126.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.126.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.127.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.127.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.128.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.128.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.129.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.129.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.130.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.130.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.131.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.131.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.132.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.132.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.133.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.133.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.134.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.134.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.135.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.135.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.136.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.136.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.137.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.137.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.138.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.138.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.139.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.139.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.140.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.140.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.141.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.141.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.142.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.142.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.143.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.143.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.144.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.144.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.145.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.145.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.146.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.146.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.147.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.147.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.148.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.148.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.149.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.149.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.150.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.150.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.151.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.151.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.152.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.152.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.153.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.153.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.154.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.154.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.155.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.155.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.156.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.156.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.157.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.157.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.158.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.158.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.159.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.159.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.160.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.160.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.161.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.161.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.162.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.162.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.163.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.163.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.164.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.164.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.165.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.165.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.166.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.166.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.167.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.167.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.168.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.168.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.169.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.169.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.170.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.170.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.171.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.171.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.172.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.172.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.173.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.173.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.174.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.174.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.175.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.175.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.176.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.176.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.177.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.177.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.178.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.178.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.179.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.179.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.180.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.180.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.181.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.181.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.182.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.182.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.183.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.183.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.184.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.184.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.185.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.185.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.186.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.186.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.187.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.187.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.188.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.188.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.189.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.189.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.190.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.190.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.191.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.191.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.192.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.192.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.193.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.193.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.194.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.194.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.195.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.195.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.196.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.196.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.197.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.197.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.198.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.198.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.199.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.199.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.200.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.200.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.201.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.201.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.202.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.202.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.203.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.203.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.204.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.204.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.205.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.205.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.206.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.206.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.207.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.207.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.208.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.208.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.209.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.209.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.210.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.210.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.211.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.211.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.212.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.212.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.213.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.213.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.214.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.214.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.215.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.215.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.216.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.216.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.217.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.217.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.218.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.218.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.219.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.219.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.220.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.220.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.221.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.221.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.222.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.222.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.223.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.223.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.224.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.224.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.225.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.225.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.226.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.226.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.227.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.227.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.228.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.228.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.229.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.229.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.230.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.230.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.231.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.231.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.232.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.232.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.233.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.233.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.234.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.234.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.235.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.235.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.236.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.236.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.237.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.237.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.238.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.238.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.239.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.239.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.240.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.240.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.241.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.241.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.242.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.242.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.243.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.243.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.244.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.244.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.245.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.245.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.246.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.246.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.247.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.247.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.248.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.248.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.249.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.249.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.250.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.250.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.251.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.251.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.252.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.252.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.253.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.253.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.254.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.254.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  token_embeds.255.proj.weight  requires_gradient  True size torch.Size([6144, 1, 4, 4])
parameter name  token_embeds.255.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  var_agg.q.weight  requires_gradient  True size torch.Size([384, 6144])
parameter name  var_agg.kv.weight  requires_gradient  True size torch.Size([768, 6144])
parameter name  var_agg.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  var_agg.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  var_linear_per_rank.weight  requires_gradient  True size torch.Size([1, 16])
parameter name  var_linear_per_rank.bias  requires_gradient  True size torch.Size([1])
parameter name  lead_time_embed.weight  requires_gradient  True size torch.Size([6144, 1])
parameter name  lead_time_embed.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.0.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.0.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.0.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.0.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.0.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.0.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.0.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.0.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.0.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.0.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.0.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.0.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.1.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.1.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.1.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.1.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.1.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.1.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.1.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.1.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.1.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.1.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.1.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.1.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.2.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.2.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.2.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.2.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.2.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.2.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.2.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.2.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.2.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.2.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.2.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.2.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.3.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.3.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.3.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.3.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.3.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.3.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.3.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.3.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.3.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.3.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.3.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.3.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.4.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.4.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.4.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.4.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.4.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.4.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.4.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.4.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.4.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.4.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.4.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.4.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.5.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.5.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.5.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.5.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.5.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.5.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.5.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.5.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.5.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.5.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.5.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.5.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.6.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.6.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.6.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.6.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.6.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.6.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.6.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.6.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.6.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.6.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.6.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.6.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.7.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.7.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.7.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.7.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.7.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.7.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.7.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.7.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.7.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.7.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.7.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.7.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.8.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.8.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.8.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.8.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.8.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.8.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.8.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.8.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.8.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.8.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.8.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.8.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.9.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.9.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.9.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.9.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.9.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.9.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.9.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.9.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.9.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.9.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.9.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.9.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.10.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.10.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.10.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.10.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.10.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.10.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.10.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.10.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.10.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.10.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.10.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.10.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.11.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.11.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.11.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.11.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.11.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.11.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.11.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.11.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.11.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.11.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.11.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.11.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.12.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.12.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.12.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.12.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.12.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.12.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.12.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.12.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.12.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.12.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.12.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.12.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.13.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.13.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.13.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.13.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.13.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.13.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.13.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.13.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.13.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.13.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.13.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.13.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.14.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.14.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.14.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.14.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.14.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.14.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.14.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.14.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.14.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.14.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.14.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.14.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.15.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.15.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.15.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.15.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.15.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.15.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.15.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.15.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.15.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.15.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.15.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.15.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.16.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.16.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.16.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.16.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.16.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.16.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.16.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.16.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.16.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.16.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.16.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.16.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.17.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.17.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.17.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.17.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.17.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.17.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.17.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.17.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.17.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.17.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.17.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.17.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.18.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.18.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.18.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.18.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.18.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.18.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.18.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.18.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.18.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.18.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.18.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.18.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.19.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.19.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.19.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.19.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.19.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.19.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.19.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.19.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.19.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.19.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.19.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.19.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.20.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.20.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.20.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.20.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.20.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.20.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.20.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.20.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.20.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.20.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.20.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.20.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.21.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.21.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.21.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.21.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.21.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.21.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.21.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.21.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.21.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.21.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.21.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.21.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.22.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.22.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.22.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.22.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.22.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.22.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.22.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.22.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.22.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.22.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.22.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.22.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.23.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.23.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.23.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.23.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.23.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.23.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.23.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.23.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.23.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.23.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.23.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.23.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.24.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.24.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.24.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.24.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.24.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.24.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.24.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.24.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.24.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.24.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.24.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.24.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.25.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.25.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.25.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.25.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.25.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.25.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.25.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.25.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.25.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.25.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.25.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.25.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.26.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.26.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.26.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.26.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.26.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.26.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.26.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.26.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.26.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.26.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.26.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.26.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.27.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.27.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.27.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.27.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.27.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.27.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.27.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.27.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.27.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.27.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.27.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.27.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.28.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.28.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.28.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.28.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.28.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.28.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.28.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.28.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.28.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.28.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.28.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.28.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.29.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.29.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.29.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.29.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.29.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.29.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.29.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.29.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.29.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.29.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.29.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.29.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.30.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.30.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.30.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.30.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.30.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.30.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.30.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.30.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.30.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.30.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.30.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.30.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.31.norm1.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.31.norm1.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.31.attn.qkv.weight  requires_gradient  True size torch.Size([1152, 6144])
parameter name  blocks.31.attn.qkv.bias  requires_gradient  True size torch.Size([1152])
parameter name  blocks.31.attn.proj.weight  requires_gradient  True size torch.Size([6144, 384])
parameter name  blocks.31.attn.proj.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.31.norm2.weight  requires_gradient  True size torch.Size([6144])
parameter name  blocks.31.norm2.bias  requires_gradient  True size torch.Size([6144])
parameter name  blocks.31.mlp.fc1.weight  requires_gradient  True size torch.Size([1536, 6144])
parameter name  blocks.31.mlp.fc1.bias  requires_gradient  True size torch.Size([1536])
parameter name  blocks.31.mlp.fc2.weight  requires_gradient  True size torch.Size([6144, 1536])
parameter name  blocks.31.mlp.fc2.bias  requires_gradient  True size torch.Size([6144])
parameter name  norm.weight  requires_gradient  True size torch.Size([6144])
parameter name  norm.bias  requires_gradient  True size torch.Size([6144])
parameter name  head.0.weight  requires_gradient  True size torch.Size([6144, 6144])
parameter name  head.0.bias  requires_gradient  True size torch.Size([6144])
parameter name  head.2.weight  requires_gradient  True size torch.Size([6144, 6144])
parameter name  head.2.bias  requires_gradient  True size torch.Size([6144])
parameter name  head.5.weight  requires_gradient  True size torch.Size([4096, 6144])
parameter name  head.5.bias  requires_gradient  True size torch.Size([4096])
total_params before FSDP tensor(14796673041) params_per_gpu tensor(1058289681)
total_params after FSDP FullyShardedDataParallel(
  (_fsdp_wrapped_module): ClimaX(
    (token_embeds): ModuleList(
      (0-255): 256 x PatchEmbed(
        (proj): Conv2d(1, 6144, kernel_size=(4, 4), stride=(4, 4))
        (norm): Identity()
      )
    )
    (var_agg): FullyShardedDataParallel(
      (_fsdp_wrapped_module): CheckpointWrapper(
        (_checkpoint_wrapped_module): VariableMapping_Attention(
          (q): Linear(in_features=6144, out_features=384, bias=False)
          (kv): Linear(in_features=6144, out_features=768, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=6144, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (var_linear_per_rank): Linear(in_features=16, out_features=1, bias=True)
    (lead_time_embed): Linear(in_features=1, out_features=6144, bias=True)
    (pos_drop): Dropout(p=0.1, inplace=False)
    (blocks): ModuleList(
      (0-31): 32 x FullyShardedDataParallel(
        (_fsdp_wrapped_module): CheckpointWrapper(
          (_checkpoint_wrapped_module): Block(
            (norm1): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=6144, out_features=1152, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=384, out_features=6144, bias=True)
              (proj_drop): Dropout(p=0.1, inplace=False)
            )
            (ls1): Identity()
            (norm2): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=6144, out_features=1536, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.1, inplace=False)
              (fc2): Linear(in_features=1536, out_features=6144, bias=True)
            )
            (ls2): Identity()
          )
        )
      )
    )
    (norm): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)
    (head): Sequential(
      (0): Linear(in_features=6144, out_features=6144, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=6144, out_features=6144, bias=True)
      (3): GELU(approximate='none')
      (4): Pred_Rearrange()
      (5): Linear(in_features=6144, out_features=4096, bias=True)
    )
  )
)
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
rank  15 lister_train reset: mpi-esm 9 9 9
rank  15 lister_test reset: mpi-esm 9 9 9
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
use_ddstore is : 0
len(dict_root_dirs) 1
rank  0 lister_train reset: mpi-esm 9 9 9
rank  0 lister_test reset: mpi-esm 9 9 9
use_ddstore is : 0
use_ddstore is : 0
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
memory stats reset, ready to track
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
epoch-train:  0 batch_idx 0 world_rank 0  loss  3.140625
epoch-train:  0 batch_idx 1 world_rank 0  loss  2.921875
epoch-train:  0 batch_idx 2 world_rank 0  loss  nan
epoch-train:  0 batch_idx 3 world_rank 0  loss  nan
[2025-03-16 18:38:18,953] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:38:18,954] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:38:18,954] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:38:18,954] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:38:18,955] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:38:18,955] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:38:18,955] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:38:18,955] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:38:18,955] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:38:18,956] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:38:18,956] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:38:18,956] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:38:18,956] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:38:18,957] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:38:18,958] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:38:18,962] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-16 18:38:19,722] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:38:19,722] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:38:19,722] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:38:19,723] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:38:19,723] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:38:19,723] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:38:19,723] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:38:19,724] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:38:19,724] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:38:19,724] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:38:19,724] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:38:19,724] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:38:19,724] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:38:19,725] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:38:19,725] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-16 18:38:19,725] [INFO] [profiler.py:226:end_profile] Flops profiler finished
global rank 7: ddp rank 0 iter_start,iter_end = 0 8
global rank 7: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 15: ddp rank 0 iter_start,iter_end = 0 8
global rank 15: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 12: ddp rank 0 iter_start,iter_end = 0 8
global rank 12: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 1: ddp rank 0 iter_start,iter_end = 0 8
global rank 1: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 0: ddp rank 0 iter_start,iter_end = 0 8
global rank 0: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 3: ddp rank 0 iter_start,iter_end = 0 8
global rank 3: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 13: ddp rank 0 iter_start,iter_end = 0 8
global rank 13: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 2: ddp rank 0 iter_start,iter_end = 0 8
global rank 2: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 11: ddp rank 0 iter_start,iter_end = 0 8
global rank 11: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 8: ddp rank 0 iter_start,iter_end = 0 8
global rank 8: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 14: ddp rank 0 iter_start,iter_end = 0 8
global rank 14: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 9: ddp rank 0 iter_start,iter_end = 0 8
global rank 9: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 10: ddp rank 0 iter_start,iter_end = 0 8
global rank 10: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 6: ddp rank 0 iter_start,iter_end = 0 8
global rank 6: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 4: ddp rank 0 iter_start,iter_end = 0 8
global rank 4: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 5: ddp rank 0 iter_start,iter_end = 0 8
global rank 5: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
I am here
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
MUST: Model 1058.289681 M TFLOPS: 23.13815727412712

--> cuda max reserved memory = 28.4199
--> max reserved percentage = 44.42 %

--> cuda max memory allocated = 20.9464
--> max allocated percentage = 32.74 %

--> peak active memory = 26.392
--> peak active memory 41.25 %

cudaMalloc retries = 0
cuda OOM = 0

HERE1 Namespace(arch='orbit_linear', batch_size=2, channels=256, depth=32, embed_dim=6144, fa2=True, fsdp_size=1, imagex=128, imagey=256, max_epochs=1, num_heads=32, real=False, seq_par_size=1, simple_ddp_size=1, tensor_par_size=16, yaml_config='configs/ERA5-100million-91variables.yaml') 28.4199 23.13815727412712 tensor(14796673041)
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
I am here
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
0.02user 0.20system 4:56.50elapsed 0%CPU (0avgtext+0avgdata 17408maxresident)k
2806inputs+1360outputs (10major+3188minor)pagefaults 0swaps
