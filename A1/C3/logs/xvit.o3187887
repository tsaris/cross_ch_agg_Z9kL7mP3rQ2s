
Lmod is automatically replacing "cce/18.0.1" with "gcc-native/13.2".


Lmod is automatically replacing "PrgEnv-cray/8.6.0" with "PrgEnv-gnu/8.6.0".


Due to MODULEPATH changes, the following have been reloaded:
  1) cray-libsci/24.11.0     2) cray-mpich/8.1.31     3) darshan-runtime/3.4.6-mpi


Lmod is automatically replacing "gcc-native/13.2" with "gcc/12.2.0".


Inactive Modules:
  1) darshan-runtime

The following have been reloaded with a version change:
  1) cray-libsci/24.11.0 => cray-libsci/23.09.1.1
  2) cray-mpich/8.1.31 => cray-mpich/8.1.27
  3) libfabric/1.22.0 => libfabric/1.20.1

Lmod has detected the following error: These module(s) or extension(s) exist
but cannot be loaded as requested: "darshan-runtime"
   Try: "module spider darshan-runtime" to see how to load the module(s).



for i in {orbit_linear,orbit_hier,orbit_hier_token_noagg_self}; do for j in {8192,}; do for k in {256,512,768}; do python train.py \ configs/ERA5-100million-91variables.yaml \ --max_epochs 1 \ --fa2 \ --fsdp_size 1 \ --simple_ddp_size 1 \ --seq_par_size 1 \ --tensor_par_size 32 \ --batch_size 2 \ --arch $i \ --channels $k \ --imagex 128 \ --imagey 256 \ --embed_dim $j \ --depth 32 \ --num_heads 32 echo "sleeping..." sleep 5 echo "Done" done done done
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:04:32,272] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:04:32,272] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:04:32,272] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:04:32,272] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:04:32,272] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:04:32,272] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:04:32,272] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:04:32,272] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:04:32,277] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:04:32,277] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:04:32,277] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:04:32,277] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:04:32,277] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:04:32,277] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:04:32,277] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:04:32,277] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:04:32,278] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:04:32,278] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:04:32,278] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:04:32,278] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:04:32,278] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:04:32,278] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:04:32,278] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:04:32,278] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:04:32,282] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:04:32,282] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:04:32,282] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:04:32,282] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:04:32,282] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:04:32,282] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:04:32,282] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:04:32,282] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
HERE0 Namespace(arch='orbit_linear', batch_size=2, channels=256, depth=32, embed_dim=8192, fa2=True, fsdp_size=1, imagex=128, imagey=256, max_epochs=1, num_heads=32, real=False, seq_par_size=1, simple_ddp_size=1, tensor_par_size=32, yaml_config='configs/ERA5-100million-91variables.yaml')
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
rank 20 Before initialize parallelism groups
rank 15 Before initialize parallelism groups
rank 17 Before initialize parallelism groups
rank 11 Before initialize parallelism groups
rank 19 Before initialize parallelism groups
rank 2 Before initialize parallelism groups
rank 7 Before initialize parallelism groups
rank 22 Before initialize parallelism groups
rank 4 Before initialize parallelism groups
rank 16 Before initialize parallelism groups
rank 15 After initialize parallelism groups
rank 17 After initialize parallelism groups
rank 20 After initialize parallelism groups
rank 14 Before initialize parallelism groups
rank 10 Before initialize parallelism groups
rank 5 Before initialize parallelism groups
rank 12 Before initialize parallelism groups
rank 11 After initialize parallelism groups
rank 6 Before initialize parallelism groups
rank 19 After initialize parallelism groups
rank 21 Before initialize parallelism groups
rank 18 Before initialize parallelism groups
rank 23 Before initialize parallelism groups
rank 22 After initialize parallelism groups
rank 16 After initialize parallelism groups
rank 8 Before initialize parallelism groups
rank 2 After initialize parallelism groups
rank 4 After initialize parallelism groups
rank 13 Before initialize parallelism groups
rank 7 After initialize parallelism groups
rank 3 Before initialize parallelism groups
rank 9 Before initialize parallelism groups
rank 14 After initialize parallelism groups
rank 10 After initialize parallelism groups
rank 1 Before initialize parallelism groups
rank 5 After initialize parallelism groups
rank 12 After initialize parallelism groups
rank 6 After initialize parallelism groups
rank 21 After initialize parallelism groups
rank 23 After initialize parallelism groups
rank 18 After initialize parallelism groups
rank 8 After initialize parallelism groups
rank 13 After initialize parallelism groups
rank 3 After initialize parallelism groups
rank 9 After initialize parallelism groups
rank 1 After initialize parallelism groups
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
config_path  configs/ERA5-100million-91variables.yaml
rank 24 Before initialize parallelism groups
rank 30 Before initialize parallelism groups
rank 27 Before initialize parallelism groups
rank 26 Before initialize parallelism groups
rank 25 Before initialize parallelism groups
rank 29 Before initialize parallelism groups
rank 31 Before initialize parallelism groups
rank 28 Before initialize parallelism groups
rank 26 After initialize parallelism groups
rank 27 After initialize parallelism groups
rank 30 After initialize parallelism groups
rank 24 After initialize parallelism groups
rank 25 After initialize parallelism groups
rank 29 After initialize parallelism groups
rank 31 After initialize parallelism groups
{'seed_everything': 42, 'trainer': {'checkpoint_path': '/lustre/orion/stf218/world-shared/atsaris/tmp/tmp_checkpoints', 'checkpoint_filename': 'multi_last', 'resume_from_checkpoint': False}, 'parallelism': {'cpu_offloading': False}, 'model': {'lr': 2e-05, 'beta_1': 0.9, 'beta_2': 0.95, 'weight_decay': '1e-5', 'warmup_steps': 1000, 'max_steps': 20000, 'warmup_start_lr': '1e-8', 'eta_min': '1e-8', 'net': {'class_path': 'climax.arch.ClimaX', 'init_args': {'default_vars': ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_70', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000'], 'patch_size': 4, 'decoder_depth': 2, 'mlp_ratio': 4, 'drop_path': 0.1, 'drop_rate': 0.1, 'aggregated_variables': 1}}}, 'data': {'dict_root_dirs': {'mpi-esm': '/lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0'}, 'dict_start_idx': {'mpi-esm': 0}, 'dict_end_idx': {'mpi-esm': 1}, 'dict_in_variables': {'mpi-esm': ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000']}, 'dict_out_variables': {'mpi-esm': ['geopotential_500', 'temperature_850', '2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind']}, 'dict_max_predict_ranges': {'mpi-esm': 24}, 'dict_random_lead_time': {'mpi-esm': False}, 'dict_hrs_each_step': {'mpi-esm': 1}, 'dict_buffer_sizes': {'mpi-esm': 1}, 'num_workers': 1, 'pin_memory': False}}
rank 28 After initialize parallelism groups
max_epochs 1 data_par_size 1 fsdp_size 1 simple_ddp_size 1 tensor_par_size 32 seq_par_size 1 cpu_offloading False
lr  2e-05 beta_1  0.9 beta_2 0.95 weight_decay 1e-05 class_path climax.arch.ClimaX default_vars ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_70', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000']
img_size_x 128 img_size_y 256 patch_size 4 emb_dim 8192 depth 32 decoder_depth 2 num_heads 32 mlp_ratio 4 drop_path 0.1 drop_rate 0.1 aggregated_variables 1
dict_root_dirs {'mpi-esm': '/lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0'} dict_start_idx {'mpi-esm': 0} dict_end_idx {'mpi-esm': 1} batch_size 2 num_workers 1
warmup_steps 1000 max_steps 20000 warmup_start_lr 1e-08 eta_min 1e-08
checkpoint_path /lustre/orion/stf218/world-shared/atsaris/tmp/tmp_checkpoints checkpoint_filename multi_last resume_from_checkpoint False
rank 0 Before initialize parallelism groups
rank 0 After initialize parallelism groups
rank 19 After initialize model
rank 19 Before the second dist.barrier()
rank 6 After initialize model
rank 6 Before the second dist.barrier()
rank 9 After initialize model
rank 9 Before the second dist.barrier()
rank 1 After initialize model
rank 1 Before the second dist.barrier()
rank 27 After initialize model
rank 27 Before the second dist.barrier()
rank 0 After initialize model
rank 0 Before the second dist.barrier()
rank 17 After initialize model
rank 17 Before the second dist.barrier()
rank 4 After initialize model
rank 4 Before the second dist.barrier()
rank 7 After initialize model
rank 7 Before the second dist.barrier()
rank 5 After initialize model
rank 5 Before the second dist.barrier()
rank 22 After initialize model
rank 22 Before the second dist.barrier()
rank 2 After initialize model
rank 2 Before the second dist.barrier()
rank 31 After initialize model
rank 31 Before the second dist.barrier()
rank 20 After initialize model
rank 20 Before the second dist.barrier()
rank 3 After initialize model
rank 3 Before the second dist.barrier()
rank 26 After initialize model
rank 26 Before the second dist.barrier()
rank 23 After initialize model
rank 23 Before the second dist.barrier()
rank 10 After initialize model
rank 10 Before the second dist.barrier()
rank 18 After initialize model
rank 18 Before the second dist.barrier()
rank 14 After initialize model
rank 14 Before the second dist.barrier()
rank 16 After initialize model
rank 16 Before the second dist.barrier()
rank 21 After initialize model
rank 21 Before the second dist.barrier()
rank 15 After initialize model
rank 15 Before the second dist.barrier()
rank 25 After initialize model
rank 25 Before the second dist.barrier()
rank 13 After initialize model
rank 13 Before the second dist.barrier()
rank 8 After initialize model
rank 8 Before the second dist.barrier()
rank 30 After initialize model
rank 30 Before the second dist.barrier()
rank 11 After initialize model
rank 11 Before the second dist.barrier()
rank 24 After initialize model
rank 24 Before the second dist.barrier()
rank 12 After initialize model
rank 12 Before the second dist.barrier()
rank 28 After initialize model
rank 28 Before the second dist.barrier()
rank 29 After initialize model
rank 29 Before the second dist.barrier()
rank 16 After the second dist.barrier()
rank 17 After the second dist.barrier()
rank 0 After the second dist.barrier()
rank 1 After the second dist.barrier()
rank 21 After the second dist.barrier()
rank 4 After the second dist.barrier()
rank 5 After the second dist.barrier()
rank 20 After the second dist.barrier()
rank 8 After the second dist.barrier()
rank 9 After the second dist.barrier()
rank 24 After the second dist.barrier()
rank 13 After the second dist.barrier()
rank 25 After the second dist.barrier()
rank 12 After the second dist.barrier()
rank 29 After the second dist.barrier()
rank 22 After the second dist.barrier()
rank 6 After the second dist.barrier()
rank 23 After the second dist.barrier()
rank 28 After the second dist.barrier()
rank 7 After the second dist.barrier()
rank 30 After the second dist.barrier()
rank 31 After the second dist.barrier()
rank 18 After the second dist.barrier()
parameter name  var_embed  requires_gradient  True size torch.Size([1, 256, 8192])
rank 19 After the second dist.barrier()
rank 3 After the second dist.barrier()
rank 14 After the second dist.barrier()
rank 15 After the second dist.barrier()
rank 2 After the second dist.barrier()
rank 11 After the second dist.barrier()
rank 27 After the second dist.barrier()
rank 26 After the second dist.barrier()
rank 10 After the second dist.barrier()
parameter name  var_query  requires_gradient  True size torch.Size([1, 1, 8192])
parameter name  var_query_per_rank  requires_gradient  True size torch.Size([1, 1, 8192])
parameter name  pos_embed  requires_gradient  True size torch.Size([1, 2048, 8192])
parameter name  token_embeds.0.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.0.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.1.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.1.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.2.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.2.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.3.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.3.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.4.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.4.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.5.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.5.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.6.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.6.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.7.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.7.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.8.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.8.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.9.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.9.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.10.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.10.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.11.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.11.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.12.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.12.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.13.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.13.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.14.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.14.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.15.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.15.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.16.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.16.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.17.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.17.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.18.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.18.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.19.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.19.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.20.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.20.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.21.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.21.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.22.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.22.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.23.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.23.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.24.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.24.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.25.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.25.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.26.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.26.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.27.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.27.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.28.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.28.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.29.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.29.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.30.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.30.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.31.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.31.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.32.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.32.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.33.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.33.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.34.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.34.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.35.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.35.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.36.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.36.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.37.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.37.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.38.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.38.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.39.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.39.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.40.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.40.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.41.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.41.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.42.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.42.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.43.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.43.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.44.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.44.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.45.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.45.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.46.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.46.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.47.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.47.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.48.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.48.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.49.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.49.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.50.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.50.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.51.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.51.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.52.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.52.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.53.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.53.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.54.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.54.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.55.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.55.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.56.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.56.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.57.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.57.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.58.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.58.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.59.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.59.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.60.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.60.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.61.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.61.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.62.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.62.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.63.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.63.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.64.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.64.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.65.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.65.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.66.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.66.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.67.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.67.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.68.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.68.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.69.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.69.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.70.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.70.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.71.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.71.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.72.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.72.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.73.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.73.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.74.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.74.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.75.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.75.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.76.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.76.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.77.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.77.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.78.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.78.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.79.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.79.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.80.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.80.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.81.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.81.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.82.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.82.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.83.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.83.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.84.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.84.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.85.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.85.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.86.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.86.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.87.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.87.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.88.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.88.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.89.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.89.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.90.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.90.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.91.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.91.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.92.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.92.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.93.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.93.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.94.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.94.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.95.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.95.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.96.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.96.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.97.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.97.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.98.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.98.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.99.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.99.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.100.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.100.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.101.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.101.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.102.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.102.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.103.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.103.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.104.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.104.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.105.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.105.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.106.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.106.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.107.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.107.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.108.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.108.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.109.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.109.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.110.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.110.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.111.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.111.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.112.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.112.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.113.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.113.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.114.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.114.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.115.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.115.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.116.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.116.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.117.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.117.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.118.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.118.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.119.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.119.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.120.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.120.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.121.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.121.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.122.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.122.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.123.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.123.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.124.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.124.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.125.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.125.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.126.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.126.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.127.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.127.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.128.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.128.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.129.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.129.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.130.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.130.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.131.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.131.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.132.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.132.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.133.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.133.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.134.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.134.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.135.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.135.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.136.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.136.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.137.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.137.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.138.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.138.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.139.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.139.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.140.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.140.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.141.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.141.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.142.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.142.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.143.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.143.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.144.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.144.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.145.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.145.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.146.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.146.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.147.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.147.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.148.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.148.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.149.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.149.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.150.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.150.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.151.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.151.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.152.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.152.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.153.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.153.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.154.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.154.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.155.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.155.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.156.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.156.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.157.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.157.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.158.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.158.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.159.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.159.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.160.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.160.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.161.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.161.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.162.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.162.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.163.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.163.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.164.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.164.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.165.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.165.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.166.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.166.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.167.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.167.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.168.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.168.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.169.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.169.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.170.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.170.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.171.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.171.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.172.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.172.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.173.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.173.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.174.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.174.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.175.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.175.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.176.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.176.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.177.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.177.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.178.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.178.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.179.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.179.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.180.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.180.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.181.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.181.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.182.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.182.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.183.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.183.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.184.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.184.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.185.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.185.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.186.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.186.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.187.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.187.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.188.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.188.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.189.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.189.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.190.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.190.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.191.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.191.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.192.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.192.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.193.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.193.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.194.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.194.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.195.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.195.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.196.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.196.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.197.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.197.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.198.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.198.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.199.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.199.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.200.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.200.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.201.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.201.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.202.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.202.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.203.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.203.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.204.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.204.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.205.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.205.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.206.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.206.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.207.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.207.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.208.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.208.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.209.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.209.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.210.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.210.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.211.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.211.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.212.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.212.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.213.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.213.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.214.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.214.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.215.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.215.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.216.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.216.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.217.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.217.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.218.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.218.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.219.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.219.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.220.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.220.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.221.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.221.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.222.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.222.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.223.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.223.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.224.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.224.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.225.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.225.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.226.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.226.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.227.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.227.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.228.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.228.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.229.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.229.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.230.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.230.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.231.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.231.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.232.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.232.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.233.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.233.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.234.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.234.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.235.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.235.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.236.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.236.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.237.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.237.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.238.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.238.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.239.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.239.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.240.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.240.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.241.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.241.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.242.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.242.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.243.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.243.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.244.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.244.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.245.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.245.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.246.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.246.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.247.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.247.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.248.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.248.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.249.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.249.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.250.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.250.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.251.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.251.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.252.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.252.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.253.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.253.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.254.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.254.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.255.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.255.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  var_agg.q.weight  requires_gradient  True size torch.Size([256, 8192])
parameter name  var_agg.kv.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  var_agg.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  var_agg.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  var_linear_per_rank.weight  requires_gradient  True size torch.Size([1, 8])
parameter name  var_linear_per_rank.bias  requires_gradient  True size torch.Size([1])
parameter name  lead_time_embed.weight  requires_gradient  True size torch.Size([8192, 1])
parameter name  lead_time_embed.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.0.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.0.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.0.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.0.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.0.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.0.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.1.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.1.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.1.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.1.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.1.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.1.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.2.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.2.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.2.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.2.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.2.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.2.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.3.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.3.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.3.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.3.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.3.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.3.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.4.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.4.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.4.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.4.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.4.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.4.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.5.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.5.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.5.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.5.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.5.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.5.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.6.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.6.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.6.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.6.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.6.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.6.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.7.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.7.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.7.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.7.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.7.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.7.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.8.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.8.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.8.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.8.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.8.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.8.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.9.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.9.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.9.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.9.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.9.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.9.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.10.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.10.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.10.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.10.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.10.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.10.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.11.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.11.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.11.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.11.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.11.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.11.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.12.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.12.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.12.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.12.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.12.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.12.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.13.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.13.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.13.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.13.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.13.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.13.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.14.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.14.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.14.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.14.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.14.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.14.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.15.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.15.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.15.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.15.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.15.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.15.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.16.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.16.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.16.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.16.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.16.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.16.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.17.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.17.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.17.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.17.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.17.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.17.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.18.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.18.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.18.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.18.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.18.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.18.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.19.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.19.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.19.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.19.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.19.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.19.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.20.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.20.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.20.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.20.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.20.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.20.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.21.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.21.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.21.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.21.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.21.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.21.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.22.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.22.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.22.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.22.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.22.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.22.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.23.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.23.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.23.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.23.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.23.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.23.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.24.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.24.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.24.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.24.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.24.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.24.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.25.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.25.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.25.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.25.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.25.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.25.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.26.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.26.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.26.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.26.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.26.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.26.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.27.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.27.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.27.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.27.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.27.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.27.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.28.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.28.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.28.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.28.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.28.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.28.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.29.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.29.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.29.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.29.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.29.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.29.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.30.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.30.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.30.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.30.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.30.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.30.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.31.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.31.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.31.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.31.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.31.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.31.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  norm.weight  requires_gradient  True size torch.Size([8192])
parameter name  norm.bias  requires_gradient  True size torch.Size([8192])
parameter name  head.0.weight  requires_gradient  True size torch.Size([8192, 8192])
parameter name  head.0.bias  requires_gradient  True size torch.Size([8192])
parameter name  head.2.weight  requires_gradient  True size torch.Size([8192, 8192])
parameter name  head.2.bias  requires_gradient  True size torch.Size([8192])
parameter name  head.5.weight  requires_gradient  True size torch.Size([4096, 8192])
parameter name  head.5.bias  requires_gradient  True size torch.Size([4096])
total_params before FSDP tensor(26280529929) params_per_gpu tensor(1037701129)
total_params after FSDP FullyShardedDataParallel(
  (_fsdp_wrapped_module): ClimaX(
    (token_embeds): ModuleList(
      (0-255): 256 x PatchEmbed(
        (proj): Conv2d(1, 8192, kernel_size=(4, 4), stride=(4, 4))
        (norm): Identity()
      )
    )
    (var_agg): FullyShardedDataParallel(
      (_fsdp_wrapped_module): CheckpointWrapper(
        (_checkpoint_wrapped_module): VariableMapping_Attention(
          (q): Linear(in_features=8192, out_features=256, bias=False)
          (kv): Linear(in_features=8192, out_features=512, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=8192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (var_linear_per_rank): Linear(in_features=8, out_features=1, bias=True)
    (lead_time_embed): Linear(in_features=1, out_features=8192, bias=True)
    (pos_drop): Dropout(p=0.1, inplace=False)
    (blocks): ModuleList(
      (0-31): 32 x FullyShardedDataParallel(
        (_fsdp_wrapped_module): CheckpointWrapper(
          (_checkpoint_wrapped_module): Block(
            (norm1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=8192, out_features=768, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=256, out_features=8192, bias=True)
              (proj_drop): Dropout(p=0.1, inplace=False)
            )
            (ls1): Identity()
            (norm2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=8192, out_features=1024, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.1, inplace=False)
              (fc2): Linear(in_features=1024, out_features=8192, bias=True)
            )
            (ls2): Identity()
          )
        )
      )
    )
    (norm): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
    (head): Sequential(
      (0): Linear(in_features=8192, out_features=8192, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=8192, out_features=8192, bias=True)
      (3): GELU(approximate='none')
      (4): Pred_Rearrange()
      (5): Linear(in_features=8192, out_features=4096, bias=True)
    )
  )
)
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
rank  0 lister_train reset: mpi-esm 9 9 9
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
rank  31 lister_train reset: mpi-esm 9 9 9
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
memory stats reset, ready to track
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
MIOpen(HIP): Warning [PlainTextDb] Unable to create a directory: "/tmp/.config/miopen"
epoch-train:  0 batch_idx 0 world_rank 0  loss  9.625
epoch-train:  0 batch_idx 1 world_rank 0  loss  nan
epoch-train:  0 batch_idx 2 world_rank 0  loss  nan
[2025-03-12 17:05:25,022] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:05:25,023] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:05:25,023] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:05:25,023] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:05:25,024] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:05:25,024] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:05:25,024] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:05:25,024] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:05:25,024] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:05:25,024] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:05:25,024] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:05:25,025] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:05:25,025] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:05:25,025] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:05:25,025] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:05:25,025] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:05:25,025] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:05:25,026] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:05:25,026] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:05:25,026] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:05:25,027] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:05:25,027] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:05:25,028] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:05:25,029] [INFO] [profiler.py:80:start_profile] Flops profiler started
epoch-train:  0 batch_idx 3 world_rank 0  loss  nan
[2025-03-12 17:05:25,031] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:05:25,031] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:05:25,031] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:05:25,031] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:05:25,032] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:05:25,032] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:05:25,032] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:05:25,032] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:05:25,902] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:05:25,902] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:05:25,902] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:05:25,902] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:05:25,903] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:05:25,903] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:05:25,903] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:05:25,903] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:05:25,903] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:05:25,903] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:05:25,903] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:05:25,903] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:05:25,903] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:05:25,903] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:05:25,904] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:05:25,904] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:05:25,904] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:05:25,904] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:05:25,904] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:05:25,904] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:05:25,904] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:05:25,904] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:05:25,904] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:05:25,904] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:05:25,904] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:05:25,904] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:05:25,904] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:05:25,905] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:05:25,905] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:05:25,905] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:05:25,905] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:05:25,906] [INFO] [profiler.py:226:end_profile] Flops profiler finished
global rank 3: ddp rank 0 iter_start,iter_end = 0 8
global rank 3: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 2: ddp rank 0 iter_start,iter_end = 0 8
global rank 2: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 23: ddp rank 0 iter_start,iter_end = 0 8
global rank 23: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 24: ddp rank 0 iter_start,iter_end = 0 8
global rank 24: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 18: ddp rank 0 iter_start,iter_end = 0 8
global rank 18: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 22: ddp rank 0 iter_start,iter_end = 0 8
global rank 22: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 12: ddp rank 0 iter_start,iter_end = 0 8
global rank 12: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 30: ddp rank 0 iter_start,iter_end = 0 8
global rank 30: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 9: ddp rank 0 iter_start,iter_end = 0 8
global rank 9: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 1: ddp rank 0 iter_start,iter_end = 0 8
global rank 1: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 4: ddp rank 0 iter_start,iter_end = 0 8
global rank 4: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 17: ddp rank 0 iter_start,iter_end = 0 8
global rank 17: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 20: ddp rank 0 iter_start,iter_end = 0 8
global rank 20: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 15: ddp rank 0 iter_start,iter_end = 0 8
global rank 15: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 26: ddp rank 0 iter_start,iter_end = 0 8
global rank 26: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 29: ddp rank 0 iter_start,iter_end = 0 8
global rank 29: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 11: ddp rank 0 iter_start,iter_end = 0 8
global rank 11: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 8: ddp rank 0 iter_start,iter_end = 0 8
global rank 8: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 27: ddp rank 0 iter_start,iter_end = 0 8
global rank 27: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 28: ddp rank 0 iter_start,iter_end = 0 8
global rank 28: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 16: ddp rank 0 iter_start,iter_end = 0 8
global rank 16: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 6: ddp rank 0 iter_start,iter_end = 0 8
global rank 6: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 25: ddp rank 0 iter_start,iter_end = 0 8
global rank 25: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 19: ddp rank 0 iter_start,iter_end = 0 8
global rank 19: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 31: ddp rank 0 iter_start,iter_end = 0 8
global rank 31: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 5: ddp rank 0 iter_start,iter_end = 0 8
global rank 5: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 14: ddp rank 0 iter_start,iter_end = 0 8
global rank 14: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 13: ddp rank 0 iter_start,iter_end = 0 8
global rank 13: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 21: ddp rank 0 iter_start,iter_end = 0 8
global rank 21: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 10: ddp rank 0 iter_start,iter_end = 0 8
global rank 10: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 0: ddp rank 0 iter_start,iter_end = 0 8
global rank 0: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 7: ddp rank 0 iter_start,iter_end = 0 8
global rank 7: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
MUST: Model 1037.701129 M TFLOPS: 15.052554147162574

--> cuda max reserved memory = 33.7832
--> max reserved percentage = 52.8 %

--> cuda max memory allocated = 23.3641
--> max allocated percentage = 36.52 %

--> peak active memory = 30.8983
--> peak active memory 48.29 %

cudaMalloc retries = 0
cuda OOM = 0

HERE1 Namespace(arch='orbit_linear', batch_size=2, channels=256, depth=32, embed_dim=8192, fa2=True, fsdp_size=1, imagex=128, imagey=256, max_epochs=1, num_heads=32, real=False, seq_par_size=1, simple_ddp_size=1, tensor_par_size=32, yaml_config='configs/ERA5-100million-91variables.yaml') 33.7832 15.052554147162574 tensor(26280529929)
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:05:40,908] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:05:40,908] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:05:40,908] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:05:40,908] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:05:40,908] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:05:40,908] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:05:40,908] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:05:40,908] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:05:40,915] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:05:40,915] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:05:40,915] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:05:40,915] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:05:40,915] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:05:40,915] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:05:40,915] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:05:40,916] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:05:40,915] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:05:40,916] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:05:40,916] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:05:40,916] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:05:40,916] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:05:40,916] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:05:40,916] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:05:40,916] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:05:40,917] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:05:40,917] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:05:40,917] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:05:40,917] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:05:40,917] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:05:40,917] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:05:40,917] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:05:40,917] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
HERE0 Namespace(arch='orbit_linear', batch_size=2, channels=512, depth=32, embed_dim=8192, fa2=True, fsdp_size=1, imagex=128, imagey=256, max_epochs=1, num_heads=32, real=False, seq_par_size=1, simple_ddp_size=1, tensor_par_size=32, yaml_config='configs/ERA5-100million-91variables.yaml')
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
rank 5 Before initialize parallelism groups
rank 2 Before initialize parallelism groups
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
rank 17 Before initialize parallelism groups
rank 23 Before initialize parallelism groups
rank 21 Before initialize parallelism groups
rank 11 Before initialize parallelism groups
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
config_path  configs/ERA5-100million-91variables.yaml
rank 8 Before initialize parallelism groups
rank 13 Before initialize parallelism groups
rank 2 After initialize parallelism groups
rank 5 After initialize parallelism groups
rank 25 Before initialize parallelism groups
rank 30 Before initialize parallelism groups
rank 28 Before initialize parallelism groups
rank 10 Before initialize parallelism groups
rank 31 Before initialize parallelism groups
rank 29 Before initialize parallelism groups
rank 17 After initialize parallelism groups
rank 1 Before initialize parallelism groups
rank 21 After initialize parallelism groups
rank 23 After initialize parallelism groups
rank 14 Before initialize parallelism groups
rank 11 After initialize parallelism groups
rank 16 Before initialize parallelism groups
rank 20 Before initialize parallelism groups
rank 24 Before initialize parallelism groups
rank 4 Before initialize parallelism groups
rank 7 Before initialize parallelism groups
rank 19 Before initialize parallelism groups
rank 18 Before initialize parallelism groups
rank 8 After initialize parallelism groups
rank 13 After initialize parallelism groups
rank 9 Before initialize parallelism groups
rank 28 After initialize parallelism groups
rank 30 After initialize parallelism groups
rank 10 After initialize parallelism groups
rank 25 After initialize parallelism groups
rank 27 Before initialize parallelism groups
rank 31 After initialize parallelism groups
rank 29 After initialize parallelism groups
rank 22 Before initialize parallelism groups
rank 1 After initialize parallelism groups
rank 14 After initialize parallelism groups
rank 16 After initialize parallelism groups
rank 12 Before initialize parallelism groups
rank 20 After initialize parallelism groups
rank 24 After initialize parallelism groups
rank 4 After initialize parallelism groups
rank 7 After initialize parallelism groups
{'seed_everything': 42, 'trainer': {'checkpoint_path': '/lustre/orion/stf218/world-shared/atsaris/tmp/tmp_checkpoints', 'checkpoint_filename': 'multi_last', 'resume_from_checkpoint': False}, 'parallelism': {'cpu_offloading': False}, 'model': {'lr': 2e-05, 'beta_1': 0.9, 'beta_2': 0.95, 'weight_decay': '1e-5', 'warmup_steps': 1000, 'max_steps': 20000, 'warmup_start_lr': '1e-8', 'eta_min': '1e-8', 'net': {'class_path': 'climax.arch.ClimaX', 'init_args': {'default_vars': ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_70', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000'], 'patch_size': 4, 'decoder_depth': 2, 'mlp_ratio': 4, 'drop_path': 0.1, 'drop_rate': 0.1, 'aggregated_variables': 1}}}, 'data': {'dict_root_dirs': {'mpi-esm': '/lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0'}, 'dict_start_idx': {'mpi-esm': 0}, 'dict_end_idx': {'mpi-esm': 1}, 'dict_in_variables': {'mpi-esm': ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000']}, 'dict_out_variables': {'mpi-esm': ['geopotential_500', 'temperature_850', '2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind']}, 'dict_max_predict_ranges': {'mpi-esm': 24}, 'dict_random_lead_time': {'mpi-esm': False}, 'dict_hrs_each_step': {'mpi-esm': 1}, 'dict_buffer_sizes': {'mpi-esm': 1}, 'num_workers': 1, 'pin_memory': False}}
rank 6 Before initialize parallelism groups
rank 18 After initialize parallelism groups
rank 19 After initialize parallelism groups
rank 9 After initialize parallelism groups
rank 26 Before initialize parallelism groups
rank 3 Before initialize parallelism groups
rank 27 After initialize parallelism groups
rank 15 Before initialize parallelism groups
max_epochs 1 data_par_size 1 fsdp_size 1 simple_ddp_size 1 tensor_par_size 32 seq_par_size 1 cpu_offloading False
lr  2e-05 beta_1  0.9 beta_2 0.95 weight_decay 1e-05 class_path climax.arch.ClimaX default_vars ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_70', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000']
img_size_x 128 img_size_y 256 patch_size 4 emb_dim 8192 depth 32 decoder_depth 2 num_heads 32 mlp_ratio 4 drop_path 0.1 drop_rate 0.1 aggregated_variables 1
dict_root_dirs {'mpi-esm': '/lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0'} dict_start_idx {'mpi-esm': 0} dict_end_idx {'mpi-esm': 1} batch_size 2 num_workers 1
warmup_steps 1000 max_steps 20000 warmup_start_lr 1e-08 eta_min 1e-08
checkpoint_path /lustre/orion/stf218/world-shared/atsaris/tmp/tmp_checkpoints checkpoint_filename multi_last resume_from_checkpoint False
rank 0 Before initialize parallelism groups
rank 22 After initialize parallelism groups
rank 12 After initialize parallelism groups
rank 6 After initialize parallelism groups
rank 26 After initialize parallelism groups
rank 3 After initialize parallelism groups
rank 15 After initialize parallelism groups
rank 0 After initialize parallelism groups
rank 13 After initialize model
rank 13 Before the second dist.barrier()
rank 2 After initialize model
rank 2 Before the second dist.barrier()
rank 30 After initialize model
rank 30 Before the second dist.barrier()
rank 22 After initialize model
rank 22 Before the second dist.barrier()
rank 21 After initialize model
rank 21 Before the second dist.barrier()
rank 3 After initialize model
rank 3 Before the second dist.barrier()
rank 4 After initialize model
rank 4 Before the second dist.barrier()
rank 6 After initialize model
rank 6 Before the second dist.barrier()
rank 31 After initialize model
rank 31 Before the second dist.barrier()
rank 1 After initialize model
rank 1 Before the second dist.barrier()
rank 0 After initialize model
rank 0 Before the second dist.barrier()
rank 5 After initialize model
rank 5 Before the second dist.barrier()
rank 23 After initialize model
rank 23 Before the second dist.barrier()
rank 7 After initialize model
rank 7 Before the second dist.barrier()
rank 15 After initialize model
rank 15 Before the second dist.barrier()
rank 14 After initialize model
rank 14 Before the second dist.barrier()
rank 20 After initialize model
rank 20 Before the second dist.barrier()
rank 19 After initialize model
rank 19 Before the second dist.barrier()
rank 25 After initialize model
rank 25 Before the second dist.barrier()
rank 11 After initialize model
rank 11 Before the second dist.barrier()
rank 18 After initialize model
rank 18 Before the second dist.barrier()
rank 12 After initialize model
rank 12 Before the second dist.barrier()
rank 10 After initialize model
rank 10 Before the second dist.barrier()
rank 9 After initialize model
rank 9 Before the second dist.barrier()
rank 8 After initialize model
rank 8 Before the second dist.barrier()
rank 17 After initialize model
rank 17 Before the second dist.barrier()
rank 16 After initialize model
rank 16 Before the second dist.barrier()
rank 28 After initialize model
rank 28 Before the second dist.barrier()
rank 29 After initialize model
rank 29 Before the second dist.barrier()
rank 26 After initialize model
rank 26 Before the second dist.barrier()
rank 24 After initialize model
rank 24 Before the second dist.barrier()
rank 27 After initialize model
rank 27 Before the second dist.barrier()
rank 1 After the second dist.barrier()
rank 16 After the second dist.barrier()
rank 17 After the second dist.barrier()
rank 18 After the second dist.barrier()
rank 19 After the second dist.barrier()
rank 20 After the second dist.barrier()
rank 21 After the second dist.barrier()
rank 22 After the second dist.barrier()
rank 23 After the second dist.barrier()
rank 0 After the second dist.barrier()
rank 24 After the second dist.barrier()
rank 8 After the second dist.barrier()
rank 25 After the second dist.barrier()
rank 5 After the second dist.barrier()
rank 9 After the second dist.barrier()
rank 4 After the second dist.barrier()
parameter name  var_embed  requires_gradient  True size torch.Size([1, 512, 8192])
rank 28 After the second dist.barrier()
rank 6 After the second dist.barrier()
rank 12 After the second dist.barrier()
rank 29 After the second dist.barrier()
rank 7 After the second dist.barrier()
rank 13 After the second dist.barrier()
parameter name  var_query  requires_gradient  True size torch.Size([1, 1, 8192])
parameter name  var_query_per_rank  requires_gradient  True size torch.Size([1, 1, 8192])
rank 30 After the second dist.barrier()
rank 3 After the second dist.barrier()
rank 14 After the second dist.barrier()
parameter name  pos_embed  requires_gradient  True size torch.Size([1, 2048, 8192])
rank 15 After the second dist.barrier()
rank 2 After the second dist.barrier()
parameter name  token_embeds.0.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.0.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.1.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.1.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.2.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
rank 31 After the second dist.barrier()
parameter name  token_embeds.2.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.3.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.3.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.4.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.4.proj.bias  requires_gradient  True size torch.Size([8192])
rank 27 After the second dist.barrier()
parameter name  token_embeds.5.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
rank 10 After the second dist.barrier()
parameter name  token_embeds.5.proj.bias  requires_gradient  True size torch.Size([8192])
rank 11 After the second dist.barrier()
parameter name  token_embeds.6.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.6.proj.bias  requires_gradient  True size torch.Size([8192])
rank 26 After the second dist.barrier()
parameter name  token_embeds.7.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.7.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.8.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.8.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.9.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.9.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.10.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.10.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.11.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.11.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.12.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.12.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.13.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.13.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.14.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.14.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.15.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.15.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.16.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.16.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.17.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.17.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.18.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.18.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.19.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.19.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.20.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.20.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.21.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.21.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.22.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.22.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.23.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.23.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.24.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.24.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.25.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.25.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.26.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.26.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.27.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.27.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.28.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.28.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.29.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.29.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.30.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.30.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.31.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.31.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.32.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.32.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.33.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.33.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.34.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.34.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.35.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.35.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.36.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.36.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.37.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.37.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.38.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.38.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.39.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.39.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.40.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.40.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.41.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.41.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.42.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.42.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.43.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.43.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.44.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.44.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.45.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.45.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.46.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.46.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.47.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.47.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.48.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.48.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.49.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.49.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.50.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.50.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.51.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.51.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.52.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.52.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.53.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.53.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.54.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.54.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.55.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.55.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.56.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.56.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.57.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.57.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.58.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.58.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.59.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.59.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.60.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.60.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.61.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.61.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.62.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.62.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.63.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.63.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.64.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.64.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.65.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.65.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.66.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.66.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.67.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.67.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.68.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.68.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.69.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.69.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.70.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.70.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.71.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.71.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.72.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.72.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.73.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.73.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.74.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.74.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.75.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.75.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.76.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.76.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.77.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.77.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.78.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.78.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.79.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.79.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.80.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.80.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.81.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.81.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.82.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.82.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.83.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.83.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.84.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.84.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.85.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.85.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.86.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.86.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.87.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.87.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.88.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.88.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.89.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.89.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.90.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.90.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.91.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.91.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.92.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.92.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.93.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.93.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.94.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.94.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.95.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.95.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.96.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.96.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.97.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.97.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.98.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.98.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.99.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.99.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.100.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.100.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.101.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.101.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.102.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.102.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.103.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.103.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.104.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.104.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.105.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.105.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.106.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.106.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.107.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.107.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.108.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.108.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.109.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.109.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.110.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.110.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.111.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.111.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.112.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.112.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.113.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.113.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.114.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.114.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.115.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.115.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.116.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.116.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.117.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.117.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.118.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.118.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.119.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.119.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.120.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.120.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.121.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.121.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.122.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.122.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.123.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.123.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.124.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.124.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.125.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.125.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.126.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.126.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.127.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.127.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.128.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.128.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.129.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.129.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.130.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.130.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.131.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.131.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.132.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.132.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.133.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.133.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.134.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.134.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.135.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.135.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.136.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.136.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.137.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.137.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.138.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.138.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.139.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.139.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.140.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.140.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.141.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.141.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.142.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.142.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.143.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.143.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.144.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.144.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.145.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.145.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.146.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.146.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.147.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.147.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.148.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.148.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.149.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.149.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.150.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.150.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.151.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.151.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.152.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.152.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.153.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.153.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.154.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.154.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.155.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.155.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.156.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.156.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.157.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.157.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.158.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.158.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.159.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.159.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.160.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.160.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.161.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.161.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.162.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.162.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.163.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.163.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.164.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.164.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.165.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.165.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.166.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.166.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.167.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.167.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.168.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.168.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.169.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.169.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.170.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.170.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.171.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.171.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.172.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.172.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.173.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.173.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.174.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.174.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.175.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.175.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.176.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.176.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.177.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.177.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.178.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.178.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.179.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.179.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.180.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.180.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.181.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.181.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.182.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.182.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.183.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.183.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.184.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.184.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.185.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.185.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.186.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.186.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.187.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.187.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.188.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.188.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.189.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.189.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.190.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.190.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.191.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.191.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.192.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.192.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.193.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.193.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.194.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.194.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.195.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.195.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.196.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.196.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.197.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.197.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.198.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.198.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.199.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.199.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.200.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.200.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.201.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.201.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.202.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.202.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.203.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.203.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.204.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.204.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.205.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.205.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.206.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.206.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.207.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.207.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.208.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.208.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.209.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.209.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.210.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.210.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.211.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.211.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.212.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.212.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.213.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.213.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.214.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.214.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.215.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.215.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.216.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.216.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.217.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.217.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.218.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.218.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.219.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.219.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.220.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.220.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.221.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.221.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.222.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.222.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.223.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.223.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.224.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.224.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.225.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.225.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.226.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.226.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.227.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.227.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.228.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.228.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.229.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.229.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.230.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.230.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.231.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.231.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.232.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.232.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.233.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.233.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.234.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.234.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.235.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.235.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.236.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.236.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.237.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.237.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.238.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.238.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.239.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.239.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.240.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.240.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.241.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.241.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.242.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.242.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.243.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.243.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.244.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.244.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.245.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.245.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.246.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.246.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.247.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.247.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.248.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.248.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.249.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.249.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.250.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.250.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.251.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.251.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.252.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.252.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.253.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.253.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.254.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.254.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.255.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.255.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.256.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.256.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.257.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.257.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.258.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.258.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.259.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.259.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.260.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.260.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.261.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.261.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.262.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.262.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.263.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.263.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.264.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.264.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.265.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.265.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.266.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.266.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.267.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.267.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.268.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.268.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.269.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.269.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.270.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.270.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.271.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.271.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.272.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.272.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.273.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.273.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.274.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.274.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.275.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.275.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.276.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.276.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.277.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.277.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.278.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.278.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.279.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.279.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.280.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.280.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.281.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.281.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.282.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.282.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.283.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.283.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.284.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.284.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.285.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.285.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.286.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.286.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.287.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.287.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.288.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.288.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.289.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.289.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.290.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.290.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.291.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.291.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.292.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.292.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.293.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.293.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.294.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.294.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.295.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.295.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.296.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.296.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.297.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.297.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.298.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.298.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.299.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.299.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.300.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.300.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.301.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.301.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.302.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.302.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.303.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.303.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.304.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.304.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.305.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.305.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.306.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.306.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.307.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.307.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.308.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.308.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.309.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.309.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.310.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.310.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.311.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.311.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.312.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.312.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.313.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.313.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.314.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.314.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.315.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.315.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.316.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.316.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.317.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.317.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.318.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.318.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.319.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.319.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.320.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.320.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.321.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.321.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.322.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.322.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.323.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.323.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.324.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.324.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.325.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.325.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.326.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.326.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.327.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.327.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.328.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.328.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.329.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.329.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.330.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.330.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.331.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.331.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.332.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.332.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.333.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.333.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.334.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.334.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.335.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.335.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.336.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.336.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.337.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.337.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.338.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.338.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.339.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.339.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.340.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.340.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.341.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.341.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.342.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.342.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.343.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.343.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.344.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.344.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.345.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.345.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.346.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.346.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.347.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.347.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.348.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.348.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.349.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.349.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.350.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.350.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.351.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.351.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.352.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.352.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.353.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.353.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.354.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.354.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.355.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.355.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.356.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.356.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.357.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.357.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.358.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.358.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.359.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.359.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.360.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.360.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.361.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.361.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.362.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.362.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.363.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.363.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.364.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.364.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.365.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.365.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.366.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.366.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.367.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.367.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.368.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.368.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.369.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.369.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.370.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.370.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.371.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.371.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.372.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.372.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.373.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.373.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.374.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.374.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.375.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.375.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.376.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.376.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.377.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.377.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.378.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.378.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.379.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.379.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.380.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.380.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.381.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.381.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.382.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.382.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.383.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.383.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.384.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.384.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.385.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.385.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.386.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.386.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.387.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.387.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.388.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.388.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.389.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.389.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.390.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.390.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.391.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.391.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.392.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.392.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.393.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.393.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.394.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.394.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.395.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.395.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.396.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.396.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.397.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.397.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.398.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.398.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.399.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.399.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.400.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.400.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.401.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.401.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.402.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.402.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.403.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.403.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.404.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.404.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.405.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.405.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.406.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.406.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.407.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.407.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.408.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.408.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.409.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.409.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.410.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.410.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.411.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.411.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.412.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.412.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.413.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.413.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.414.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.414.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.415.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.415.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.416.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.416.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.417.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.417.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.418.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.418.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.419.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.419.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.420.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.420.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.421.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.421.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.422.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.422.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.423.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.423.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.424.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.424.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.425.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.425.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.426.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.426.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.427.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.427.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.428.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.428.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.429.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.429.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.430.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.430.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.431.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.431.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.432.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.432.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.433.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.433.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.434.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.434.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.435.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.435.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.436.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.436.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.437.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.437.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.438.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.438.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.439.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.439.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.440.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.440.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.441.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.441.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.442.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.442.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.443.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.443.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.444.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.444.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.445.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.445.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.446.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.446.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.447.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.447.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.448.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.448.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.449.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.449.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.450.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.450.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.451.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.451.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.452.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.452.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.453.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.453.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.454.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.454.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.455.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.455.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.456.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.456.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.457.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.457.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.458.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.458.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.459.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.459.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.460.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.460.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.461.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.461.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.462.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.462.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.463.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.463.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.464.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.464.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.465.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.465.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.466.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.466.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.467.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.467.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.468.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.468.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.469.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.469.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.470.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.470.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.471.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.471.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.472.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.472.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.473.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.473.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.474.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.474.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.475.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.475.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.476.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.476.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.477.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.477.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.478.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.478.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.479.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.479.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.480.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.480.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.481.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.481.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.482.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.482.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.483.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.483.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.484.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.484.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.485.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.485.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.486.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.486.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.487.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.487.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.488.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.488.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.489.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.489.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.490.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.490.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.491.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.491.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.492.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.492.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.493.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.493.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.494.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.494.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.495.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.495.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.496.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.496.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.497.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.497.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.498.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.498.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.499.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.499.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.500.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.500.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.501.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.501.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.502.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.502.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.503.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.503.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.504.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.504.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.505.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.505.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.506.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.506.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.507.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.507.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.508.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.508.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.509.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.509.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.510.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.510.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.511.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.511.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  var_agg.q.weight  requires_gradient  True size torch.Size([256, 8192])
parameter name  var_agg.kv.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  var_agg.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  var_agg.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  var_linear_per_rank.weight  requires_gradient  True size torch.Size([1, 16])
parameter name  var_linear_per_rank.bias  requires_gradient  True size torch.Size([1])
parameter name  lead_time_embed.weight  requires_gradient  True size torch.Size([8192, 1])
parameter name  lead_time_embed.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.0.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.0.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.0.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.0.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.0.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.0.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.1.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.1.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.1.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.1.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.1.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.1.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.2.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.2.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.2.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.2.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.2.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.2.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.3.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.3.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.3.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.3.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.3.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.3.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.4.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.4.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.4.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.4.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.4.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.4.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.5.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.5.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.5.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.5.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.5.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.5.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.6.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.6.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.6.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.6.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.6.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.6.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.7.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.7.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.7.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.7.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.7.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.7.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.8.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.8.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.8.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.8.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.8.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.8.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.9.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.9.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.9.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.9.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.9.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.9.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.10.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.10.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.10.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.10.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.10.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.10.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.11.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.11.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.11.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.11.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.11.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.11.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.12.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.12.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.12.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.12.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.12.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.12.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.13.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.13.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.13.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.13.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.13.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.13.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.14.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.14.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.14.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.14.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.14.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.14.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.15.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.15.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.15.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.15.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.15.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.15.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.16.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.16.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.16.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.16.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.16.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.16.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.17.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.17.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.17.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.17.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.17.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.17.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.18.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.18.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.18.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.18.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.18.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.18.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.19.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.19.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.19.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.19.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.19.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.19.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.20.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.20.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.20.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.20.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.20.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.20.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.21.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.21.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.21.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.21.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.21.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.21.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.22.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.22.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.22.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.22.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.22.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.22.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.23.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.23.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.23.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.23.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.23.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.23.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.24.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.24.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.24.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.24.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.24.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.24.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.25.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.25.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.25.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.25.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.25.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.25.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.26.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.26.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.26.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.26.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.26.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.26.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.27.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.27.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.27.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.27.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.27.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.27.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.28.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.28.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.28.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.28.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.28.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.28.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.29.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.29.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.29.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.29.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.29.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.29.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.30.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.30.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.30.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.30.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.30.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.30.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.31.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.31.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.31.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.31.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.31.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.31.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  norm.weight  requires_gradient  True size torch.Size([8192])
parameter name  norm.bias  requires_gradient  True size torch.Size([8192])
parameter name  head.0.weight  requires_gradient  True size torch.Size([8192, 8192])
parameter name  head.0.bias  requires_gradient  True size torch.Size([8192])
parameter name  head.2.weight  requires_gradient  True size torch.Size([8192, 8192])
parameter name  head.2.bias  requires_gradient  True size torch.Size([8192])
parameter name  head.5.weight  requires_gradient  True size torch.Size([8192, 8192])
parameter name  head.5.bias  requires_gradient  True size torch.Size([8192])
total_params before FSDP tensor(26351837201) params_per_gpu tensor(1109008401)
total_params after FSDP FullyShardedDataParallel(
  (_fsdp_wrapped_module): ClimaX(
    (token_embeds): ModuleList(
      (0-511): 512 x PatchEmbed(
        (proj): Conv2d(1, 8192, kernel_size=(4, 4), stride=(4, 4))
        (norm): Identity()
      )
    )
    (var_agg): FullyShardedDataParallel(
      (_fsdp_wrapped_module): CheckpointWrapper(
        (_checkpoint_wrapped_module): VariableMapping_Attention(
          (q): Linear(in_features=8192, out_features=256, bias=False)
          (kv): Linear(in_features=8192, out_features=512, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=8192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (var_linear_per_rank): Linear(in_features=16, out_features=1, bias=True)
    (lead_time_embed): Linear(in_features=1, out_features=8192, bias=True)
    (pos_drop): Dropout(p=0.1, inplace=False)
    (blocks): ModuleList(
      (0-31): 32 x FullyShardedDataParallel(
        (_fsdp_wrapped_module): CheckpointWrapper(
          (_checkpoint_wrapped_module): Block(
            (norm1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=8192, out_features=768, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=256, out_features=8192, bias=True)
              (proj_drop): Dropout(p=0.1, inplace=False)
            )
            (ls1): Identity()
            (norm2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=8192, out_features=1024, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.1, inplace=False)
              (fc2): Linear(in_features=1024, out_features=8192, bias=True)
            )
            (ls2): Identity()
          )
        )
      )
    )
    (norm): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
    (head): Sequential(
      (0): Linear(in_features=8192, out_features=8192, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=8192, out_features=8192, bias=True)
      (3): GELU(approximate='none')
      (4): Pred_Rearrange()
      (5): Linear(in_features=8192, out_features=8192, bias=True)
    )
  )
)
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
rank  31 lister_train reset: mpi-esm 9 9 9
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
rank  0 lister_train reset: mpi-esm 9 9 9
use_ddstore is : 0
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
memory stats reset, ready to track
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
epoch-train:  0 batch_idx 0 world_rank 0  loss  nan
epoch-train:  0 batch_idx 1 world_rank 0  loss  nan
epoch-train:  0 batch_idx 2 world_rank 0  loss  nan
[2025-03-12 17:06:28,011] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:06:28,012] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:06:28,012] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:06:28,012] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:06:28,013] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:06:28,013] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:06:28,013] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:06:28,014] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:06:28,014] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:06:28,014] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:06:28,015] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:06:28,015] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:06:28,015] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:06:28,015] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:06:28,015] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:06:28,016] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:06:28,016] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:06:28,016] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:06:28,016] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:06:28,016] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:06:28,017] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:06:28,020] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:06:28,021] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:06:28,023] [INFO] [profiler.py:80:start_profile] Flops profiler started
epoch-train:  0 batch_idx 3 world_rank 0  loss  nan
[2025-03-12 17:06:28,024] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:06:28,024] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:06:28,025] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:06:28,025] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:06:28,025] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:06:28,025] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:06:28,026] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:06:28,026] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:06:29,032] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:06:29,033] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:06:29,033] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:06:29,033] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:06:29,033] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:06:29,033] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:06:29,034] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:06:29,034] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:06:29,034] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:06:29,034] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:06:29,034] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:06:29,034] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:06:29,034] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:06:29,034] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:06:29,034] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:06:29,034] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:06:29,034] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:06:29,034] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:06:29,035] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:06:29,035] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:06:29,035] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:06:29,035] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:06:29,035] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:06:29,035] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:06:29,035] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:06:29,035] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:06:29,036] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:06:29,037] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:06:29,037] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:06:29,037] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:06:29,037] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:06:29,038] [INFO] [profiler.py:226:end_profile] Flops profiler finished
global rank 30: ddp rank 0 iter_start,iter_end = 0 8
global rank 30: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 6: ddp rank 0 iter_start,iter_end = 0 8
global rank 6: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 31: ddp rank 0 iter_start,iter_end = 0 8
global rank 31: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 25: ddp rank 0 iter_start,iter_end = 0 8
global rank 25: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 7: ddp rank 0 iter_start,iter_end = 0 8
global rank 7: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 17: ddp rank 0 iter_start,iter_end = 0 8
global rank 17: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 12: ddp rank 0 iter_start,iter_end = 0 8
global rank 12: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 27: ddp rank 0 iter_start,iter_end = 0 8
global rank 27: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 21: ddp rank 0 iter_start,iter_end = 0 8
global rank 21: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 2: ddp rank 0 iter_start,iter_end = 0 8
global rank 2: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 29: ddp rank 0 iter_start,iter_end = 0 8
global rank 29: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 24: ddp rank 0 iter_start,iter_end = 0 8
global rank 24: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 13: ddp rank 0 iter_start,iter_end = 0 8
global rank 13: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 14: ddp rank 0 iter_start,iter_end = 0 8
global rank 14: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 3: ddp rank 0 iter_start,iter_end = 0 8
global rank 3: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 5: ddp rank 0 iter_start,iter_end = 0 8
global rank 5: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 1: ddp rank 0 iter_start,iter_end = 0 8
global rank 1: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 19: ddp rank 0 iter_start,iter_end = 0 8
global rank 19: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 11: ddp rank 0 iter_start,iter_end = 0 8
global rank 11: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 16: ddp rank 0 iter_start,iter_end = 0 8
global rank 16: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 26: ddp rank 0 iter_start,iter_end = 0 8
global rank 26: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 28: ddp rank 0 iter_start,iter_end = 0 8
global rank 28: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 20: ddp rank 0 iter_start,iter_end = 0 8
global rank 20: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 9: ddp rank 0 iter_start,iter_end = 0 8
global rank 9: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 0: ddp rank 0 iter_start,iter_end = 0 8
global rank 0: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 22: ddp rank 0 iter_start,iter_end = 0 8
global rank 22: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 23: ddp rank 0 iter_start,iter_end = 0 8
global rank 23: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 18: ddp rank 0 iter_start,iter_end = 0 8
global rank 18: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 4: ddp rank 0 iter_start,iter_end = 0 8
global rank 4: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 15: ddp rank 0 iter_start,iter_end = 0 8
global rank 15: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 8: ddp rank 0 iter_start,iter_end = 0 8
global rank 8: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 10: ddp rank 0 iter_start,iter_end = 0 8
global rank 10: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
MUST: Model 1109.008401 M TFLOPS: 14.372312115793127

--> cuda max reserved memory = 36.1172
--> max reserved percentage = 56.45 %

--> cuda max memory allocated = 25.6533
--> max allocated percentage = 40.09 %

--> peak active memory = 33.1876
--> peak active memory 51.87 %

cudaMalloc retries = 0
cuda OOM = 0

HERE1 Namespace(arch='orbit_linear', batch_size=2, channels=512, depth=32, embed_dim=8192, fa2=True, fsdp_size=1, imagex=128, imagey=256, max_epochs=1, num_heads=32, real=False, seq_par_size=1, simple_ddp_size=1, tensor_par_size=32, yaml_config='configs/ERA5-100million-91variables.yaml') 36.1172 14.372312115793127 tensor(26351837201)
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:06:43,753] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:06:43,753] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:06:43,753] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:06:43,753] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:06:43,753] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:06:43,753] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:06:43,753] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:06:43,753] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:06:43,759] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:06:43,759] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:06:43,759] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:06:43,759] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:06:43,759] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:06:43,759] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:06:43,759] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:06:43,759] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:06:43,759] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:06:43,759] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:06:43,759] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:06:43,759] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:06:43,759] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:06:43,759] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:06:43,759] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:06:43,759] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:06:43,761] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:06:43,761] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:06:43,761] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:06:43,761] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:06:43,761] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:06:43,761] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:06:43,761] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:06:43,761] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
HERE0 Namespace(arch='orbit_linear', batch_size=2, channels=768, depth=32, embed_dim=8192, fa2=True, fsdp_size=1, imagex=128, imagey=256, max_epochs=1, num_heads=32, real=False, seq_par_size=1, simple_ddp_size=1, tensor_par_size=32, yaml_config='configs/ERA5-100million-91variables.yaml')
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
rank 16 Before initialize parallelism groups
rank 23 Before initialize parallelism groups
Using dist.init_process_group. world_size  32
rank 20 Before initialize parallelism groups
rank 17 Before initialize parallelism groups
rank 31 Before initialize parallelism groups
rank 25 Before initialize parallelism groups
rank 2 Before initialize parallelism groups
rank 5 Before initialize parallelism groups
rank 21 Before initialize parallelism groups
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
rank 9 Before initialize parallelism groups
rank 8 Before initialize parallelism groups
Using dist.init_process_group. world_size  32
rank 15 Before initialize parallelism groups
rank 12 Before initialize parallelism groups
rank 3 Before initialize parallelism groups
rank 11 Before initialize parallelism groups
rank 26 Before initialize parallelism groups
Using dist.init_process_group. world_size  32
config_path  configs/ERA5-100million-91variables.yaml
rank 4 Before initialize parallelism groups
rank 27 Before initialize parallelism groups
rank 24 Before initialize parallelism groups
rank 14 Before initialize parallelism groups
rank 23 After initialize parallelism groups
rank 16 After initialize parallelism groups
rank 17 After initialize parallelism groups
rank 20 After initialize parallelism groups
rank 21 After initialize parallelism groups
rank 31 After initialize parallelism groups
rank 25 After initialize parallelism groups
rank 2 After initialize parallelism groups
rank 5 After initialize parallelism groups
rank 9 After initialize parallelism groups
rank 8 After initialize parallelism groups
rank 3 After initialize parallelism groups
rank 11 After initialize parallelism groups
rank 1 Before initialize parallelism groups
rank 15 After initialize parallelism groups
rank 12 After initialize parallelism groups
rank 26 After initialize parallelism groups
rank 18 Before initialize parallelism groups
rank 7 Before initialize parallelism groups
rank 4 After initialize parallelism groups
rank 28 Before initialize parallelism groups
rank 27 After initialize parallelism groups
rank 24 After initialize parallelism groups
rank 6 Before initialize parallelism groups
rank 14 After initialize parallelism groups
rank 22 Before initialize parallelism groups
rank 29 Before initialize parallelism groups
rank 19 Before initialize parallelism groups
rank 30 Before initialize parallelism groups
rank 13 Before initialize parallelism groups
{'seed_everything': 42, 'trainer': {'checkpoint_path': '/lustre/orion/stf218/world-shared/atsaris/tmp/tmp_checkpoints', 'checkpoint_filename': 'multi_last', 'resume_from_checkpoint': False}, 'parallelism': {'cpu_offloading': False}, 'model': {'lr': 2e-05, 'beta_1': 0.9, 'beta_2': 0.95, 'weight_decay': '1e-5', 'warmup_steps': 1000, 'max_steps': 20000, 'warmup_start_lr': '1e-8', 'eta_min': '1e-8', 'net': {'class_path': 'climax.arch.ClimaX', 'init_args': {'default_vars': ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_70', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000'], 'patch_size': 4, 'decoder_depth': 2, 'mlp_ratio': 4, 'drop_path': 0.1, 'drop_rate': 0.1, 'aggregated_variables': 1}}}, 'data': {'dict_root_dirs': {'mpi-esm': '/lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0'}, 'dict_start_idx': {'mpi-esm': 0}, 'dict_end_idx': {'mpi-esm': 1}, 'dict_in_variables': {'mpi-esm': ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_2rank 1 After initialize parallelism groups
00', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000']}, 'dict_out_variables': {'mpi-esm': ['geopotential_500', 'temperature_850', '2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind']}, 'dict_max_predict_ranges': {'mpi-esm': 24}, 'dict_random_lead_time': {'mpi-esm': False}, 'dict_hrs_each_step': {'mpi-esm': 1}, 'dict_buffer_sizes': {'mpi-esm': 1}, 'num_workers': 1, 'pin_memory': False}}
rank 7 After initialize parallelism groups
rank 18 After initialize parallelism groups
rank 10 Before initialize parallelism groups
rank 28 After initialize parallelism groups
rank 6 After initialize parallelism groups
rank 22 After initialize parallelism groups
max_epochs 1 data_par_size 1 fsdp_size 1 simple_ddp_size 1 tensor_par_size 32 seq_par_size 1 cpu_offloading False
lr  2e-05 beta_1  0.9 beta_2 0.95 weight_decay 1e-05 class_path climax.arch.ClimaX default_vars ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_70', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000']
img_size_x 128 img_size_y 256 patch_size 4 emb_dim 8192 depth 32 decoder_depth 2 num_heads 32 mlp_ratio 4 drop_path 0.1 drop_rate 0.1 aggregated_variables 1
dict_root_dirs {'mpi-esm': '/lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0'} dict_start_idx {'mpi-esm': 0} dict_end_idx {'mpi-esm': 1} batch_size 2 num_workers 1
warmup_steps 1000 max_steps 20000 warmup_start_lr 1e-08 eta_min 1e-08
checkpoint_path /lustre/orion/stf218/world-shared/atsaris/tmp/tmp_checkpoints checkpoint_filename multi_last resume_from_checkpoint False
rank 0 Before initialize parallelism groups
rank 29 After initialize parallelism groups
rank 19 After initialize parallelism groups
rank 30 After initialize parallelism groups
rank 13 After initialize parallelism groups
rank 10 After initialize parallelism groups
rank 0 After initialize parallelism groups
rank 5 After initialize model
rank 5 Before the second dist.barrier()
rank 15 After initialize model
rank 15 Before the second dist.barrier()
rank 19 After initialize model
rank 19 Before the second dist.barrier()
rank 20 After initialize model
rank 20 Before the second dist.barrier()
rank 7 After initialize model
rank 7 Before the second dist.barrier()
rank 24 After initialize model
rank 24 Before the second dist.barrier()
rank 13 After initialize model
rank 13 Before the second dist.barrier()
rank 8 After initialize model
rank 8 Before the second dist.barrier()
rank 18 After initialize model
rank 18 Before the second dist.barrier()
rank 14 After initialize model
rank 14 Before the second dist.barrier()
rank 22 After initialize model
rank 22 Before the second dist.barrier()
rank 23 After initialize model
rank 23 Before the second dist.barrier()
rank 9 After initialize model
rank 9 Before the second dist.barrier()
rank 31 After initialize model
rank 31 Before the second dist.barrier()
rank 10 After initialize model
rank 10 Before the second dist.barrier()
rank 21 After initialize model
rank 21 Before the second dist.barrier()
rank 29 After initialize model
rank 29 Before the second dist.barrier()
rank 16 After initialize model
rank 16 Before the second dist.barrier()
rank 12 After initialize model
rank 12 Before the second dist.barrier()
rank 11 After initialize model
rank 11 Before the second dist.barrier()
rank 28 After initialize model
rank 28 Before the second dist.barrier()
rank 4 After initialize model
rank 4 Before the second dist.barrier()
rank 6 After initialize model
rank 6 Before the second dist.barrier()
rank 3 After initialize model
rank 3 Before the second dist.barrier()
rank 2 After initialize model
rank 2 Before the second dist.barrier()
rank 17 After initialize model
rank 17 Before the second dist.barrier()
rank 1 After initialize model
rank 1 Before the second dist.barrier()
rank 0 After initialize model
rank 0 Before the second dist.barrier()
rank 30 After initialize model
rank 30 Before the second dist.barrier()
rank 26 After initialize model
rank 26 Before the second dist.barrier()
rank 25 After initialize model
rank 25 Before the second dist.barrier()
rank 27 After initialize model
rank 27 Before the second dist.barrier()
rank 1 After the second dist.barrier()
rank 9 After the second dist.barrier()
rank 12 After the second dist.barrier()
rank 8 After the second dist.barrier()
rank 13 After the second dist.barrier()
rank 16 After the second dist.barrier()
rank 17 After the second dist.barrier()
rank 0 After the second dist.barrier()
rank 21 After the second dist.barrier()
rank 4 After the second dist.barrier()
rank 5 After the second dist.barrier()
rank 14 After the second dist.barrier()
rank 15 After the second dist.barrier()
rank 20 After the second dist.barrier()
rank 10 After the second dist.barrier()
rank 24 After the second dist.barrier()
parameter name  var_embed  requires_gradient  True size torch.Size([1, 768, 8192])
rank 11 After the second dist.barrier()
rank 25 After the second dist.barrier()
rank 28 After the second dist.barrier()
rank 29 After the second dist.barrier()
rank 6 After the second dist.barrier()
rank 22 After the second dist.barrier()
rank 23 After the second dist.barrier()
rank 7 After the second dist.barrier()
rank 30 After the second dist.barrier()
parameter name  var_query  requires_gradient  True size torch.Size([1, 1, 8192])
parameter name  var_query_per_rank  requires_gradient  True size torch.Size([1, 1, 8192])
parameter name  pos_embed  requires_gradient  True size torch.Size([1, 2048, 8192])
rank 2 After the second dist.barrier()
rank 19 After the second dist.barrier()
rank 31 After the second dist.barrier()
rank 3 After the second dist.barrier()
parameter name  token_embeds.0.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.0.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.1.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.1.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.2.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.2.proj.bias  requires_gradient  True size torch.Size([8192])
rank 18 After the second dist.barrier()
rank 27 After the second dist.barrier()
parameter name  token_embeds.3.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.3.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.4.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.4.proj.bias  requires_gradient  True size torch.Size([8192])
rank 26 After the second dist.barrier()
parameter name  token_embeds.5.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.5.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.6.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.6.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.7.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.7.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.8.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.8.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.9.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.9.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.10.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.10.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.11.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.11.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.12.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.12.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.13.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.13.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.14.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.14.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.15.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.15.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.16.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.16.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.17.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.17.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.18.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.18.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.19.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.19.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.20.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.20.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.21.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.21.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.22.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.22.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.23.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.23.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.24.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.24.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.25.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.25.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.26.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.26.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.27.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.27.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.28.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.28.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.29.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.29.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.30.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.30.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.31.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.31.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.32.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.32.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.33.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.33.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.34.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.34.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.35.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.35.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.36.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.36.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.37.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.37.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.38.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.38.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.39.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.39.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.40.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.40.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.41.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.41.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.42.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.42.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.43.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.43.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.44.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.44.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.45.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.45.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.46.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.46.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.47.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.47.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.48.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.48.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.49.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.49.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.50.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.50.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.51.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.51.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.52.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.52.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.53.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.53.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.54.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.54.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.55.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.55.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.56.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.56.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.57.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.57.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.58.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.58.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.59.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.59.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.60.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.60.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.61.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.61.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.62.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.62.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.63.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.63.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.64.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.64.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.65.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.65.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.66.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.66.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.67.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.67.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.68.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.68.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.69.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.69.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.70.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.70.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.71.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.71.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.72.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.72.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.73.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.73.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.74.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.74.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.75.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.75.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.76.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.76.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.77.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.77.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.78.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.78.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.79.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.79.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.80.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.80.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.81.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.81.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.82.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.82.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.83.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.83.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.84.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.84.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.85.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.85.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.86.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.86.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.87.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.87.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.88.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.88.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.89.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.89.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.90.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.90.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.91.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.91.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.92.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.92.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.93.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.93.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.94.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.94.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.95.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.95.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.96.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.96.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.97.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.97.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.98.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.98.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.99.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.99.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.100.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.100.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.101.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.101.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.102.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.102.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.103.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.103.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.104.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.104.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.105.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.105.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.106.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.106.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.107.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.107.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.108.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.108.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.109.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.109.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.110.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.110.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.111.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.111.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.112.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.112.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.113.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.113.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.114.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.114.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.115.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.115.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.116.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.116.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.117.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.117.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.118.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.118.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.119.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.119.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.120.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.120.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.121.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.121.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.122.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.122.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.123.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.123.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.124.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.124.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.125.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.125.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.126.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.126.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.127.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.127.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.128.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.128.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.129.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.129.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.130.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.130.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.131.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.131.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.132.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.132.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.133.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.133.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.134.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.134.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.135.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.135.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.136.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.136.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.137.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.137.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.138.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.138.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.139.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.139.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.140.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.140.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.141.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.141.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.142.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.142.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.143.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.143.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.144.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.144.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.145.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.145.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.146.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.146.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.147.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.147.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.148.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.148.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.149.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.149.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.150.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.150.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.151.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.151.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.152.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.152.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.153.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.153.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.154.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.154.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.155.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.155.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.156.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.156.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.157.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.157.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.158.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.158.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.159.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.159.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.160.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.160.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.161.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.161.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.162.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.162.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.163.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.163.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.164.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.164.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.165.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.165.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.166.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.166.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.167.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.167.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.168.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.168.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.169.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.169.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.170.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.170.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.171.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.171.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.172.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.172.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.173.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.173.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.174.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.174.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.175.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.175.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.176.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.176.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.177.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.177.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.178.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.178.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.179.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.179.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.180.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.180.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.181.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.181.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.182.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.182.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.183.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.183.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.184.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.184.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.185.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.185.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.186.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.186.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.187.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.187.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.188.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.188.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.189.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.189.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.190.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.190.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.191.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.191.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.192.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.192.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.193.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.193.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.194.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.194.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.195.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.195.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.196.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.196.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.197.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.197.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.198.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.198.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.199.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.199.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.200.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.200.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.201.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.201.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.202.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.202.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.203.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.203.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.204.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.204.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.205.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.205.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.206.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.206.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.207.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.207.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.208.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.208.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.209.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.209.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.210.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.210.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.211.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.211.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.212.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.212.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.213.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.213.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.214.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.214.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.215.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.215.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.216.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.216.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.217.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.217.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.218.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.218.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.219.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.219.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.220.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.220.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.221.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.221.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.222.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.222.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.223.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.223.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.224.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.224.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.225.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.225.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.226.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.226.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.227.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.227.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.228.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.228.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.229.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.229.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.230.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.230.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.231.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.231.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.232.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.232.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.233.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.233.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.234.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.234.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.235.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.235.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.236.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.236.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.237.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.237.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.238.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.238.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.239.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.239.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.240.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.240.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.241.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.241.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.242.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.242.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.243.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.243.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.244.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.244.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.245.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.245.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.246.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.246.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.247.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.247.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.248.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.248.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.249.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.249.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.250.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.250.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.251.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.251.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.252.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.252.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.253.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.253.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.254.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.254.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.255.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.255.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.256.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.256.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.257.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.257.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.258.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.258.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.259.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.259.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.260.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.260.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.261.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.261.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.262.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.262.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.263.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.263.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.264.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.264.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.265.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.265.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.266.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.266.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.267.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.267.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.268.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.268.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.269.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.269.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.270.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.270.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.271.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.271.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.272.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.272.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.273.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.273.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.274.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.274.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.275.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.275.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.276.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.276.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.277.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.277.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.278.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.278.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.279.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.279.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.280.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.280.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.281.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.281.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.282.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.282.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.283.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.283.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.284.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.284.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.285.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.285.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.286.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.286.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.287.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.287.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.288.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.288.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.289.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.289.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.290.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.290.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.291.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.291.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.292.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.292.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.293.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.293.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.294.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.294.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.295.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.295.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.296.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.296.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.297.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.297.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.298.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.298.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.299.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.299.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.300.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.300.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.301.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.301.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.302.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.302.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.303.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.303.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.304.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.304.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.305.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.305.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.306.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.306.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.307.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.307.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.308.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.308.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.309.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.309.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.310.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.310.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.311.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.311.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.312.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.312.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.313.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.313.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.314.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.314.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.315.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.315.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.316.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.316.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.317.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.317.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.318.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.318.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.319.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.319.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.320.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.320.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.321.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.321.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.322.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.322.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.323.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.323.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.324.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.324.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.325.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.325.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.326.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.326.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.327.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.327.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.328.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.328.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.329.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.329.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.330.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.330.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.331.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.331.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.332.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.332.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.333.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.333.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.334.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.334.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.335.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.335.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.336.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.336.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.337.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.337.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.338.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.338.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.339.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.339.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.340.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.340.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.341.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.341.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.342.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.342.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.343.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.343.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.344.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.344.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.345.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.345.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.346.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.346.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.347.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.347.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.348.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.348.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.349.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.349.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.350.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.350.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.351.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.351.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.352.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.352.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.353.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.353.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.354.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.354.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.355.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.355.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.356.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.356.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.357.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.357.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.358.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.358.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.359.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.359.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.360.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.360.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.361.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.361.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.362.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.362.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.363.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.363.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.364.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.364.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.365.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.365.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.366.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.366.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.367.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.367.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.368.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.368.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.369.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.369.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.370.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.370.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.371.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.371.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.372.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.372.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.373.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.373.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.374.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.374.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.375.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.375.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.376.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.376.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.377.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.377.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.378.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.378.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.379.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.379.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.380.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.380.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.381.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.381.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.382.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.382.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.383.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.383.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.384.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.384.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.385.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.385.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.386.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.386.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.387.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.387.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.388.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.388.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.389.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.389.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.390.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.390.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.391.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.391.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.392.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.392.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.393.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.393.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.394.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.394.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.395.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.395.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.396.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.396.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.397.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.397.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.398.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.398.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.399.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.399.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.400.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.400.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.401.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.401.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.402.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.402.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.403.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.403.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.404.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.404.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.405.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.405.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.406.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.406.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.407.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.407.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.408.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.408.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.409.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.409.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.410.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.410.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.411.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.411.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.412.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.412.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.413.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.413.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.414.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.414.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.415.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.415.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.416.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.416.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.417.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.417.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.418.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.418.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.419.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.419.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.420.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.420.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.421.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.421.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.422.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.422.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.423.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.423.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.424.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.424.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.425.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.425.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.426.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.426.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.427.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.427.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.428.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.428.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.429.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.429.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.430.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.430.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.431.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.431.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.432.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.432.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.433.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.433.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.434.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.434.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.435.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.435.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.436.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.436.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.437.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.437.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.438.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.438.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.439.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.439.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.440.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.440.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.441.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.441.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.442.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.442.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.443.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.443.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.444.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.444.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.445.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.445.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.446.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.446.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.447.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.447.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.448.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.448.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.449.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.449.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.450.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.450.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.451.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.451.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.452.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.452.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.453.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.453.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.454.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.454.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.455.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.455.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.456.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.456.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.457.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.457.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.458.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.458.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.459.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.459.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.460.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.460.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.461.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.461.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.462.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.462.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.463.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.463.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.464.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.464.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.465.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.465.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.466.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.466.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.467.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.467.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.468.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.468.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.469.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.469.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.470.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.470.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.471.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.471.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.472.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.472.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.473.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.473.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.474.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.474.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.475.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.475.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.476.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.476.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.477.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.477.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.478.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.478.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.479.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.479.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.480.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.480.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.481.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.481.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.482.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.482.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.483.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.483.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.484.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.484.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.485.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.485.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.486.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.486.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.487.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.487.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.488.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.488.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.489.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.489.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.490.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.490.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.491.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.491.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.492.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.492.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.493.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.493.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.494.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.494.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.495.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.495.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.496.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.496.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.497.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.497.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.498.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.498.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.499.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.499.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.500.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.500.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.501.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.501.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.502.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.502.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.503.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.503.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.504.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.504.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.505.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.505.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.506.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.506.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.507.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.507.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.508.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.508.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.509.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.509.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.510.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.510.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.511.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.511.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.512.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.512.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.513.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.513.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.514.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.514.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.515.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.515.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.516.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.516.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.517.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.517.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.518.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.518.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.519.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.519.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.520.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.520.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.521.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.521.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.522.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.522.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.523.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.523.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.524.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.524.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.525.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.525.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.526.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.526.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.527.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.527.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.528.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.528.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.529.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.529.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.530.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.530.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.531.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.531.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.532.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.532.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.533.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.533.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.534.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.534.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.535.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.535.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.536.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.536.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.537.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.537.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.538.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.538.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.539.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.539.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.540.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.540.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.541.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.541.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.542.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.542.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.543.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.543.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.544.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.544.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.545.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.545.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.546.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.546.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.547.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.547.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.548.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.548.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.549.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.549.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.550.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.550.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.551.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.551.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.552.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.552.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.553.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.553.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.554.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.554.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.555.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.555.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.556.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.556.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.557.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.557.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.558.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.558.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.559.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.559.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.560.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.560.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.561.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.561.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.562.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.562.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.563.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.563.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.564.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.564.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.565.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.565.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.566.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.566.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.567.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.567.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.568.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.568.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.569.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.569.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.570.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.570.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.571.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.571.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.572.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.572.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.573.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.573.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.574.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.574.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.575.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.575.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.576.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.576.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.577.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.577.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.578.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.578.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.579.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.579.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.580.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.580.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.581.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.581.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.582.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.582.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.583.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.583.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.584.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.584.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.585.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.585.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.586.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.586.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.587.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.587.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.588.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.588.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.589.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.589.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.590.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.590.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.591.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.591.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.592.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.592.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.593.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.593.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.594.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.594.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.595.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.595.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.596.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.596.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.597.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.597.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.598.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.598.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.599.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.599.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.600.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.600.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.601.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.601.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.602.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.602.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.603.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.603.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.604.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.604.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.605.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.605.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.606.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.606.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.607.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.607.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.608.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.608.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.609.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.609.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.610.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.610.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.611.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.611.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.612.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.612.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.613.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.613.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.614.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.614.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.615.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.615.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.616.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.616.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.617.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.617.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.618.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.618.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.619.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.619.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.620.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.620.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.621.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.621.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.622.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.622.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.623.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.623.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.624.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.624.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.625.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.625.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.626.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.626.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.627.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.627.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.628.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.628.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.629.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.629.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.630.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.630.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.631.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.631.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.632.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.632.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.633.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.633.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.634.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.634.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.635.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.635.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.636.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.636.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.637.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.637.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.638.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.638.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.639.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.639.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.640.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.640.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.641.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.641.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.642.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.642.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.643.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.643.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.644.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.644.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.645.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.645.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.646.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.646.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.647.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.647.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.648.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.648.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.649.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.649.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.650.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.650.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.651.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.651.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.652.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.652.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.653.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.653.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.654.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.654.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.655.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.655.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.656.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.656.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.657.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.657.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.658.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.658.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.659.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.659.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.660.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.660.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.661.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.661.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.662.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.662.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.663.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.663.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.664.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.664.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.665.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.665.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.666.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.666.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.667.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.667.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.668.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.668.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.669.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.669.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.670.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.670.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.671.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.671.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.672.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.672.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.673.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.673.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.674.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.674.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.675.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.675.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.676.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.676.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.677.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.677.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.678.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.678.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.679.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.679.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.680.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.680.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.681.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.681.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.682.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.682.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.683.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.683.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.684.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.684.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.685.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.685.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.686.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.686.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.687.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.687.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.688.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.688.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.689.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.689.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.690.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.690.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.691.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.691.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.692.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.692.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.693.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.693.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.694.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.694.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.695.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.695.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.696.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.696.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.697.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.697.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.698.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.698.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.699.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.699.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.700.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.700.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.701.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.701.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.702.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.702.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.703.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.703.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.704.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.704.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.705.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.705.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.706.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.706.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.707.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.707.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.708.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.708.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.709.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.709.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.710.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.710.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.711.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.711.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.712.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.712.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.713.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.713.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.714.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.714.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.715.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.715.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.716.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.716.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.717.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.717.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.718.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.718.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.719.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.719.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.720.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.720.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.721.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.721.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.722.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.722.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.723.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.723.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.724.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.724.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.725.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.725.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.726.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.726.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.727.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.727.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.728.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.728.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.729.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.729.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.730.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.730.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.731.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.731.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.732.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.732.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.733.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.733.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.734.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.734.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.735.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.735.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.736.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.736.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.737.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.737.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.738.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.738.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.739.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.739.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.740.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.740.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.741.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.741.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.742.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.742.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.743.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.743.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.744.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.744.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.745.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.745.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.746.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.746.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.747.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.747.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.748.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.748.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.749.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.749.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.750.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.750.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.751.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.751.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.752.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.752.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.753.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.753.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.754.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.754.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.755.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.755.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.756.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.756.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.757.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.757.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.758.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.758.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.759.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.759.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.760.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.760.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.761.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.761.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.762.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.762.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.763.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.763.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.764.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.764.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.765.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.765.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.766.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.766.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.767.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.767.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  var_agg.q.weight  requires_gradient  True size torch.Size([256, 8192])
parameter name  var_agg.kv.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  var_agg.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  var_agg.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  var_linear_per_rank.weight  requires_gradient  True size torch.Size([1, 24])
parameter name  var_linear_per_rank.bias  requires_gradient  True size torch.Size([1])
parameter name  lead_time_embed.weight  requires_gradient  True size torch.Size([8192, 1])
parameter name  lead_time_embed.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.0.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.0.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.0.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.0.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.0.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.0.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.1.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.1.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.1.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.1.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.1.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.1.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.2.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.2.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.2.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.2.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.2.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.2.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.3.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.3.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.3.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.3.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.3.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.3.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.4.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.4.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.4.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.4.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.4.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.4.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.5.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.5.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.5.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.5.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.5.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.5.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.6.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.6.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.6.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.6.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.6.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.6.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.7.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.7.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.7.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.7.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.7.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.7.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.8.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.8.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.8.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.8.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.8.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.8.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.9.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.9.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.9.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.9.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.9.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.9.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.10.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.10.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.10.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.10.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.10.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.10.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.11.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.11.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.11.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.11.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.11.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.11.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.12.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.12.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.12.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.12.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.12.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.12.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.13.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.13.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.13.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.13.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.13.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.13.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.14.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.14.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.14.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.14.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.14.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.14.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.15.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.15.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.15.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.15.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.15.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.15.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.16.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.16.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.16.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.16.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.16.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.16.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.17.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.17.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.17.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.17.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.17.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.17.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.18.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.18.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.18.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.18.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.18.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.18.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.19.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.19.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.19.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.19.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.19.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.19.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.20.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.20.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.20.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.20.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.20.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.20.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.21.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.21.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.21.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.21.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.21.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.21.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.22.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.22.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.22.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.22.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.22.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.22.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.23.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.23.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.23.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.23.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.23.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.23.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.24.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.24.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.24.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.24.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.24.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.24.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.25.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.25.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.25.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.25.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.25.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.25.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.26.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.26.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.26.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.26.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.26.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.26.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.27.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.27.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.27.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.27.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.27.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.27.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.28.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.28.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.28.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.28.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.28.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.28.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.29.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.29.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.29.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.29.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.29.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.29.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.30.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.30.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.30.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.30.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.30.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.30.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.31.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.31.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.31.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.31.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.31.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.31.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  norm.weight  requires_gradient  True size torch.Size([8192])
parameter name  norm.bias  requires_gradient  True size torch.Size([8192])
parameter name  head.0.weight  requires_gradient  True size torch.Size([8192, 8192])
parameter name  head.0.bias  requires_gradient  True size torch.Size([8192])
parameter name  head.2.weight  requires_gradient  True size torch.Size([8192, 8192])
parameter name  head.2.bias  requires_gradient  True size torch.Size([8192])
parameter name  head.5.weight  requires_gradient  True size torch.Size([12288, 8192])
parameter name  head.5.bias  requires_gradient  True size torch.Size([12288])
total_params before FSDP tensor(26423144473) params_per_gpu tensor(1180315673)
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
total_params after FSDP FullyShardedDataParallel(
  (_fsdp_wrapped_module): ClimaX(
    (token_embeds): ModuleList(
      (0-767): 768 x PatchEmbed(
        (proj): Conv2d(1, 8192, kernel_size=(4, 4), stride=(4, 4))
        (norm): Identity()
      )
    )
    (var_agg): FullyShardedDataParallel(
      (_fsdp_wrapped_module): CheckpointWrapper(
        (_checkpoint_wrapped_module): VariableMapping_Attention(
          (q): Linear(in_features=8192, out_features=256, bias=False)
          (kv): Linear(in_features=8192, out_features=512, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=8192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (var_linear_per_rank): Linear(in_features=24, out_features=1, bias=True)
    (lead_time_embed): Linear(in_features=1, out_features=8192, bias=True)
    (pos_drop): Dropout(p=0.1, inplace=False)
    (blocks): ModuleList(
      (0-31): 32 x FullyShardedDataParallel(
        (_fsdp_wrapped_module): CheckpointWrapper(
          (_checkpoint_wrapped_module): Block(
            (norm1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=8192, out_features=768, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=256, out_features=8192, bias=True)
              (proj_drop): Dropout(p=0.1, inplace=False)
            )
            (ls1): Identity()
            (norm2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=8192, out_features=1024, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.1, inplace=False)
              (fc2): Linear(in_features=1024, out_features=8192, bias=True)
            )
            (ls2): Identity()
          )
        )
      )
    )
    (norm): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
    (head): Sequential(
      (0): Linear(in_features=8192, out_features=8192, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=8192, out_features=8192, bias=True)
      (3): GELU(approximate='none')
      (4): Pred_Rearrange()
      (5): Linear(in_features=8192, out_features=12288, bias=True)
    )
  )
)
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
rank  0 lister_train reset: mpi-esm 9 9 9
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
rank  31 lister_train reset: mpi-esm 9 9 9
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
memory stats reset, ready to track
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
epoch-train:  0 batch_idx 0 world_rank 0  loss  nan
epoch-train:  0 batch_idx 1 world_rank 0  loss  nan
epoch-train:  0 batch_idx 2 world_rank 0  loss  nan
[2025-03-12 17:07:32,809] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:07:32,810] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:07:32,810] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:07:32,810] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:07:32,810] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:07:32,810] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:07:32,811] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:07:32,811] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:07:32,811] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:07:32,812] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:07:32,813] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:07:32,813] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:07:32,813] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:07:32,813] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:07:32,813] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:07:32,814] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:07:32,814] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:07:32,814] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:07:32,815] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:07:32,815] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:07:32,815] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:07:32,816] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:07:32,816] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:07:32,818] [INFO] [profiler.py:80:start_profile] Flops profiler started
epoch-train:  0 batch_idx 3 world_rank 0  loss  nan
[2025-03-12 17:07:32,823] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:07:32,824] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:07:32,824] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:07:32,824] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:07:32,824] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:07:32,825] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:07:32,825] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:07:32,825] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:07:34,058] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:07:34,058] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:07:34,059] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:07:34,059] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:07:34,059] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:07:34,059] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:07:34,059] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:07:34,059] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:07:34,060] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:07:34,060] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:07:34,060] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:07:34,060] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:07:34,060] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:07:34,060] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:07:34,060] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:07:34,061] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:07:34,061] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:07:34,061] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:07:34,061] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:07:34,061] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:07:34,062] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:07:34,062] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:07:34,062] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:07:34,062] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:07:34,063] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:07:34,063] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:07:34,063] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:07:34,063] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:07:34,064] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:07:34,064] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:07:34,065] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:07:34,065] [INFO] [profiler.py:226:end_profile] Flops profiler finished
global rank 25: ddp rank 0 iter_start,iter_end = 0 8
global rank 25: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 14: ddp rank 0 iter_start,iter_end = 0 8
global rank 14: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 5: ddp rank 0 iter_start,iter_end = 0 8
global rank 5: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 6: ddp rank 0 iter_start,iter_end = 0 8
global rank 6: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 22: ddp rank 0 iter_start,iter_end = 0 8
global rank 22: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 7: ddp rank 0 iter_start,iter_end = 0 8
global rank 7: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 23: ddp rank 0 iter_start,iter_end = 0 8
global rank 23: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 9: ddp rank 0 iter_start,iter_end = 0 8
global rank 9: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 13: ddp rank 0 iter_start,iter_end = 0 8
global rank 13: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 24: ddp rank 0 iter_start,iter_end = 0 8
global rank 24: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 18: ddp rank 0 iter_start,iter_end = 0 8
global rank 18: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 27: ddp rank 0 iter_start,iter_end = 0 8
global rank 27: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 4: ddp rank 0 iter_start,iter_end = 0 8
global rank 4: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 28: ddp rank 0 iter_start,iter_end = 0 8
global rank 28: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 1: ddp rank 0 iter_start,iter_end = 0 8
global rank 1: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 3: ddp rank 0 iter_start,iter_end = 0 8
global rank 3: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 15: ddp rank 0 iter_start,iter_end = 0 8
global rank 15: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 31: ddp rank 0 iter_start,iter_end = 0 8
global rank 31: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 17: ddp rank 0 iter_start,iter_end = 0 8
global rank 17: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 20: ddp rank 0 iter_start,iter_end = 0 8
global rank 20: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 16: ddp rank 0 iter_start,iter_end = 0 8
global rank 16: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 19: ddp rank 0 iter_start,iter_end = 0 8
global rank 19: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 30: ddp rank 0 iter_start,iter_end = 0 8
global rank 30: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 26: ddp rank 0 iter_start,iter_end = 0 8
global rank 26: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 12: ddp rank 0 iter_start,iter_end = 0 8
global rank 12: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 29: ddp rank 0 iter_start,iter_end = 0 8
global rank 29: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 8: ddp rank 0 iter_start,iter_end = 0 8
global rank 8: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 11: ddp rank 0 iter_start,iter_end = 0 8
global rank 11: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 21: ddp rank 0 iter_start,iter_end = 0 8
global rank 21: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 0: ddp rank 0 iter_start,iter_end = 0 8
global rank 0: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 10: ddp rank 0 iter_start,iter_end = 0 8
global rank 10: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 2: ddp rank 0 iter_start,iter_end = 0 8
global rank 2: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
MUST: Model 1180.315673 M TFLOPS: 13.840109180133815

--> cuda max reserved memory = 38.7227
--> max reserved percentage = 60.52 %

--> cuda max memory allocated = 27.9426
--> max allocated percentage = 43.67 %

--> peak active memory = 35.4768
--> peak active memory 55.45 %

cudaMalloc retries = 0
cuda OOM = 0

HERE1 Namespace(arch='orbit_linear', batch_size=2, channels=768, depth=32, embed_dim=8192, fa2=True, fsdp_size=1, imagex=128, imagey=256, max_epochs=1, num_heads=32, real=False, seq_par_size=1, simple_ddp_size=1, tensor_par_size=32, yaml_config='configs/ERA5-100million-91variables.yaml') 38.7227 13.840109180133815 tensor(26423144473)
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:07:48,678] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:07:48,678] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:07:48,678] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:07:48,678] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:07:48,678] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:07:48,678] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:07:48,678] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:07:48,678] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:07:48,755] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:07:48,755] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:07:48,755] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:07:48,755] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:07:48,755] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:07:48,755] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:07:48,755] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:07:48,755] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:07:48,755] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:07:48,755] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:07:48,755] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:07:48,755] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:07:48,755] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:07:48,755] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:07:48,755] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:07:48,755] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:07:48,850] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:07:48,850] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:07:48,850] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:07:48,850] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:07:48,850] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:07:48,850] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:07:48,850] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:07:48,850] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
HERE0 Namespace(arch='orbit_hier', batch_size=2, channels=256, depth=32, embed_dim=8192, fa2=True, fsdp_size=1, imagex=128, imagey=256, max_epochs=1, num_heads=32, real=False, seq_par_size=1, simple_ddp_size=1, tensor_par_size=32, yaml_config='configs/ERA5-100million-91variables.yaml')
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
rank 1 Before initialize parallelism groups
rank 3 Before initialize parallelism groups
rank 4 Before initialize parallelism groups
rank 2 Before initialize parallelism groups
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
rank 7 Before initialize parallelism groups
rank 2 After initialize parallelism groups
rank 4 After initialize parallelism groups
rank 3 After initialize parallelism groups
rank 1 After initialize parallelism groups
rank 18 Before initialize parallelism groups
rank 22 Before initialize parallelism groups
rank 23 Before initialize parallelism groups
rank 7 After initialize parallelism groups
rank 5 Before initialize parallelism groups
rank 16 Before initialize parallelism groups
rank 17 Before initialize parallelism groups
rank 19 Before initialize parallelism groups
rank 18 After initialize parallelism groups
rank 23 After initialize parallelism groups
rank 22 After initialize parallelism groups
rank 21 Before initialize parallelism groups
rank 5 After initialize parallelism groups
rank 20 Before initialize parallelism groups
rank 6 Before initialize parallelism groups
rank 17 After initialize parallelism groups
rank 16 After initialize parallelism groups
rank 19 After initialize parallelism groups
rank 21 After initialize parallelism groups
rank 20 After initialize parallelism groups
rank 6 After initialize parallelism groups
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
rank 26 Before initialize parallelism groups
rank 30 Before initialize parallelism groups
rank 28 Before initialize parallelism groups
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
rank 31 Before initialize parallelism groups
rank 26 After initialize parallelism groups
rank 29 Before initialize parallelism groups
rank 30 After initialize parallelism groups
rank 28 After initialize parallelism groups
rank 31 After initialize parallelism groups
rank 27 Before initialize parallelism groups
rank 29 After initialize parallelism groups
rank 27 After initialize parallelism groups
rank 24 Before initialize parallelism groups
rank 25 Before initialize parallelism groups
rank 24 After initialize parallelism groups
rank 25 After initialize parallelism groups
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
rank 9 Before initialize parallelism groups
rank 10 Before initialize parallelism groups
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
rank 10 After initialize parallelism groups
rank 14 Before initialize parallelism groups
rank 9 After initialize parallelism groups
Using dist.init_process_group. world_size  32
rank 8 Before initialize parallelism groups
Using dist.init_process_group. world_size  32
config_path  configs/ERA5-100million-91variables.yaml
rank 14 After initialize parallelism groups
rank 15 Before initialize parallelism groups
rank 11 Before initialize parallelism groups
rank 8 After initialize parallelism groups
rank 13 Before initialize parallelism groups
rank 15 After initialize parallelism groups
rank 11 After initialize parallelism groups
rank 12 Before initialize parallelism groups
{'seed_everything': 42, 'trainer': {'checkpoint_path': '/lustre/orion/stf218/world-shared/atsaris/tmp/tmp_checkpoints', 'checkpoint_filename': 'multi_last', 'resume_from_checkpoint': False}, 'parallelism': {'cpu_offloading': False}, 'model': {'lr': 2e-05, 'beta_1': 0.9, 'beta_2': 0.95, 'weight_decay': '1e-5', 'warmup_steps': 1000, 'max_steps': 20000, 'warmup_start_lr': '1e-8', 'eta_min': '1e-8', 'net': {'class_path': 'climax.arch.ClimaX', 'init_args': {'default_vars': ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_70', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000'], 'patch_size': 4, 'decoder_depth': 2, 'mlp_ratio': 4, 'drop_path': 0.1, 'drop_rate': 0.1, 'aggregated_variables': 1}}}, 'data': {'dict_root_dirs': {'mpi-esm': '/lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0'}, 'dict_start_idx': {'mpi-esm': 0}, 'dict_end_idx': {'mpi-esm': 1}, 'dict_in_variables': {'mpi-esm': ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000']}, 'dict_out_variables': {'mpi-esm': ['geopotential_500', 'temperature_850', '2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind']}, 'dict_max_predict_ranges': {'mpi-esm': 24}, 'dict_random_lead_time': {'mpi-esm': False}, 'dict_hrs_each_step': {'mpi-esm': 1}, 'dict_buffer_sizes': {'mpi-esm': 1}, 'num_workers': 1, 'pin_memory': False}}
rank 13 After initialize parallelism groups
max_epochs 1 data_par_size 1 fsdp_size 1 simple_ddp_size 1 tensor_par_size 32 seq_par_size 1 cpu_offloading False
lr  2e-05 beta_1  0.9 beta_2 0.95 weight_decay 1e-05 class_path climax.arch.ClimaX default_vars ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_70', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000']
img_size_x 128 img_size_y 256 patch_size 4 emb_dim 8192 depth 32 decoder_depth 2 num_heads 32 mlp_ratio 4 drop_path 0.1 drop_rate 0.1 aggregated_variables 1
dict_root_dirs {'mpi-esm': '/lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0'} dict_start_idx {'mpi-esm': 0} dict_end_idx {'mpi-esm': 1} batch_size 2 num_workers 1
warmup_steps 1000 max_steps 20000 warmup_start_lr 1e-08 eta_min 1e-08
checkpoint_path /lustre/orion/stf218/world-shared/atsaris/tmp/tmp_checkpoints checkpoint_filename multi_last resume_from_checkpoint False
rank 0 Before initialize parallelism groups
rank 12 After initialize parallelism groups
rank 0 After initialize parallelism groups
rank 1 After initialize model
rank 1 Before the second dist.barrier()
rank 27 After initialize model
rank 27 Before the second dist.barrier()
rank 5 After initialize model
rank 5 Before the second dist.barrier()
rank 21 After initialize model
rank 21 Before the second dist.barrier()
rank 20 After initialize model
rank 20 Before the second dist.barrier()
rank 22 After initialize model
rank 22 Before the second dist.barrier()
rank 2 After initialize model
rank 2 Before the second dist.barrier()
rank 7 After initialize model
rank 7 Before the second dist.barrier()
rank 17 After initialize model
rank 17 Before the second dist.barrier()
rank 4 After initialize model
rank 4 Before the second dist.barrier()
rank 3 After initialize model
rank 3 Before the second dist.barrier()
rank 19 After initialize model
rank 19 Before the second dist.barrier()
rank 16 After initialize model
rank 16 Before the second dist.barrier()
rank 6 After initialize model
rank 6 Before the second dist.barrier()
rank 23 After initialize model
rank 23 Before the second dist.barrier()
rank 18 After initialize model
rank 18 Before the second dist.barrier()
rank 31 After initialize model
rank 31 Before the second dist.barrier()
rank 30 After initialize model
rank 30 Before the second dist.barrier()
rank 26 After initialize model
rank 26 Before the second dist.barrier()
rank 25 After initialize model
rank 25 Before the second dist.barrier()
rank 28 After initialize model
rank 28 Before the second dist.barrier()
rank 29 After initialize model
rank 29 Before the second dist.barrier()
rank 24 After initialize model
rank 24 Before the second dist.barrier()
rank 0 After initialize model
rank 0 Before the second dist.barrier()
rank 10 After initialize model
rank 10 Before the second dist.barrier()
rank 12 After initialize model
rank 12 Before the second dist.barrier()
rank 9 After initialize model
rank 9 Before the second dist.barrier()
rank 14 After initialize model
rank 14 Before the second dist.barrier()
rank 13 After initialize model
rank 13 Before the second dist.barrier()
rank 15 After initialize model
rank 15 Before the second dist.barrier()
rank 8 After initialize model
rank 8 Before the second dist.barrier()
rank 11 After initialize model
rank 11 Before the second dist.barrier()
rank 0 After the second dist.barrier()
rank 16 After the second dist.barrier()
rank 1 After the second dist.barrier()
rank 20 After the second dist.barrier()
rank 21 After the second dist.barrier()
rank 8 After the second dist.barrier()
parameter name  var_embed  requires_gradient  True size torch.Size([1, 256, 8192])
rank 22 After the second dist.barrier()
rank 17 After the second dist.barrier()
rank 23 After the second dist.barrier()
rank 4 After the second dist.barrier()
rank 9 After the second dist.barrier()
rank 5 After the second dist.barrier()
rank 12 After the second dist.barrier()
rank 24 After the second dist.barrier()
parameter name  var_query  requires_gradient  True size torch.Size([1, 1, 8192])
rank 13 After the second dist.barrier()
rank 25 After the second dist.barrier()
parameter name  var_query_per_rank  requires_gradient  True size torch.Size([1, 1, 8192])
parameter name  pos_embed  requires_gradient  True size torch.Size([1, 2048, 8192])
parameter name  token_embeds.0.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.0.proj.bias  requires_gradient  True size torch.Size([8192])
rank 18 After the second dist.barrier()
rank 14 After the second dist.barrier()
rank 19 After the second dist.barrier()
rank 15 After the second dist.barrier()
parameter name  token_embeds.1.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
rank 6 After the second dist.barrier()
rank 7 After the second dist.barrier()
parameter name  token_embeds.1.proj.bias  requires_gradient  True size torch.Size([8192])
rank 29 After the second dist.barrier()
parameter name  token_embeds.2.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
rank 28 After the second dist.barrier()
parameter name  token_embeds.2.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.3.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.3.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.4.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.4.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.5.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
rank 10 After the second dist.barrier()
parameter name  token_embeds.5.proj.bias  requires_gradient  True size torch.Size([8192])
rank 11 After the second dist.barrier()
parameter name  token_embeds.6.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.6.proj.bias  requires_gradient  True size torch.Size([8192])
rank 30 After the second dist.barrier()
rank 3 After the second dist.barrier()
parameter name  token_embeds.7.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.7.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.8.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.8.proj.bias  requires_gradient  True size torch.Size([8192])
rank 2 After the second dist.barrier()
parameter name  token_embeds.9.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
rank 31 After the second dist.barrier()
parameter name  token_embeds.9.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.10.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.10.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.11.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.11.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.12.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.12.proj.bias  requires_gradient  True size torch.Size([8192])
rank 26 After the second dist.barrier()
rank 27 After the second dist.barrier()
parameter name  token_embeds.13.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.13.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.14.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.14.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.15.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.15.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.16.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.16.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.17.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.17.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.18.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.18.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.19.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.19.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.20.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.20.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.21.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.21.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.22.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.22.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.23.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.23.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.24.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.24.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.25.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.25.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.26.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.26.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.27.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.27.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.28.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.28.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.29.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.29.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.30.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.30.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.31.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.31.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.32.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.32.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.33.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.33.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.34.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.34.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.35.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.35.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.36.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.36.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.37.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.37.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.38.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.38.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.39.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.39.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.40.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.40.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.41.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.41.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.42.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.42.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.43.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.43.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.44.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.44.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.45.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.45.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.46.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.46.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.47.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.47.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.48.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.48.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.49.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.49.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.50.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.50.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.51.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.51.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.52.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.52.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.53.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.53.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.54.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.54.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.55.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.55.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.56.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.56.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.57.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.57.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.58.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.58.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.59.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.59.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.60.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.60.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.61.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.61.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.62.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.62.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.63.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.63.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.64.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.64.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.65.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.65.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.66.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.66.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.67.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.67.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.68.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.68.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.69.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.69.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.70.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.70.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.71.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.71.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.72.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.72.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.73.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.73.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.74.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.74.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.75.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.75.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.76.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.76.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.77.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.77.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.78.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.78.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.79.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.79.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.80.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.80.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.81.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.81.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.82.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.82.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.83.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.83.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.84.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.84.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.85.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.85.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.86.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.86.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.87.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.87.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.88.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.88.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.89.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.89.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.90.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.90.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.91.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.91.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.92.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.92.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.93.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.93.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.94.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.94.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.95.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.95.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.96.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.96.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.97.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.97.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.98.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.98.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.99.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.99.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.100.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.100.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.101.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.101.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.102.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.102.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.103.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.103.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.104.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.104.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.105.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.105.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.106.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.106.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.107.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.107.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.108.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.108.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.109.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.109.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.110.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.110.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.111.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.111.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.112.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.112.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.113.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.113.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.114.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.114.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.115.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.115.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.116.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.116.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.117.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.117.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.118.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.118.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.119.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.119.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.120.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.120.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.121.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.121.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.122.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.122.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.123.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.123.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.124.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.124.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.125.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.125.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.126.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.126.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.127.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.127.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.128.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.128.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.129.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.129.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.130.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.130.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.131.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.131.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.132.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.132.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.133.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.133.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.134.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.134.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.135.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.135.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.136.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.136.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.137.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.137.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.138.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.138.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.139.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.139.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.140.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.140.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.141.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.141.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.142.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.142.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.143.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.143.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.144.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.144.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.145.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.145.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.146.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.146.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.147.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.147.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.148.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.148.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.149.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.149.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.150.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.150.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.151.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.151.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.152.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.152.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.153.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.153.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.154.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.154.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.155.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.155.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.156.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.156.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.157.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.157.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.158.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.158.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.159.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.159.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.160.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.160.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.161.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.161.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.162.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.162.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.163.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.163.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.164.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.164.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.165.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.165.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.166.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.166.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.167.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.167.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.168.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.168.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.169.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.169.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.170.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.170.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.171.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.171.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.172.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.172.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.173.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.173.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.174.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.174.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.175.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.175.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.176.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.176.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.177.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.177.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.178.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.178.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.179.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.179.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.180.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.180.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.181.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.181.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.182.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.182.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.183.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.183.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.184.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.184.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.185.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.185.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.186.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.186.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.187.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.187.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.188.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.188.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.189.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.189.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.190.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.190.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.191.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.191.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.192.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.192.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.193.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.193.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.194.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.194.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.195.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.195.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.196.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.196.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.197.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.197.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.198.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.198.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.199.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.199.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.200.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.200.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.201.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.201.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.202.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.202.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.203.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.203.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.204.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.204.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.205.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.205.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.206.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.206.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.207.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.207.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.208.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.208.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.209.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.209.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.210.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.210.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.211.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.211.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.212.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.212.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.213.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.213.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.214.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.214.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.215.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.215.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.216.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.216.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.217.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.217.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.218.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.218.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.219.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.219.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.220.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.220.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.221.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.221.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.222.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.222.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.223.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.223.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.224.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.224.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.225.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.225.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.226.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.226.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.227.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.227.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.228.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.228.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.229.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.229.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.230.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.230.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.231.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.231.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.232.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.232.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.233.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.233.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.234.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.234.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.235.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.235.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.236.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.236.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.237.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.237.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.238.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.238.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.239.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.239.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.240.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.240.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.241.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.241.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.242.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.242.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.243.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.243.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.244.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.244.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.245.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.245.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.246.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.246.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.247.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.247.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.248.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.248.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.249.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.249.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.250.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.250.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.251.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.251.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.252.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.252.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.253.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.253.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.254.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.254.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.255.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.255.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  var_agg.q.weight  requires_gradient  True size torch.Size([256, 8192])
parameter name  var_agg.kv.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  var_agg.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  var_agg.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  var_agg_per_rank.q.weight  requires_gradient  True size torch.Size([8192, 8192])
parameter name  var_agg_per_rank.kv.weight  requires_gradient  True size torch.Size([16384, 8192])
parameter name  var_agg_per_rank.proj.weight  requires_gradient  True size torch.Size([8192, 8192])
parameter name  var_agg_per_rank.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  lead_time_embed.weight  requires_gradient  True size torch.Size([8192, 1])
parameter name  lead_time_embed.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.0.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.0.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.0.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.0.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.0.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.0.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.1.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.1.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.1.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.1.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.1.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.1.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.2.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.2.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.2.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.2.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.2.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.2.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.3.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.3.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.3.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.3.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.3.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.3.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.4.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.4.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.4.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.4.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.4.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.4.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.5.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.5.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.5.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.5.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.5.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.5.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.6.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.6.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.6.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.6.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.6.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.6.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.7.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.7.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.7.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.7.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.7.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.7.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.8.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.8.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.8.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.8.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.8.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.8.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.9.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.9.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.9.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.9.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.9.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.9.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.10.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.10.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.10.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.10.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.10.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.10.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.11.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.11.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.11.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.11.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.11.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.11.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.12.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.12.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.12.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.12.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.12.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.12.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.13.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.13.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.13.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.13.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.13.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.13.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.14.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.14.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.14.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.14.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.14.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.14.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.15.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.15.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.15.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.15.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.15.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.15.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.16.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.16.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.16.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.16.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.16.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.16.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.17.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.17.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.17.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.17.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.17.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.17.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.18.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.18.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.18.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.18.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.18.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.18.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.19.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.19.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.19.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.19.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.19.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.19.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.20.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.20.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.20.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.20.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.20.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.20.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.21.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.21.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.21.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.21.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.21.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.21.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.22.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.22.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.22.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.22.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.22.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.22.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.23.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.23.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.23.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.23.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.23.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.23.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.24.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.24.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.24.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.24.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.24.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.24.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.25.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.25.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.25.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.25.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.25.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.25.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.26.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.26.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.26.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.26.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.26.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.26.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.27.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.27.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.27.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.27.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.27.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.27.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.28.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.28.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.28.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.28.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.28.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.28.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.29.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.29.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.29.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.29.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.29.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.29.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.30.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.30.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.30.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.30.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.30.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.30.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.31.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.31.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.31.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.31.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.31.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.31.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  norm.weight  requires_gradient  True size torch.Size([8192])
parameter name  norm.bias  requires_gradient  True size torch.Size([8192])
parameter name  head.0.weight  requires_gradient  True size torch.Size([8192, 8192])
parameter name  head.0.bias  requires_gradient  True size torch.Size([8192])
parameter name  head.2.weight  requires_gradient  True size torch.Size([8192, 8192])
parameter name  head.2.bias  requires_gradient  True size torch.Size([8192])
parameter name  head.5.weight  requires_gradient  True size torch.Size([4096, 8192])
parameter name  head.5.bias  requires_gradient  True size torch.Size([4096])
total_params before FSDP tensor(34870726656) params_per_gpu tensor(1306144768)
total_params after FSDP FullyShardedDataParallel(
  (_fsdp_wrapped_module): ClimaX(
    (token_embeds): ModuleList(
      (0-255): 256 x PatchEmbed(
        (proj): Conv2d(1, 8192, kernel_size=(4, 4), stride=(4, 4))
        (norm): Identity()
      )
    )
    (var_agg): FullyShardedDataParallel(
      (_fsdp_wrapped_module): CheckpointWrapper(
        (_checkpoint_wrapped_module): VariableMapping_Attention(
          (q): Linear(in_features=8192, out_features=256, bias=False)
          (kv): Linear(in_features=8192, out_features=512, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=8192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (var_agg_per_rank): CustomCrossAttention(
      (q): Linear(in_features=8192, out_features=8192, bias=False)
      (kv): Linear(in_features=8192, out_features=16384, bias=False)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=8192, out_features=8192, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (lead_time_embed): Linear(in_features=1, out_features=8192, bias=True)
    (pos_drop): Dropout(p=0.1, inplace=False)
    (blocks): ModuleList(
      (0-31): 32 x FullyShardedDataParallel(
        (_fsdp_wrapped_module): CheckpointWrapper(
          (_checkpoint_wrapped_module): Block(
            (norm1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=8192, out_features=768, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=256, out_features=8192, bias=True)
              (proj_drop): Dropout(p=0.1, inplace=False)
            )
            (ls1): Identity()
            (norm2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=8192, out_features=1024, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.1, inplace=False)
              (fc2): Linear(in_features=1024, out_features=8192, bias=True)
            )
            (ls2): Identity()
          )
        )
      )
    )
    (norm): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
    (head): Sequential(
      (0): Linear(in_features=8192, out_features=8192, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=8192, out_features=8192, bias=True)
      (3): GELU(approximate='none')
      (4): Pred_Rearrange()
      (5): Linear(in_features=8192, out_features=4096, bias=True)
    )
  )
)
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
rank  31 lister_train reset: mpi-esm 9 9 9
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
rank  0 lister_train reset: mpi-esm 9 9 9
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
memory stats reset, ready to track
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
epoch-train:  0 batch_idx 0 world_rank 0  loss  8.625
epoch-train:  0 batch_idx 1 world_rank 0  loss  nan
epoch-train:  0 batch_idx 2 world_rank 0  loss  nan
[2025-03-12 17:08:38,263] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:08:38,265] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:08:38,266] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:08:38,267] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:08:38,267] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:08:38,267] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:08:38,267] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:08:38,268] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:08:38,269] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:08:38,268] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:08:38,268] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:08:38,268] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:08:38,269] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:08:38,269] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:08:38,269] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:08:38,269] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:08:38,270] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:08:38,271] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:08:38,271] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:08:38,271] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:08:38,271] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:08:38,272] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:08:38,272] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:08:38,274] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:08:38,274] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:08:38,274] [INFO] [profiler.py:80:start_profile] Flops profiler started
epoch-train:  0 batch_idx 3 world_rank 0  loss  nan
[2025-03-12 17:08:38,275] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:08:38,275] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:08:38,275] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:08:38,276] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:08:38,276] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:08:38,277] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:08:39,316] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:08:39,316] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:08:39,316] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:08:39,316] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:08:39,316] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:08:39,316] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:08:39,316] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:08:39,317] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:08:39,317] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:08:39,317] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:08:39,317] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:08:39,317] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:08:39,317] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:08:39,317] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:08:39,317] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:08:39,317] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:08:39,317] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:08:39,317] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:08:39,317] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:08:39,317] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:08:39,318] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:08:39,318] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:08:39,318] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:08:39,318] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:08:39,318] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:08:39,318] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:08:39,318] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:08:39,318] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:08:39,319] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:08:39,319] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:08:39,319] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:08:39,319] [INFO] [profiler.py:226:end_profile] Flops profiler finished
global rank 27: ddp rank 0 iter_start,iter_end = 0 8
global rank 27: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 24: ddp rank 0 iter_start,iter_end = 0 8
global rank 24: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 29: ddp rank 0 iter_start,iter_end = 0 8
global rank 29: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 13: ddp rank 0 iter_start,iter_end = 0 8
global rank 13: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 1: ddp rank 0 iter_start,iter_end = 0 8
global rank 1: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 25: ddp rank 0 iter_start,iter_end = 0 8
global rank 25: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 30: ddp rank 0 iter_start,iter_end = 0 8
global rank 30: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 11: ddp rank 0 iter_start,iter_end = 0 8
global rank 11: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 3: ddp rank 0 iter_start,iter_end = 0 8
global rank 3: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 6: ddp rank 0 iter_start,iter_end = 0 8
global rank 6: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 22: ddp rank 0 iter_start,iter_end = 0 8
global rank 22: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 20: ddp rank 0 iter_start,iter_end = 0 8
global rank 20: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 26: ddp rank 0 iter_start,iter_end = 0 8
global rank 26: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 12: ddp rank 0 iter_start,iter_end = 0 8
global rank 12: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 31: ddp rank 0 iter_start,iter_end = 0 8
global rank 31: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 4: ddp rank 0 iter_start,iter_end = 0 8
global rank 4: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 17: ddp rank 0 iter_start,iter_end = 0 8
global rank 17: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 15: ddp rank 0 iter_start,iter_end = 0 8
global rank 15: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 18: ddp rank 0 iter_start,iter_end = 0 8
global rank 18: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 2: ddp rank 0 iter_start,iter_end = 0 8
global rank 2: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 10: ddp rank 0 iter_start,iter_end = 0 8
global rank 10: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 28: ddp rank 0 iter_start,iter_end = 0 8
global rank 28: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 7: ddp rank 0 iter_start,iter_end = 0 8
global rank 7: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 8: ddp rank 0 iter_start,iter_end = 0 8
global rank 8: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 23: ddp rank 0 iter_start,iter_end = 0 8
global rank 23: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 0: ddp rank 0 iter_start,iter_end = 0 8
global rank 0: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 16: ddp rank 0 iter_start,iter_end = 0 8
global rank 16: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 19: ddp rank 0 iter_start,iter_end = 0 8
global rank 19: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 5: ddp rank 0 iter_start,iter_end = 0 8
global rank 5: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 14: ddp rank 0 iter_start,iter_end = 0 8
global rank 14: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 9: ddp rank 0 iter_start,iter_end = 0 8
global rank 9: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 21: ddp rank 0 iter_start,iter_end = 0 8
global rank 21: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
MUST: Model 1306.144768 M TFLOPS: 26.053995671252217

--> cuda max reserved memory = 39.2012
--> max reserved percentage = 61.27 %

--> cuda max memory allocated = 29.0536
--> max allocated percentage = 45.41 %

--> peak active memory = 36.5879
--> peak active memory 57.18 %

cudaMalloc retries = 0
cuda OOM = 0

HERE1 Namespace(arch='orbit_hier', batch_size=2, channels=256, depth=32, embed_dim=8192, fa2=True, fsdp_size=1, imagex=128, imagey=256, max_epochs=1, num_heads=32, real=False, seq_par_size=1, simple_ddp_size=1, tensor_par_size=32, yaml_config='configs/ERA5-100million-91variables.yaml') 39.2012 26.053995671252217 tensor(34870726656)
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:08:53,963] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:08:53,962] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:08:53,962] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:08:53,962] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:08:53,963] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:08:53,962] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:08:53,963] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:08:53,963] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:08:53,963] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:08:53,963] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:08:53,963] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:08:53,963] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:08:53,963] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:08:53,963] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:08:53,963] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:08:53,963] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:08:53,964] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:08:53,964] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:08:53,964] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:08:53,964] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:08:53,964] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:08:53,964] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:08:53,964] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:08:53,964] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:08:53,969] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:08:53,969] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:08:53,969] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:08:53,969] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:08:53,969] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:08:53,969] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:08:53,969] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:08:53,969] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
HERE0 Namespace(arch='orbit_hier', batch_size=2, channels=512, depth=32, embed_dim=8192, fa2=True, fsdp_size=1, imagex=128, imagey=256, max_epochs=1, num_heads=32, real=False, seq_par_size=1, simple_ddp_size=1, tensor_par_size=32, yaml_config='configs/ERA5-100million-91variables.yaml')
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
rank 24 Before initialize parallelism groups
rank 27 Before initialize parallelism groups
rank 30 Before initialize parallelism groups
rank 2 Before initialize parallelism groups
rank 6 Before initialize parallelism groups
rank 1 Before initialize parallelism groups
Using dist.init_process_group. world_size  32
rank 13 Before initialize parallelism groups
rank 29 Before initialize parallelism groups
rank 25 Before initialize parallelism groups
rank 7 Before initialize parallelism groups
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
rank 18 Before initialize parallelism groups
rank 15 Before initialize parallelism groups
rank 10 Before initialize parallelism groups
Using dist.init_process_group. world_size  32
rank 31 Before initialize parallelism groups
rank 19 Before initialize parallelism groups
rank 21 Before initialize parallelism groups
rank 22 Before initialize parallelism groups
rank 23 Before initialize parallelism groups
Using dist.init_process_group. world_size  32
config_path  configs/ERA5-100million-91variables.yaml
rank 9 Before initialize parallelism groups
rank 30 After initialize parallelism groups
rank 24 After initialize parallelism groups
rank 27 After initialize parallelism groups
rank 6 After initialize parallelism groups
rank 2 After initialize parallelism groups
rank 1 After initialize parallelism groups
rank 16 Before initialize parallelism groups
rank 3 Before initialize parallelism groups
rank 13 After initialize parallelism groups
rank 29 After initialize parallelism groups
rank 25 After initialize parallelism groups
rank 4 Before initialize parallelism groups
rank 7 After initialize parallelism groups
rank 18 After initialize parallelism groups
rank 15 After initialize parallelism groups
rank 10 After initialize parallelism groups
rank 14 Before initialize parallelism groups
rank 8 Before initialize parallelism groups
rank 31 After initialize parallelism groups
rank 22 After initialize parallelism groups
rank 21 After initialize parallelism groups
rank 23 After initialize parallelism groups
rank 19 After initialize parallelism groups
rank 9 After initialize parallelism groups
rank 26 Before initialize parallelism groups
rank 16 After initialize parallelism groups
rank 12 Before initialize parallelism groups
rank 3 After initialize parallelism groups
rank 4 After initialize parallelism groups
rank 14 After initialize parallelism groups
rank 28 Before initialize parallelism groups
rank 8 After initialize parallelism groups
{'seed_everything': 42, 'trainer': {'checkpoint_path': '/lustre/orion/stf218/world-shared/atsaris/tmp/tmp_checkpoints', 'checkpoint_filename': 'multi_last', 'resume_from_checkpoint': False}, 'parallelism': {'cpu_offloading': False}, 'model': {'lr': 2e-05, 'beta_1': 0.9, 'beta_2': 0.95, 'weight_decay': '1e-5', 'warmup_steps': 1000, 'max_steps': 20000, 'warmup_start_lr': '1e-8', 'eta_min': '1e-8', 'net': {'class_path': 'climax.arch.ClimaX', 'init_args': {'default_vars': ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_70', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000'], 'patch_size': 4, 'decoder_depth': 2, 'mlp_ratio': 4, 'drop_path': 0.1, 'drop_rate': 0.1, 'aggregated_variables': 1}}}, 'data': {'dict_root_dirs': {'mpi-esm': '/lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0'}, 'dict_start_idx': {'mpi-esm': 0}, 'dict_end_idx': {'mpi-esm': 1}, 'dict_in_variables': {'mpi-esm': ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000']}, 'dict_out_variables': {'mpi-esm': ['geopotential_500', 'temperature_850', '2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind']}, 'dict_max_predict_ranges': {'mpi-esm': 24}, 'dict_random_lead_time': {'mpi-esm': False}, 'dict_hrs_each_step': {'mpi-esm': 1}, 'dict_buffer_sizes': {'mpi-esm': 1}, 'num_workers': 1, 'pin_memory': False}}
rank 5 Before initialize parallelism groups
rank 20 Before initialize parallelism groups
rank 11 Before initialize parallelism groups
rank 26 After initialize parallelism groups
rank 17 Before initialize parallelism groups
rank 12 After initialize parallelism groups
max_epochs 1 data_par_size 1 fsdp_size 1 simple_ddp_size 1 tensor_par_size 32 seq_par_size 1 cpu_offloading False
lr  2e-05 beta_1  0.9 beta_2 0.95 weight_decay 1e-05 class_path climax.arch.ClimaX default_vars ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_70', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000']
img_size_x 128 img_size_y 256 patch_size 4 emb_dim 8192 depth 32 decoder_depth 2 num_heads 32 mlp_ratio 4 drop_path 0.1 drop_rate 0.1 aggregated_variables 1
dict_root_dirs {'mpi-esm': '/lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0'} dict_start_idx {'mpi-esm': 0} dict_end_idx {'mpi-esm': 1} batch_size 2 num_workers 1
warmup_steps 1000 max_steps 20000 warmup_start_lr 1e-08 eta_min 1e-08
checkpoint_path /lustre/orion/stf218/world-shared/atsaris/tmp/tmp_checkpoints checkpoint_filename multi_last resume_from_checkpoint False
rank 0 Before initialize parallelism groups
rank 28 After initialize parallelism groups
rank 5 After initialize parallelism groups
rank 20 After initialize parallelism groups
rank 11 After initialize parallelism groups
rank 17 After initialize parallelism groups
rank 0 After initialize parallelism groups
rank 17 After initialize model
rank 17 Before the second dist.barrier()
rank 20 After initialize model
rank 20 Before the second dist.barrier()
rank 21 After initialize model
rank 21 Before the second dist.barrier()
rank 6 After initialize model
rank 6 Before the second dist.barrier()
rank 19 After initialize model
rank 19 Before the second dist.barrier()
rank 16 After initialize model
rank 16 Before the second dist.barrier()
rank 26 After initialize model
rank 26 Before the second dist.barrier()
rank 23 After initialize model
rank 23 Before the second dist.barrier()
rank 14 After initialize model
rank 14 Before the second dist.barrier()
rank 30 After initialize model
rank 30 Before the second dist.barrier()
rank 22 After initialize model
rank 22 Before the second dist.barrier()
rank 13 After initialize model
rank 13 Before the second dist.barrier()
rank 18 After initialize model
rank 18 Before the second dist.barrier()
rank 31 After initialize model
rank 31 Before the second dist.barrier()
rank 10 After initialize model
rank 10 Before the second dist.barrier()
rank 25 After initialize model
rank 25 Before the second dist.barrier()
rank 3 After initialize model
rank 3 Before the second dist.barrier()
rank 2 After initialize model
rank 2 Before the second dist.barrier()
rank 7 After initialize model
rank 7 Before the second dist.barrier()
rank 12 After initialize model
rank 12 Before the second dist.barrier()
rank 9 After initialize model
rank 9 Before the second dist.barrier()
rank 28 After initialize model
rank 28 Before the second dist.barrier()
rank 24 After initialize model
rank 24 Before the second dist.barrier()
rank 27 After initialize model
rank 27 Before the second dist.barrier()
rank 1 After initialize model
rank 1 Before the second dist.barrier()
rank 11 After initialize model
rank 11 Before the second dist.barrier()
rank 8 After initialize model
rank 8 Before the second dist.barrier()
rank 15 After initialize model
rank 15 Before the second dist.barrier()
rank 29 After initialize model
rank 29 Before the second dist.barrier()
rank 0 After initialize model
rank 0 Before the second dist.barrier()
rank 4 After initialize model
rank 4 Before the second dist.barrier()
rank 5 After initialize model
rank 5 Before the second dist.barrier()
rank 16 After the second dist.barrier()
rank 17 After the second dist.barrier()
rank 8 After the second dist.barrier()
rank 9 After the second dist.barrier()
rank 1 After the second dist.barrier()
rank 10 After the second dist.barrier()
rank 11 After the second dist.barrier()
rank 12 After the second dist.barrier()
rank 14 After the second dist.barrier()
rank 20 After the second dist.barrier()
rank 15 After the second dist.barrier()
rank 21 After the second dist.barrier()
rank 13 After the second dist.barrier()
rank 0 After the second dist.barrier()
rank 5 After the second dist.barrier()
rank 4 After the second dist.barrier()
rank 22 After the second dist.barrier()
parameter name  var_embed  requires_gradient  True size torch.Size([1, 512, 8192])
rank 25 After the second dist.barrier()
rank 29 After the second dist.barrier()
rank 24 After the second dist.barrier()
rank 28 After the second dist.barrier()
rank 6 After the second dist.barrier()
rank 23 After the second dist.barrier()
rank 7 After the second dist.barrier()
rank 19 After the second dist.barrier()
parameter name  var_query  requires_gradient  True size torch.Size([1, 1, 8192])
rank 30 After the second dist.barrier()
rank 31 After the second dist.barrier()
parameter name  var_query_per_rank  requires_gradient  True size torch.Size([1, 1, 8192])
rank 3 After the second dist.barrier()
parameter name  pos_embed  requires_gradient  True size torch.Size([1, 2048, 8192])
rank 18 After the second dist.barrier()
parameter name  token_embeds.0.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.0.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.1.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
rank 2 After the second dist.barrier()
parameter name  token_embeds.1.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.2.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
rank 26 After the second dist.barrier()
parameter name  token_embeds.2.proj.bias  requires_gradient  True size torch.Size([8192])
rank 27 After the second dist.barrier()
parameter name  token_embeds.3.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.3.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.4.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.4.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.5.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.5.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.6.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.6.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.7.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.7.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.8.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.8.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.9.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.9.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.10.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.10.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.11.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.11.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.12.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.12.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.13.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.13.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.14.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.14.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.15.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.15.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.16.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.16.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.17.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.17.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.18.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.18.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.19.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.19.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.20.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.20.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.21.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.21.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.22.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.22.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.23.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.23.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.24.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.24.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.25.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.25.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.26.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.26.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.27.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.27.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.28.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.28.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.29.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.29.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.30.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.30.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.31.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.31.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.32.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.32.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.33.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.33.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.34.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.34.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.35.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.35.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.36.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.36.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.37.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.37.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.38.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.38.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.39.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.39.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.40.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.40.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.41.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.41.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.42.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.42.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.43.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.43.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.44.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.44.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.45.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.45.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.46.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.46.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.47.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.47.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.48.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.48.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.49.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.49.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.50.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.50.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.51.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.51.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.52.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.52.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.53.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.53.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.54.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.54.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.55.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.55.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.56.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.56.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.57.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.57.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.58.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.58.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.59.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.59.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.60.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.60.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.61.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.61.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.62.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.62.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.63.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.63.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.64.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.64.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.65.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.65.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.66.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.66.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.67.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.67.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.68.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.68.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.69.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.69.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.70.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.70.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.71.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.71.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.72.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.72.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.73.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.73.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.74.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.74.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.75.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.75.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.76.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.76.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.77.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.77.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.78.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.78.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.79.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.79.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.80.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.80.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.81.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.81.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.82.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.82.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.83.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.83.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.84.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.84.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.85.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.85.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.86.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.86.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.87.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.87.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.88.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.88.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.89.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.89.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.90.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.90.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.91.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.91.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.92.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.92.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.93.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.93.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.94.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.94.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.95.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.95.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.96.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.96.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.97.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.97.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.98.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.98.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.99.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.99.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.100.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.100.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.101.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.101.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.102.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.102.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.103.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.103.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.104.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.104.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.105.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.105.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.106.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.106.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.107.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.107.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.108.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.108.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.109.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.109.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.110.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.110.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.111.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.111.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.112.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.112.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.113.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.113.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.114.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.114.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.115.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.115.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.116.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.116.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.117.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.117.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.118.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.118.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.119.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.119.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.120.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.120.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.121.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.121.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.122.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.122.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.123.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.123.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.124.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.124.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.125.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.125.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.126.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.126.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.127.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.127.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.128.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.128.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.129.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.129.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.130.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.130.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.131.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.131.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.132.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.132.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.133.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.133.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.134.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.134.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.135.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.135.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.136.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.136.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.137.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.137.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.138.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.138.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.139.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.139.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.140.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.140.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.141.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.141.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.142.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.142.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.143.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.143.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.144.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.144.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.145.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.145.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.146.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.146.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.147.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.147.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.148.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.148.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.149.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.149.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.150.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.150.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.151.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.151.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.152.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.152.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.153.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.153.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.154.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.154.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.155.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.155.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.156.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.156.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.157.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.157.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.158.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.158.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.159.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.159.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.160.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.160.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.161.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.161.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.162.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.162.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.163.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.163.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.164.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.164.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.165.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.165.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.166.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.166.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.167.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.167.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.168.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.168.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.169.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.169.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.170.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.170.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.171.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.171.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.172.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.172.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.173.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.173.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.174.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.174.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.175.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.175.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.176.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.176.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.177.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.177.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.178.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.178.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.179.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.179.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.180.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.180.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.181.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.181.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.182.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.182.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.183.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.183.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.184.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.184.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.185.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.185.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.186.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.186.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.187.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.187.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.188.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.188.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.189.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.189.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.190.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.190.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.191.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.191.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.192.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.192.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.193.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.193.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.194.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.194.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.195.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.195.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.196.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.196.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.197.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.197.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.198.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.198.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.199.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.199.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.200.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.200.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.201.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.201.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.202.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.202.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.203.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.203.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.204.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.204.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.205.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.205.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.206.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.206.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.207.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.207.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.208.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.208.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.209.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.209.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.210.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.210.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.211.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.211.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.212.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.212.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.213.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.213.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.214.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.214.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.215.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.215.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.216.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.216.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.217.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.217.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.218.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.218.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.219.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.219.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.220.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.220.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.221.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.221.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.222.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.222.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.223.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.223.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.224.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.224.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.225.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.225.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.226.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.226.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.227.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.227.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.228.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.228.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.229.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.229.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.230.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.230.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.231.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.231.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.232.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.232.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.233.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.233.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.234.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.234.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.235.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.235.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.236.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.236.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.237.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.237.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.238.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.238.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.239.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.239.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.240.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.240.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.241.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.241.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.242.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.242.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.243.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.243.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.244.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.244.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.245.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.245.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.246.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.246.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.247.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.247.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.248.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.248.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.249.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.249.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.250.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.250.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.251.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.251.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.252.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.252.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.253.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.253.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.254.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.254.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.255.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.255.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.256.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.256.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.257.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.257.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.258.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.258.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.259.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.259.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.260.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.260.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.261.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.261.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.262.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.262.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.263.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.263.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.264.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.264.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.265.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.265.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.266.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.266.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.267.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.267.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.268.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.268.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.269.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.269.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.270.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.270.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.271.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.271.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.272.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.272.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.273.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.273.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.274.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.274.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.275.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.275.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.276.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.276.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.277.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.277.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.278.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.278.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.279.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.279.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.280.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.280.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.281.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.281.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.282.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.282.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.283.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.283.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.284.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.284.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.285.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.285.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.286.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.286.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.287.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.287.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.288.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.288.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.289.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.289.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.290.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.290.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.291.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.291.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.292.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.292.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.293.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.293.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.294.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.294.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.295.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.295.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.296.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.296.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.297.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.297.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.298.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.298.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.299.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.299.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.300.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.300.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.301.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.301.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.302.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.302.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.303.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.303.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.304.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.304.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.305.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.305.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.306.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.306.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.307.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.307.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.308.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.308.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.309.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.309.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.310.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.310.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.311.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.311.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.312.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.312.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.313.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.313.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.314.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.314.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.315.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.315.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.316.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.316.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.317.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.317.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.318.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.318.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.319.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.319.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.320.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.320.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.321.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.321.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.322.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.322.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.323.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.323.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.324.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.324.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.325.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.325.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.326.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.326.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.327.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.327.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.328.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.328.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.329.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.329.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.330.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.330.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.331.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.331.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.332.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.332.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.333.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.333.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.334.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.334.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.335.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.335.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.336.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.336.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.337.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.337.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.338.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.338.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.339.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.339.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.340.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.340.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.341.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.341.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.342.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.342.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.343.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.343.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.344.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.344.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.345.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.345.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.346.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.346.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.347.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.347.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.348.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.348.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.349.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.349.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.350.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.350.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.351.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.351.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.352.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.352.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.353.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.353.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.354.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.354.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.355.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.355.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.356.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.356.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.357.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.357.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.358.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.358.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.359.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.359.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.360.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.360.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.361.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.361.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.362.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.362.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.363.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.363.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.364.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.364.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.365.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.365.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.366.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.366.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.367.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.367.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.368.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.368.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.369.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.369.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.370.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.370.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.371.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.371.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.372.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.372.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.373.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.373.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.374.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.374.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.375.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.375.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.376.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.376.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.377.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.377.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.378.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.378.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.379.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.379.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.380.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.380.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.381.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.381.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.382.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.382.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.383.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.383.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.384.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.384.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.385.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.385.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.386.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.386.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.387.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.387.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.388.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.388.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.389.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.389.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.390.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.390.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.391.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.391.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.392.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.392.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.393.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.393.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.394.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.394.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.395.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.395.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.396.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.396.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.397.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.397.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.398.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.398.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.399.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.399.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.400.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.400.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.401.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.401.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.402.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.402.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.403.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.403.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.404.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.404.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.405.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.405.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.406.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.406.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.407.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.407.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.408.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.408.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.409.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.409.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.410.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.410.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.411.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.411.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.412.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.412.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.413.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.413.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.414.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.414.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.415.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.415.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.416.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.416.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.417.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.417.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.418.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.418.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.419.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.419.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.420.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.420.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.421.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.421.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.422.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.422.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.423.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.423.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.424.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.424.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.425.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.425.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.426.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.426.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.427.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.427.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.428.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.428.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.429.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.429.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.430.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.430.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.431.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.431.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.432.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.432.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.433.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.433.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.434.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.434.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.435.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.435.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.436.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.436.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.437.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.437.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.438.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.438.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.439.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.439.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.440.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.440.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.441.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.441.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.442.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.442.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.443.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.443.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.444.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.444.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.445.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.445.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.446.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.446.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.447.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.447.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.448.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.448.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.449.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.449.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.450.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.450.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.451.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.451.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.452.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.452.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.453.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.453.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.454.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.454.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.455.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.455.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.456.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.456.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.457.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.457.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.458.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.458.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.459.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.459.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.460.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.460.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.461.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.461.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.462.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.462.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.463.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.463.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.464.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.464.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.465.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.465.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.466.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.466.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.467.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.467.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.468.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.468.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.469.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.469.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.470.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.470.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.471.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.471.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.472.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.472.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.473.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.473.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.474.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.474.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.475.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.475.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.476.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.476.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.477.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.477.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.478.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.478.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.479.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.479.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.480.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.480.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.481.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.481.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.482.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.482.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.483.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.483.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.484.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.484.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.485.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.485.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.486.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.486.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.487.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.487.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.488.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.488.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.489.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.489.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.490.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.490.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.491.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.491.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.492.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.492.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.493.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.493.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.494.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.494.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.495.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.495.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.496.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.496.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.497.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.497.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.498.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.498.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.499.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.499.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.500.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.500.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.501.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.501.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.502.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.502.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.503.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.503.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.504.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.504.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.505.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.505.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.506.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.506.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.507.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.507.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.508.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.508.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.509.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.509.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.510.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.510.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.511.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.511.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  var_agg.q.weight  requires_gradient  True size torch.Size([256, 8192])
parameter name  var_agg.kv.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  var_agg.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  var_agg.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  var_agg_per_rank.q.weight  requires_gradient  True size torch.Size([8192, 8192])
parameter name  var_agg_per_rank.kv.weight  requires_gradient  True size torch.Size([16384, 8192])
parameter name  var_agg_per_rank.proj.weight  requires_gradient  True size torch.Size([8192, 8192])
parameter name  var_agg_per_rank.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  lead_time_embed.weight  requires_gradient  True size torch.Size([8192, 1])
parameter name  lead_time_embed.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.0.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.0.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.0.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.0.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.0.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.0.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.1.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.1.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.1.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.1.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.1.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.1.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.2.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.2.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.2.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.2.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.2.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.2.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.3.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.3.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.3.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.3.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.3.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.3.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.4.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.4.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.4.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.4.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.4.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.4.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.5.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.5.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.5.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.5.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.5.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.5.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.6.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.6.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.6.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.6.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.6.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.6.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.7.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.7.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.7.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.7.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.7.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.7.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.8.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.8.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.8.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.8.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.8.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.8.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.9.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.9.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.9.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.9.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.9.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.9.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.10.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.10.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.10.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.10.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.10.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.10.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.11.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.11.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.11.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.11.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.11.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.11.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.12.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.12.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.12.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.12.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.12.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.12.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.13.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.13.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.13.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.13.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.13.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.13.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.14.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.14.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.14.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.14.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.14.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.14.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.15.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.15.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.15.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.15.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.15.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.15.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.16.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.16.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.16.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.16.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.16.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.16.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.17.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.17.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.17.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.17.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.17.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.17.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.18.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.18.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.18.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.18.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.18.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.18.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.19.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.19.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.19.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.19.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.19.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.19.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.20.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.20.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.20.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.20.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.20.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.20.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.21.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.21.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.21.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.21.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.21.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.21.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.22.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.22.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.22.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.22.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.22.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.22.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.23.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.23.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.23.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.23.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.23.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.23.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.24.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.24.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.24.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.24.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.24.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.24.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.25.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.25.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.25.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.25.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.25.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.25.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.26.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.26.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.26.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.26.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.26.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.26.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.27.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.27.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.27.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.27.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.27.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.27.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.28.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.28.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.28.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.28.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.28.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.28.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.29.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.29.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.29.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.29.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.29.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.29.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.30.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.30.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.30.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.30.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.30.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.30.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.31.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.31.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.31.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.31.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.31.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.31.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  norm.weight  requires_gradient  True size torch.Size([8192])
parameter name  norm.bias  requires_gradient  True size torch.Size([8192])
parameter name  head.0.weight  requires_gradient  True size torch.Size([8192, 8192])
parameter name  head.0.bias  requires_gradient  True size torch.Size([8192])
parameter name  head.2.weight  requires_gradient  True size torch.Size([8192, 8192])
parameter name  head.2.bias  requires_gradient  True size torch.Size([8192])
parameter name  head.5.weight  requires_gradient  True size torch.Size([8192, 8192])
parameter name  head.5.bias  requires_gradient  True size torch.Size([8192])
total_params before FSDP tensor(34942033920) params_per_gpu tensor(1377452032)
total_params after FSDP FullyShardedDataParallel(
  (_fsdp_wrapped_module): ClimaX(
    (token_embeds): ModuleList(
      (0-511): 512 x PatchEmbed(
        (proj): Conv2d(1, 8192, kernel_size=(4, 4), stride=(4, 4))
        (norm): Identity()
      )
    )
    (var_agg): FullyShardedDataParallel(
      (_fsdp_wrapped_module): CheckpointWrapper(
        (_checkpoint_wrapped_module): VariableMapping_Attention(
          (q): Linear(in_features=8192, out_features=256, bias=False)
          (kv): Linear(in_features=8192, out_features=512, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=8192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (var_agg_per_rank): CustomCrossAttention(
      (q): Linear(in_features=8192, out_features=8192, bias=False)
      (kv): Linear(in_features=8192, out_features=16384, bias=False)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=8192, out_features=8192, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (lead_time_embed): Linear(in_features=1, out_features=8192, bias=True)
    (pos_drop): Dropout(p=0.1, inplace=False)
    (blocks): ModuleList(
      (0-31): 32 x FullyShardedDataParallel(
        (_fsdp_wrapped_module): CheckpointWrapper(
          (_checkpoint_wrapped_module): Block(
            (norm1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=8192, out_features=768, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=256, out_features=8192, bias=True)
              (proj_drop): Dropout(p=0.1, inplace=False)
            )
            (ls1): Identity()
            (norm2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=8192, out_features=1024, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.1, inplace=False)
              (fc2): Linear(in_features=1024, out_features=8192, bias=True)
            )
            (ls2): Identity()
          )
        )
      )
    )
    (norm): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
    (head): Sequential(
      (0): Linear(in_features=8192, out_features=8192, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=8192, out_features=8192, bias=True)
      (3): GELU(approximate='none')
      (4): Pred_Rearrange()
      (5): Linear(in_features=8192, out_features=8192, bias=True)
    )
  )
)
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
rank  31 lister_train reset: mpi-esm 9 9 9
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
rank  0 lister_train reset: mpi-esm 9 9 9
use_ddstore is : 0
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
memory stats reset, ready to track
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
epoch-train:  0 batch_idx 0 world_rank 0  loss  nan
epoch-train:  0 batch_idx 1 world_rank 0  loss  nan
epoch-train:  0 batch_idx 2 world_rank 0  loss  nan
[2025-03-12 17:09:46,642] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:09:46,642] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:09:46,643] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:09:46,643] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:09:46,643] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:09:46,643] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:09:46,644] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:09:46,644] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:09:46,644] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:09:46,645] [INFO] [profiler.py:80:start_profile] Flops profiler started
epoch-train:  0 batch_idx 3 world_rank 0  loss  nan
[2025-03-12 17:09:46,645] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:09:46,646] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:09:46,646] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:09:46,646] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:09:46,647] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:09:46,648] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:09:46,648] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:09:46,648] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:09:46,648] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:09:46,648] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:09:46,649] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:09:46,649] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:09:46,650] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:09:46,651] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:09:46,653] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:09:46,654] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:09:46,655] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:09:46,656] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:09:46,656] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:09:46,656] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:09:46,657] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:09:46,659] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:09:47,887] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:09:47,887] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:09:47,888] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:09:47,888] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:09:47,888] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:09:47,888] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:09:47,888] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:09:47,888] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:09:47,888] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:09:47,888] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:09:47,888] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:09:47,888] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:09:47,888] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:09:47,888] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:09:47,888] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:09:47,888] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:09:47,889] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:09:47,889] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:09:47,889] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:09:47,889] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:09:47,889] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:09:47,889] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:09:47,889] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:09:47,889] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:09:47,890] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:09:47,890] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:09:47,890] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:09:47,890] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:09:47,891] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:09:47,891] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:09:47,892] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:09:47,892] [INFO] [profiler.py:226:end_profile] Flops profiler finished
global rank 5: ddp rank 0 iter_start,iter_end = 0 8
global rank 5: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 11: ddp rank 0 iter_start,iter_end = 0 8
global rank 11: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 6: ddp rank 0 iter_start,iter_end = 0 8
global rank 6: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 22: ddp rank 0 iter_start,iter_end = 0 8
global rank 22: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 1: ddp rank 0 iter_start,iter_end = 0 8
global rank 1: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 19: ddp rank 0 iter_start,iter_end = 0 8
global rank 19: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 20: ddp rank 0 iter_start,iter_end = 0 8
global rank 20: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 21: ddp rank 0 iter_start,iter_end = 0 8
global rank 21: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 17: ddp rank 0 iter_start,iter_end = 0 8
global rank 17: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 30: ddp rank 0 iter_start,iter_end = 0 8
global rank 30: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 29: ddp rank 0 iter_start,iter_end = 0 8
global rank 29: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 31: ddp rank 0 iter_start,iter_end = 0 8
global rank 31: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 0: ddp rank 0 iter_start,iter_end = 0 8
global rank 0: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 25: ddp rank 0 iter_start,iter_end = 0 8
global rank 25: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 10: ddp rank 0 iter_start,iter_end = 0 8
global rank 10: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 27: ddp rank 0 iter_start,iter_end = 0 8
global rank 27: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 26: ddp rank 0 iter_start,iter_end = 0 8
global rank 26: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 4: ddp rank 0 iter_start,iter_end = 0 8
global rank 4: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 14: ddp rank 0 iter_start,iter_end = 0 8
global rank 14: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 3: ddp rank 0 iter_start,iter_end = 0 8
global rank 3: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 7: ddp rank 0 iter_start,iter_end = 0 8
global rank 7: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 12: ddp rank 0 iter_start,iter_end = 0 8
global rank 12: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 18: ddp rank 0 iter_start,iter_end = 0 8
global rank 18: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 28: ddp rank 0 iter_start,iter_end = 0 8
global rank 28: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 9: ddp rank 0 iter_start,iter_end = 0 8
global rank 9: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 23: ddp rank 0 iter_start,iter_end = 0 8
global rank 23: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 16: ddp rank 0 iter_start,iter_end = 0 8
global rank 16: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 24: ddp rank 0 iter_start,iter_end = 0 8
global rank 24: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 15: ddp rank 0 iter_start,iter_end = 0 8
global rank 15: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 2: ddp rank 0 iter_start,iter_end = 0 8
global rank 2: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 13: ddp rank 0 iter_start,iter_end = 0 8
global rank 13: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 8: ddp rank 0 iter_start,iter_end = 0 8
global rank 8: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
MUST: Model 1377.452032 M TFLOPS: 32.3250566983888

--> cuda max reserved memory = 46.6504
--> max reserved percentage = 72.91 %

--> cuda max memory allocated = 32.3449
--> max allocated percentage = 50.55 %

--> peak active memory = 39.8791
--> peak active memory 62.33 %

cudaMalloc retries = 0
cuda OOM = 0

HERE1 Namespace(arch='orbit_hier', batch_size=2, channels=512, depth=32, embed_dim=8192, fa2=True, fsdp_size=1, imagex=128, imagey=256, max_epochs=1, num_heads=32, real=False, seq_par_size=1, simple_ddp_size=1, tensor_par_size=32, yaml_config='configs/ERA5-100million-91variables.yaml') 46.6504 32.3250566983888 tensor(34942033920)
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:10:02,646] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:10:02,646] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:10:02,646] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:10:02,646] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:10:02,646] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:10:02,646] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:10:02,646] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:10:02,646] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:10:02,646] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:10:02,646] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:10:02,646] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:10:02,646] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:10:02,646] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:10:02,646] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:10:02,646] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:10:02,646] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:10:02,647] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:10:02,647] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:10:02,647] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:10:02,647] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:10:02,647] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:10:02,647] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:10:02,647] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:10:02,647] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:10:02,652] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:10:02,652] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:10:02,652] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:10:02,652] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:10:02,652] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:10:02,652] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:10:02,652] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:10:02,652] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
HERE0 Namespace(arch='orbit_hier', batch_size=2, channels=768, depth=32, embed_dim=8192, fa2=True, fsdp_size=1, imagex=128, imagey=256, max_epochs=1, num_heads=32, real=False, seq_par_size=1, simple_ddp_size=1, tensor_par_size=32, yaml_config='configs/ERA5-100million-91variables.yaml')
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
rank 3 Before initialize parallelism groups
rank 4 Before initialize parallelism groups
rank 7 Before initialize parallelism groups
rank 6 Before initialize parallelism groups
rank 5 Before initialize parallelism groups
Using dist.init_process_group. world_size  32
rank 27 Before initialize parallelism groups
rank 28 Before initialize parallelism groups
rank 29 Before initialize parallelism groups
Using dist.init_process_group. world_size  32
rank 8 Before initialize parallelism groups
rank 9 Before initialize parallelism groups
rank 10 Before initialize parallelism groups
rank 11 Before initialize parallelism groups
rank 12 Before initialize parallelism groups
rank 14 Before initialize parallelism groups
rank 15 Before initialize parallelism groups
Using dist.init_process_group. world_size  32
rank 2 Before initialize parallelism groups
rank 1 Before initialize parallelism groups
rank 6 After initialize parallelism groups
rank 3 After initialize parallelism groups
rank 4 After initialize parallelism groups
rank 7 After initialize parallelism groups
rank 13 Before initialize parallelism groups
rank 5 After initialize parallelism groups
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
config_path  configs/ERA5-100million-91variables.yaml
Using dist.init_process_group. world_size  32
rank 29 After initialize parallelism groups
rank 28 After initialize parallelism groups
rank 27 After initialize parallelism groups
rank 16 Before initialize parallelism groups
rank 19 Before initialize parallelism groups
rank 20 Before initialize parallelism groups
rank 21 Before initialize parallelism groups
rank 22 Before initialize parallelism groups
rank 17 Before initialize parallelism groups
rank 23 Before initialize parallelism groups
rank 18 Before initialize parallelism groups
rank 2 After initialize parallelism groups
rank 11 After initialize parallelism groups
rank 8 After initialize parallelism groups
rank 12 After initialize parallelism groups
rank 10 After initialize parallelism groups
rank 9 After initialize parallelism groups
rank 14 After initialize parallelism groups
rank 15 After initialize parallelism groups
rank 1 After initialize parallelism groups
rank 13 After initialize parallelism groups
rank 26 Before initialize parallelism groups
rank 21 After initialize parallelism groups
rank 18 After initialize parallelism groups
rank 19 After initialize parallelism groups
rank 23 After initialize parallelism groups
rank 16 After initialize parallelism groups
rank 20 After initialize parallelism groups
rank 22 After initialize parallelism groups
rank 17 After initialize parallelism groups
rank 25 Before initialize parallelism groups
{'seed_everything': 42, 'trainer': {'checkpoint_path': '/lustre/orion/stf218/world-shared/atsaris/tmp/tmp_checkpoints', 'checkpoint_filename': 'multi_last', 'resume_from_checkpoint': False}, 'parallelism': {'cpu_offloading': False}, 'model': {'lr': 2e-05, 'beta_1': 0.9, 'beta_2': 0.95, 'weight_decay': '1e-5', 'warmup_steps': 1000, 'max_steps': 20000, 'warmup_start_lr': '1e-8', 'eta_min': '1e-8', 'net': {'class_path': 'climax.arch.ClimaX', 'init_args': {'default_vars': ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_70', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000'], 'patch_size': 4, 'decoder_depth': 2, 'mlp_ratio': 4, 'drop_path': 0.1, 'drop_rate': 0.1, 'aggregated_variables': 1}}}, 'data': {'dict_root_dirs': {'mpi-esm': '/lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0'}, 'dict_start_idx': {'mpi-esm': 0}, 'dict_end_idx': {'mpi-esm': 1}, 'dict_in_variables': {'mpi-esm': ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000']}, 'dict_out_variables': {'mpi-esm': ['geopotential_500', 'temperature_850', '2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind']}, 'dict_max_predict_ranges': {'mpi-esm': 24}, 'dict_random_lead_time': {'mpi-esm': False}, 'dict_hrs_each_step': {'mpi-esm': 1}, 'dict_buffer_sizes': {'mpi-esm': 1}, 'num_workers': 1, 'pin_memory': False}}
rank 30 Before initialize parallelism groups
rank 31 Before initialize parallelism groups
rank 24 Before initialize parallelism groups
rank 26 After initialize parallelism groups
max_epochs 1 data_par_size 1 fsdp_size 1 simple_ddp_size 1 tensor_par_size 32 seq_par_size 1 cpu_offloading False
lr  2e-05 beta_1  0.9 beta_2 0.95 weight_decay 1e-05 class_path climax.arch.ClimaX default_vars ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_70', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000']
img_size_x 128 img_size_y 256 patch_size 4 emb_dim 8192 depth 32 decoder_depth 2 num_heads 32 mlp_ratio 4 drop_path 0.1 drop_rate 0.1 aggregated_variables 1
dict_root_dirs {'mpi-esm': '/lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0'} dict_start_idx {'mpi-esm': 0} dict_end_idx {'mpi-esm': 1} batch_size 2 num_workers 1
warmup_steps 1000 max_steps 20000 warmup_start_lr 1e-08 eta_min 1e-08
checkpoint_path /lustre/orion/stf218/world-shared/atsaris/tmp/tmp_checkpoints checkpoint_filename multi_last resume_from_checkpoint False
rank 0 Before initialize parallelism groups
rank 25 After initialize parallelism groups
rank 30 After initialize parallelism groups
rank 31 After initialize parallelism groups
rank 24 After initialize parallelism groups
rank 0 After initialize parallelism groups
rank 19 After initialize model
rank 19 Before the second dist.barrier()
rank 29 After initialize model
rank 29 Before the second dist.barrier()
rank 13 After initialize model
rank 13 Before the second dist.barrier()
rank 0 After initialize model
rank 0 Before the second dist.barrier()
rank 3 After initialize model
rank 3 Before the second dist.barrier()
rank 26 After initialize model
rank 26 Before the second dist.barrier()
rank 27 After initialize model
rank 27 Before the second dist.barrier()
rank 28 After initialize model
rank 28 Before the second dist.barrier()
rank 22 After initialize model
rank 22 Before the second dist.barrier()
rank 8 After initialize model
rank 8 Before the second dist.barrier()
rank 25 After initialize model
rank 25 Before the second dist.barrier()
rank 17 After initialize model
rank 17 Before the second dist.barrier()
rank 15 After initialize model
rank 15 Before the second dist.barrier()
rank 20 After initialize model
rank 20 Before the second dist.barrier()
rank 11 After initialize model
rank 11 Before the second dist.barrier()
rank 6 After initialize model
rank 6 Before the second dist.barrier()
rank 23 After initialize model
rank 23 Before the second dist.barrier()
rank 1 After initialize model
rank 1 Before the second dist.barrier()
rank 2 After initialize model
rank 2 Before the second dist.barrier()
rank 4 After initialize model
rank 9 After initialize model
rank 4 Before the second dist.barrier()
rank 9 Before the second dist.barrier()
rank 14 After initialize model
rank 14 Before the second dist.barrier()
rank 16 After initialize model
rank 16 Before the second dist.barrier()
rank 12 After initialize model
rank 12 Before the second dist.barrier()
rank 21 After initialize model
rank 24 After initialize model
rank 21 Before the second dist.barrier()
rank 24 Before the second dist.barrier()
rank 7 After initialize model
rank 7 Before the second dist.barrier()
rank 5 After initialize model
rank 5 Before the second dist.barrier()
rank 10 After initialize model
rank 10 Before the second dist.barrier()
rank 30 After initialize model
rank 30 Before the second dist.barrier()
rank 31 After initialize model
rank 31 Before the second dist.barrier()
rank 18 After initialize model
rank 18 Before the second dist.barrier()
rank 0 After the second dist.barrier()
rank 16 After the second dist.barrier()
rank 17 After the second dist.barrier()
rank 8 After the second dist.barrier()
rank 9 After the second dist.barrier()
rank 12 After the second dist.barrier()
rank 13 After the second dist.barrier()
rank 1 After the second dist.barrier()
parameter name  var_embed  requires_gradient  True size torch.Size([1, 768, 8192])
rank 11 After the second dist.barrier()
rank 15 After the second dist.barrier()
rank 10 After the second dist.barrier()
rank 21 After the second dist.barrier()
rank 4 After the second dist.barrier()
rank 14 After the second dist.barrier()
rank 24 After the second dist.barrier()
rank 5 After the second dist.barrier()
rank 25 After the second dist.barrier()
rank 20 After the second dist.barrier()
parameter name  var_query  requires_gradient  True size torch.Size([1, 1, 8192])
parameter name  var_query_per_rank  requires_gradient  True size torch.Size([1, 1, 8192])
parameter name  pos_embed  requires_gradient  True size torch.Size([1, 2048, 8192])
rank 22 After the second dist.barrier()
rank 6 After the second dist.barrier()
rank 23 After the second dist.barrier()
parameter name  token_embeds.0.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.0.proj.bias  requires_gradient  True size torch.Size([8192])
rank 29 After the second dist.barrier()
parameter name  token_embeds.1.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.1.proj.bias  requires_gradient  True size torch.Size([8192])
rank 7 After the second dist.barrier()
parameter name  token_embeds.2.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.2.proj.bias  requires_gradient  True size torch.Size([8192])
rank 28 After the second dist.barrier()
parameter name  token_embeds.3.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.3.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.4.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
rank 19 After the second dist.barrier()
rank 2 After the second dist.barrier()
rank 3 After the second dist.barrier()
rank 30 After the second dist.barrier()
parameter name  token_embeds.4.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.5.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.5.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.6.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.6.proj.bias  requires_gradient  True size torch.Size([8192])
rank 18 After the second dist.barrier()
parameter name  token_embeds.7.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.7.proj.bias  requires_gradient  True size torch.Size([8192])
rank 31 After the second dist.barrier()
parameter name  token_embeds.8.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.8.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.9.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.9.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.10.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
rank 27 After the second dist.barrier()
parameter name  token_embeds.10.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.11.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.11.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.12.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
rank 26 After the second dist.barrier()
parameter name  token_embeds.12.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.13.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.13.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.14.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.14.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.15.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.15.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.16.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.16.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.17.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.17.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.18.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.18.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.19.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.19.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.20.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.20.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.21.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.21.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.22.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.22.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.23.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.23.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.24.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.24.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.25.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.25.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.26.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.26.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.27.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.27.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.28.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.28.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.29.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.29.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.30.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.30.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.31.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.31.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.32.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.32.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.33.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.33.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.34.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.34.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.35.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.35.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.36.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.36.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.37.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.37.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.38.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.38.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.39.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.39.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.40.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.40.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.41.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.41.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.42.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.42.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.43.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.43.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.44.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.44.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.45.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.45.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.46.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.46.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.47.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.47.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.48.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.48.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.49.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.49.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.50.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.50.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.51.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.51.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.52.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.52.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.53.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.53.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.54.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.54.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.55.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.55.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.56.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.56.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.57.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.57.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.58.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.58.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.59.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.59.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.60.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.60.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.61.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.61.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.62.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.62.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.63.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.63.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.64.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.64.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.65.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.65.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.66.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.66.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.67.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.67.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.68.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.68.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.69.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.69.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.70.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.70.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.71.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.71.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.72.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.72.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.73.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.73.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.74.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.74.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.75.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.75.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.76.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.76.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.77.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.77.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.78.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.78.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.79.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.79.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.80.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.80.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.81.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.81.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.82.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.82.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.83.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.83.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.84.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.84.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.85.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.85.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.86.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.86.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.87.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.87.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.88.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.88.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.89.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.89.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.90.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.90.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.91.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.91.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.92.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.92.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.93.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.93.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.94.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.94.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.95.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.95.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.96.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.96.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.97.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.97.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.98.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.98.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.99.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.99.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.100.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.100.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.101.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.101.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.102.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.102.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.103.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.103.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.104.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.104.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.105.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.105.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.106.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.106.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.107.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.107.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.108.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.108.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.109.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.109.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.110.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.110.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.111.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.111.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.112.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.112.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.113.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.113.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.114.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.114.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.115.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.115.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.116.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.116.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.117.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.117.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.118.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.118.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.119.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.119.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.120.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.120.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.121.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.121.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.122.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.122.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.123.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.123.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.124.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.124.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.125.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.125.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.126.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.126.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.127.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.127.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.128.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.128.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.129.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.129.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.130.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.130.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.131.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.131.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.132.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.132.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.133.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.133.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.134.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.134.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.135.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.135.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.136.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.136.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.137.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.137.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.138.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.138.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.139.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.139.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.140.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.140.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.141.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.141.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.142.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.142.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.143.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.143.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.144.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.144.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.145.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.145.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.146.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.146.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.147.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.147.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.148.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.148.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.149.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.149.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.150.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.150.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.151.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.151.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.152.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.152.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.153.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.153.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.154.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.154.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.155.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.155.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.156.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.156.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.157.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.157.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.158.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.158.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.159.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.159.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.160.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.160.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.161.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.161.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.162.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.162.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.163.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.163.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.164.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.164.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.165.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.165.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.166.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.166.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.167.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.167.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.168.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.168.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.169.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.169.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.170.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.170.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.171.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.171.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.172.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.172.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.173.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.173.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.174.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.174.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.175.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.175.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.176.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.176.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.177.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.177.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.178.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.178.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.179.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.179.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.180.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.180.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.181.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.181.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.182.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.182.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.183.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.183.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.184.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.184.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.185.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.185.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.186.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.186.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.187.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.187.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.188.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.188.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.189.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.189.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.190.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.190.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.191.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.191.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.192.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.192.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.193.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.193.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.194.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.194.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.195.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.195.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.196.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.196.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.197.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.197.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.198.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.198.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.199.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.199.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.200.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.200.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.201.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.201.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.202.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.202.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.203.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.203.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.204.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.204.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.205.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.205.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.206.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.206.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.207.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.207.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.208.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.208.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.209.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.209.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.210.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.210.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.211.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.211.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.212.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.212.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.213.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.213.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.214.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.214.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.215.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.215.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.216.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.216.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.217.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.217.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.218.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.218.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.219.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.219.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.220.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.220.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.221.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.221.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.222.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.222.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.223.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.223.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.224.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.224.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.225.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.225.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.226.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.226.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.227.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.227.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.228.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.228.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.229.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.229.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.230.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.230.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.231.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.231.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.232.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.232.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.233.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.233.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.234.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.234.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.235.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.235.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.236.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.236.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.237.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.237.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.238.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.238.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.239.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.239.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.240.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.240.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.241.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.241.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.242.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.242.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.243.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.243.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.244.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.244.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.245.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.245.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.246.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.246.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.247.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.247.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.248.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.248.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.249.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.249.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.250.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.250.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.251.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.251.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.252.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.252.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.253.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.253.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.254.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.254.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.255.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.255.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.256.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.256.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.257.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.257.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.258.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.258.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.259.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.259.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.260.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.260.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.261.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.261.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.262.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.262.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.263.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.263.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.264.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.264.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.265.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.265.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.266.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.266.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.267.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.267.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.268.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.268.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.269.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.269.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.270.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.270.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.271.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.271.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.272.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.272.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.273.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.273.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.274.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.274.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.275.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.275.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.276.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.276.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.277.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.277.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.278.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.278.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.279.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.279.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.280.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.280.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.281.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.281.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.282.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.282.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.283.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.283.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.284.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.284.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.285.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.285.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.286.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.286.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.287.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.287.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.288.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.288.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.289.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.289.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.290.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.290.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.291.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.291.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.292.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.292.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.293.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.293.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.294.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.294.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.295.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.295.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.296.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.296.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.297.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.297.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.298.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.298.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.299.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.299.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.300.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.300.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.301.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.301.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.302.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.302.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.303.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.303.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.304.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.304.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.305.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.305.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.306.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.306.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.307.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.307.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.308.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.308.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.309.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.309.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.310.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.310.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.311.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.311.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.312.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.312.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.313.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.313.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.314.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.314.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.315.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.315.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.316.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.316.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.317.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.317.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.318.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.318.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.319.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.319.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.320.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.320.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.321.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.321.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.322.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.322.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.323.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.323.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.324.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.324.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.325.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.325.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.326.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.326.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.327.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.327.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.328.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.328.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.329.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.329.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.330.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.330.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.331.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.331.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.332.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.332.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.333.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.333.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.334.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.334.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.335.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.335.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.336.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.336.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.337.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.337.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.338.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.338.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.339.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.339.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.340.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.340.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.341.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.341.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.342.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.342.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.343.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.343.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.344.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.344.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.345.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.345.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.346.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.346.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.347.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.347.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.348.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.348.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.349.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.349.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.350.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.350.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.351.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.351.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.352.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.352.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.353.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.353.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.354.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.354.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.355.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.355.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.356.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.356.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.357.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.357.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.358.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.358.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.359.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.359.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.360.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.360.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.361.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.361.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.362.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.362.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.363.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.363.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.364.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.364.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.365.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.365.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.366.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.366.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.367.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.367.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.368.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.368.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.369.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.369.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.370.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.370.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.371.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.371.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.372.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.372.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.373.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.373.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.374.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.374.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.375.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.375.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.376.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.376.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.377.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.377.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.378.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.378.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.379.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.379.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.380.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.380.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.381.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.381.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.382.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.382.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.383.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.383.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.384.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.384.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.385.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.385.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.386.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.386.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.387.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.387.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.388.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.388.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.389.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.389.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.390.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.390.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.391.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.391.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.392.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.392.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.393.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.393.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.394.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.394.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.395.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.395.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.396.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.396.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.397.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.397.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.398.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.398.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.399.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.399.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.400.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.400.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.401.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.401.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.402.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.402.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.403.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.403.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.404.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.404.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.405.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.405.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.406.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.406.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.407.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.407.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.408.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.408.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.409.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.409.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.410.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.410.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.411.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.411.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.412.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.412.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.413.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.413.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.414.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.414.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.415.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.415.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.416.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.416.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.417.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.417.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.418.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.418.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.419.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.419.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.420.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.420.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.421.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.421.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.422.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.422.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.423.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.423.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.424.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.424.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.425.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.425.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.426.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.426.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.427.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.427.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.428.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.428.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.429.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.429.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.430.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.430.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.431.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.431.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.432.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.432.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.433.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.433.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.434.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.434.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.435.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.435.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.436.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.436.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.437.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.437.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.438.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.438.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.439.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.439.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.440.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.440.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.441.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.441.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.442.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.442.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.443.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.443.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.444.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.444.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.445.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.445.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.446.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.446.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.447.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.447.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.448.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.448.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.449.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.449.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.450.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.450.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.451.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.451.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.452.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.452.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.453.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.453.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.454.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.454.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.455.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.455.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.456.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.456.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.457.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.457.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.458.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.458.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.459.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.459.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.460.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.460.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.461.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.461.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.462.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.462.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.463.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.463.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.464.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.464.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.465.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.465.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.466.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.466.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.467.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.467.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.468.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.468.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.469.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.469.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.470.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.470.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.471.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.471.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.472.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.472.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.473.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.473.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.474.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.474.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.475.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.475.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.476.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.476.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.477.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.477.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.478.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.478.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.479.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.479.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.480.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.480.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.481.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.481.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.482.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.482.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.483.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.483.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.484.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.484.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.485.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.485.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.486.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.486.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.487.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.487.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.488.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.488.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.489.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.489.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.490.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.490.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.491.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.491.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.492.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.492.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.493.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.493.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.494.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.494.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.495.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.495.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.496.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.496.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.497.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.497.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.498.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.498.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.499.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.499.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.500.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.500.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.501.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.501.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.502.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.502.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.503.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.503.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.504.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.504.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.505.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.505.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.506.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.506.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.507.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.507.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.508.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.508.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.509.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.509.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.510.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.510.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.511.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.511.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.512.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.512.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.513.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.513.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.514.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.514.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.515.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.515.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.516.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.516.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.517.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.517.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.518.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.518.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.519.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.519.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.520.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.520.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.521.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.521.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.522.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.522.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.523.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.523.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.524.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.524.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.525.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.525.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.526.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.526.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.527.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.527.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.528.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.528.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.529.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.529.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.530.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.530.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.531.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.531.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.532.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.532.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.533.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.533.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.534.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.534.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.535.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.535.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.536.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.536.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.537.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.537.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.538.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.538.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.539.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.539.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.540.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.540.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.541.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.541.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.542.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.542.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.543.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.543.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.544.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.544.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.545.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.545.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.546.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.546.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.547.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.547.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.548.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.548.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.549.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.549.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.550.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.550.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.551.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.551.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.552.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.552.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.553.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.553.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.554.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.554.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.555.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.555.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.556.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.556.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.557.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.557.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.558.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.558.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.559.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.559.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.560.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.560.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.561.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.561.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.562.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.562.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.563.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.563.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.564.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.564.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.565.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.565.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.566.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.566.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.567.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.567.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.568.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.568.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.569.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.569.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.570.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.570.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.571.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.571.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.572.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.572.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.573.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.573.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.574.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.574.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.575.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.575.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.576.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.576.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.577.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.577.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.578.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.578.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.579.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.579.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.580.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.580.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.581.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.581.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.582.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.582.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.583.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.583.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.584.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.584.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.585.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.585.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.586.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.586.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.587.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.587.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.588.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.588.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.589.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.589.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.590.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.590.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.591.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.591.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.592.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.592.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.593.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.593.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.594.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.594.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.595.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.595.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.596.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.596.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.597.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.597.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.598.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.598.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.599.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.599.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.600.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.600.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.601.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.601.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.602.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.602.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.603.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.603.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.604.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.604.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.605.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.605.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.606.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.606.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.607.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.607.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.608.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.608.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.609.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.609.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.610.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.610.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.611.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.611.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.612.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.612.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.613.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.613.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.614.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.614.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.615.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.615.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.616.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.616.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.617.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.617.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.618.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.618.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.619.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.619.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.620.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.620.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.621.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.621.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.622.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.622.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.623.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.623.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.624.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.624.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.625.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.625.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.626.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.626.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.627.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.627.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.628.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.628.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.629.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.629.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.630.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.630.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.631.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.631.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.632.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.632.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.633.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.633.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.634.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.634.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.635.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.635.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.636.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.636.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.637.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.637.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.638.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.638.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.639.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.639.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.640.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.640.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.641.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.641.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.642.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.642.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.643.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.643.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.644.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.644.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.645.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.645.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.646.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.646.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.647.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.647.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.648.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.648.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.649.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.649.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.650.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.650.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.651.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.651.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.652.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.652.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.653.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.653.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.654.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.654.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.655.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.655.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.656.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.656.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.657.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.657.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.658.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.658.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.659.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.659.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.660.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.660.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.661.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.661.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.662.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.662.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.663.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.663.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.664.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.664.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.665.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.665.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.666.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.666.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.667.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.667.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.668.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.668.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.669.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.669.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.670.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.670.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.671.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.671.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.672.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.672.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.673.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.673.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.674.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.674.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.675.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.675.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.676.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.676.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.677.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.677.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.678.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.678.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.679.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.679.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.680.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.680.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.681.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.681.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.682.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.682.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.683.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.683.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.684.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.684.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.685.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.685.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.686.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.686.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.687.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.687.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.688.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.688.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.689.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.689.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.690.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.690.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.691.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.691.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.692.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.692.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.693.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.693.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.694.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.694.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.695.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.695.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.696.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.696.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.697.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.697.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.698.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.698.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.699.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.699.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.700.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.700.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.701.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.701.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.702.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.702.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.703.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.703.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.704.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.704.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.705.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.705.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.706.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.706.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.707.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.707.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.708.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.708.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.709.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.709.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.710.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.710.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.711.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.711.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.712.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.712.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.713.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.713.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.714.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.714.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.715.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.715.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.716.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.716.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.717.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.717.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.718.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.718.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.719.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.719.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.720.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.720.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.721.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.721.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.722.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.722.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.723.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.723.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.724.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.724.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.725.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.725.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.726.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.726.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.727.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.727.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.728.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.728.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.729.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.729.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.730.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.730.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.731.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.731.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.732.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.732.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.733.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.733.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.734.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.734.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.735.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.735.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.736.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.736.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.737.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.737.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.738.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.738.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.739.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.739.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.740.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.740.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.741.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.741.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.742.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.742.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.743.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.743.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.744.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.744.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.745.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.745.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.746.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.746.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.747.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.747.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.748.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.748.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.749.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.749.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.750.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.750.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.751.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.751.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.752.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.752.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.753.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.753.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.754.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.754.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.755.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.755.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.756.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.756.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.757.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.757.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.758.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.758.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.759.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.759.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.760.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.760.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.761.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.761.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.762.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.762.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.763.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.763.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.764.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.764.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.765.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.765.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.766.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.766.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.767.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.767.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  var_agg.q.weight  requires_gradient  True size torch.Size([256, 8192])
parameter name  var_agg.kv.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  var_agg.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  var_agg.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  var_agg_per_rank.q.weight  requires_gradient  True size torch.Size([8192, 8192])
parameter name  var_agg_per_rank.kv.weight  requires_gradient  True size torch.Size([16384, 8192])
parameter name  var_agg_per_rank.proj.weight  requires_gradient  True size torch.Size([8192, 8192])
parameter name  var_agg_per_rank.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  lead_time_embed.weight  requires_gradient  True size torch.Size([8192, 1])
parameter name  lead_time_embed.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.0.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.0.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.0.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.0.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.0.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.0.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.1.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.1.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.1.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.1.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.1.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.1.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.2.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.2.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.2.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.2.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.2.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.2.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.3.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.3.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.3.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.3.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.3.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.3.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.4.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.4.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.4.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.4.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.4.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.4.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.5.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.5.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.5.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.5.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.5.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.5.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.6.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.6.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.6.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.6.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.6.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.6.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.7.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.7.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.7.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.7.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.7.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.7.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.8.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.8.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.8.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.8.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.8.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.8.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.9.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.9.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.9.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.9.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.9.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.9.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.10.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.10.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.10.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.10.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.10.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.10.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.11.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.11.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.11.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.11.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.11.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.11.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.12.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.12.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.12.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.12.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.12.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.12.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.13.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.13.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.13.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.13.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.13.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.13.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.14.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.14.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.14.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.14.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.14.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.14.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.15.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.15.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.15.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.15.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.15.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.15.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.16.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.16.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.16.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.16.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.16.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.16.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.17.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.17.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.17.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.17.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.17.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.17.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.18.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.18.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.18.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.18.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.18.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.18.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.19.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.19.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.19.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.19.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.19.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.19.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.20.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.20.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.20.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.20.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.20.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.20.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.21.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.21.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.21.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.21.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.21.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.21.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.22.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.22.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.22.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.22.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.22.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.22.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.23.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.23.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.23.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.23.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.23.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.23.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.24.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.24.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.24.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.24.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.24.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.24.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.25.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.25.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.25.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.25.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.25.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.25.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.26.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.26.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.26.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.26.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.26.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.26.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.27.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.27.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.27.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.27.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.27.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.27.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.28.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.28.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.28.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.28.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.28.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.28.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.29.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.29.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.29.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.29.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.29.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.29.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.30.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.30.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.30.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.30.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.30.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.30.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.31.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.31.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.31.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.31.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.31.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.31.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  norm.weight  requires_gradient  True size torch.Size([8192])
parameter name  norm.bias  requires_gradient  True size torch.Size([8192])
parameter name  head.0.weight  requires_gradient  True size torch.Size([8192, 8192])
parameter name  head.0.bias  requires_gradient  True size torch.Size([8192])
parameter name  head.2.weight  requires_gradient  True size torch.Size([8192, 8192])
parameter name  head.2.bias  requires_gradient  True size torch.Size([8192])
parameter name  head.5.weight  requires_gradient  True size torch.Size([12288, 8192])
parameter name  head.5.bias  requires_gradient  True size torch.Size([12288])
total_params before FSDP tensor(35013341184) params_per_gpu tensor(1448759296)
total_params after FSDP FullyShardedDataParallel(
  (_fsdp_wrapped_module): ClimaX(
    (token_embeds): ModuleList(
      (0-767): 768 x PatchEmbed(
        (proj): Conv2d(1, 8192, kernel_size=(4, 4), stride=(4, 4))
        (norm): Identity()
      )
    )
    (var_agg): FullyShardedDataParallel(
      (_fsdp_wrapped_module): CheckpointWrapper(
        (_checkpoint_wrapped_module): VariableMapping_Attention(
          (q): Linear(in_features=8192, out_features=256, bias=False)
          (kv): Linear(in_features=8192, out_features=512, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=8192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (var_agg_per_rank): CustomCrossAttention(
      (q): Linear(in_features=8192, out_features=8192, bias=False)
      (kv): Linear(in_features=8192, out_features=16384, bias=False)
      (attn_drop): Dropout(p=0.0, inplace=False)
      (proj): Linear(in_features=8192, out_features=8192, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
    )
    (lead_time_embed): Linear(in_features=1, out_features=8192, bias=True)
    (pos_drop): Dropout(p=0.1, inplace=False)
    (blocks): ModuleList(
      (0-31): 32 x FullyShardedDataParallel(
        (_fsdp_wrapped_module): CheckpointWrapper(
          (_checkpoint_wrapped_module): Block(
            (norm1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=8192, out_features=768, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=256, out_features=8192, bias=True)
              (proj_drop): Dropout(p=0.1, inplace=False)
            )
            (ls1): Identity()
            (norm2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=8192, out_features=1024, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.1, inplace=False)
              (fc2): Linear(in_features=1024, out_features=8192, bias=True)
            )
            (ls2): Identity()
          )
        )
      )
    )
    (norm): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
    (head): Sequential(
      (0): Linear(in_features=8192, out_features=8192, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=8192, out_features=8192, bias=True)
      (3): GELU(approximate='none')
      (4): Pred_Rearrange()
      (5): Linear(in_features=8192, out_features=12288, bias=True)
    )
  )
)
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
rank  31 lister_train reset: mpi-esm 9 9 9
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
rank  0 lister_train reset: mpi-esm 9 9 9
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
memory stats reset, ready to track
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
epoch-train:  0 batch_idx 0 world_rank 0  loss  nan
epoch-train:  0 batch_idx 1 world_rank 0  loss  nan
epoch-train:  0 batch_idx 2 world_rank 0  loss  nan
[2025-03-12 17:10:58,640] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:10:58,641] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:10:58,642] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:10:58,643] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:10:58,644] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:10:58,644] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:10:58,645] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:10:58,646] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:10:58,646] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:10:58,646] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:10:58,647] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:10:58,647] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:10:58,647] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:10:58,647] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:10:58,648] [INFO] [profiler.py:80:start_profile] Flops profiler started
epoch-train:  0 batch_idx 3 world_rank 0  loss  nan
[2025-03-12 17:10:58,650] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:10:58,650] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:10:58,650] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:10:58,651] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:10:58,651] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:10:58,651] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:10:58,651] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:10:58,652] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:10:58,653] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:10:58,653] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:10:58,654] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:10:58,657] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:10:58,657] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:10:58,658] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:10:58,662] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:10:58,663] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:10:58,663] [INFO] [profiler.py:80:start_profile] Flops profiler started
[2025-03-12 17:11:00,503] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:11:00,503] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:11:00,504] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:11:00,504] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:11:00,504] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:11:00,504] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:11:00,504] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:11:00,504] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:11:00,504] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:11:00,504] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:11:00,504] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:11:00,504] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:11:00,504] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:11:00,505] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:11:00,505] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:11:00,505] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:11:00,505] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:11:00,505] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:11:00,505] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:11:00,506] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:11:00,506] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:11:00,506] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:11:00,506] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:11:00,506] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:11:00,506] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:11:00,506] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:11:00,507] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:11:00,507] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:11:00,507] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:11:00,507] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:11:00,508] [INFO] [profiler.py:226:end_profile] Flops profiler finished
[2025-03-12 17:11:00,509] [INFO] [profiler.py:226:end_profile] Flops profiler finished
global rank 25: ddp rank 0 iter_start,iter_end = 0 8
global rank 25: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 27: ddp rank 0 iter_start,iter_end = 0 8
global rank 27: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 19: ddp rank 0 iter_start,iter_end = 0 8
global rank 19: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 2: ddp rank 0 iter_start,iter_end = 0 8
global rank 2: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 12: ddp rank 0 iter_start,iter_end = 0 8
global rank 12: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 6: ddp rank 0 iter_start,iter_end = 0 8
global rank 6: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 1: ddp rank 0 iter_start,iter_end = 0 8
global rank 1: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 20: ddp rank 0 iter_start,iter_end = 0 8
global rank 20: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 22: ddp rank 0 iter_start,iter_end = 0 8
global rank 22: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 28: ddp rank 0 iter_start,iter_end = 0 8
global rank 28: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 5: ddp rank 0 iter_start,iter_end = 0 8
global rank 5: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 31: ddp rank 0 iter_start,iter_end = 0 8
global rank 31: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 21: ddp rank 0 iter_start,iter_end = 0 8
global rank 21: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 8: ddp rank 0 iter_start,iter_end = 0 8
global rank 8: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 11: ddp rank 0 iter_start,iter_end = 0 8
global rank 11: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 10: ddp rank 0 iter_start,iter_end = 0 8
global rank 10: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 23: ddp rank 0 iter_start,iter_end = 0 8
global rank 23: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 0: ddp rank 0 iter_start,iter_end = 0 8
global rank 0: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 29: ddp rank 0 iter_start,iter_end = 0 8
global rank 29: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 18: ddp rank 0 iter_start,iter_end = 0 8
global rank 18: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 15: ddp rank 0 iter_start,iter_end = 0 8
global rank 15: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 24: ddp rank 0 iter_start,iter_end = 0 8
global rank 24: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 3: ddp rank 0 iter_start,iter_end = 0 8
global rank 3: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 26: ddp rank 0 iter_start,iter_end = 0 8
global rank 26: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 7: ddp rank 0 iter_start,iter_end = 0 8
global rank 7: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 9: ddp rank 0 iter_start,iter_end = 0 8
global rank 9: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 4: ddp rank 0 iter_start,iter_end = 0 8
global rank 4: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 13: ddp rank 0 iter_start,iter_end = 0 8
global rank 13: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 30: ddp rank 0 iter_start,iter_end = 0 8
global rank 30: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 16: ddp rank 0 iter_start,iter_end = 0 8
global rank 16: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 17: ddp rank 0 iter_start,iter_end = 0 8
global rank 17: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 14: ddp rank 0 iter_start,iter_end = 0 8
global rank 14: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
MUST: Model 1448.759296 M TFLOPS: 34.11046818290831

--> cuda max reserved memory = 51.834
--> max reserved percentage = 81.01 %

--> cuda max memory allocated = 35.6361
--> max allocated percentage = 55.69 %

--> peak active memory = 43.1703
--> peak active memory 67.47 %

cudaMalloc retries = 0
cuda OOM = 0

HERE1 Namespace(arch='orbit_hier', batch_size=2, channels=768, depth=32, embed_dim=8192, fa2=True, fsdp_size=1, imagex=128, imagey=256, max_epochs=1, num_heads=32, real=False, seq_par_size=1, simple_ddp_size=1, tensor_par_size=32, yaml_config='configs/ERA5-100million-91variables.yaml') 51.834 34.11046818290831 tensor(35013341184)
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:11:15,196] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:11:15,196] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:11:15,196] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:11:15,196] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:11:15,196] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:11:15,196] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:11:15,196] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:11:15,196] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:11:15,202] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:11:15,202] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:11:15,202] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:11:15,202] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:11:15,202] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:11:15,202] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:11:15,202] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:11:15,202] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:11:15,202] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:11:15,202] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:11:15,202] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:11:15,202] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:11:15,202] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:11:15,202] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:11:15,202] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:11:15,202] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:11:15,202] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:11:15,202] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:11:15,202] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:11:15,202] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:11:15,202] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:11:15,202] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:11:15,202] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:11:15,202] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
HERE0 Namespace(arch='orbit_hier_token_noagg_self', batch_size=2, channels=256, depth=32, embed_dim=8192, fa2=True, fsdp_size=1, imagex=128, imagey=256, max_epochs=1, num_heads=32, real=False, seq_par_size=1, simple_ddp_size=1, tensor_par_size=32, yaml_config='configs/ERA5-100million-91variables.yaml')
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
rank 8 Before initialize parallelism groups
rank 11 Before initialize parallelism groups
rank 13 Before initialize parallelism groups
rank 14 Before initialize parallelism groups
rank 1 Before initialize parallelism groups
rank 2 Before initialize parallelism groups
rank 3 Before initialize parallelism groups
rank 4 Before initialize parallelism groups
Using dist.init_process_group. world_size  32
rank 26 Before initialize parallelism groups
rank 29 Before initialize parallelism groups
rank 27 Before initialize parallelism groups
rank 28 Before initialize parallelism groups
Using dist.init_process_group. world_size  32
rank 7 Before initialize parallelism groups
rank 10 Before initialize parallelism groups
rank 12 Before initialize parallelism groups
rank 25 Before initialize parallelism groups
rank 31 Before initialize parallelism groups
rank 5 Before initialize parallelism groups
rank 20 Before initialize parallelism groups
rank 22 Before initialize parallelism groups
rank 16 Before initialize parallelism groups
rank 13 After initialize parallelism groups
rank 14 After initialize parallelism groups
rank 8 After initialize parallelism groups
rank 11 After initialize parallelism groups
rank 6 Before initialize parallelism groups
rank 26 After initialize parallelism groups
rank 28 After initialize parallelism groups
rank 2 After initialize parallelism groups
rank 27 After initialize parallelism groups
rank 3 After initialize parallelism groups
rank 29 After initialize parallelism groups
rank 1 After initialize parallelism groups
Using dist.init_process_group. world_size  32
config_path  configs/ERA5-100million-91variables.yaml
rank 4 After initialize parallelism groups
rank 7 After initialize parallelism groups
rank 10 After initialize parallelism groups
rank 25 After initialize parallelism groups
rank 31 After initialize parallelism groups
rank 12 After initialize parallelism groups
rank 19 Before initialize parallelism groups
rank 23 Before initialize parallelism groups
rank 5 After initialize parallelism groups
rank 18 Before initialize parallelism groups
rank 24 Before initialize parallelism groups
rank 21 Before initialize parallelism groups
rank 20 After initialize parallelism groups
rank 15 Before initialize parallelism groups
rank 22 After initialize parallelism groups
rank 16 After initialize parallelism groups
rank 6 After initialize parallelism groups
rank 30 Before initialize parallelism groups
rank 17 Before initialize parallelism groups
rank 9 Before initialize parallelism groups
rank 19 After initialize parallelism groups
rank 23 After initialize parallelism groups
rank 24 After initialize parallelism groups
rank 18 After initialize parallelism groups
rank 15 After initialize parallelism groups
rank 21 After initialize parallelism groups
{'seed_everything': 42, 'trainer': {'checkpoint_path': '/lustre/orion/stf218/world-shared/atsaris/tmp/tmp_checkpoints', 'checkpoint_filename': 'multi_last', 'resume_from_checkpoint': False}, 'parallelism': {'cpu_offloading': False}, 'model': {'lr': 2e-05, 'beta_1': 0.9, 'beta_2': 0.95, 'weight_decay': '1e-5', 'warmup_steps': 1000, 'max_steps': 20000, 'warmup_start_lr': '1e-8', 'eta_min': '1e-8', 'net': {'class_path': 'climax.arch.ClimaX', 'init_args': {'default_vars': ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_70', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000'], 'patch_size': 4, 'decoder_depth': 2, 'mlp_ratio': 4, 'drop_path': 0.1, 'drop_rate': 0.1, 'aggregated_variables': 1}}}, 'data': {'dict_root_dirs': {'mpi-esm': '/lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0'}, 'dict_start_idx': {'mpi-esm': 0}, 'dict_end_idx': {'mpi-esm': 1}, 'dict_in_variables': {'mpi-esm': ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000']}, 'dict_out_variables': {'mpi-esm': ['geopotential_500', 'temperature_850', '2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind']}, 'dict_max_predict_ranges': {'mpi-esm': 24}, 'dict_random_lead_time': {'mpi-esm': False}, 'dict_hrs_each_step': {'mpi-esm': 1}, 'dict_buffer_sizes': {'mpi-esm': 1}, 'num_workers': 1, 'pin_memory': False}}
rank 30 After initialize parallelism groups
rank 9 After initialize parallelism groups
rank 17 After initialize parallelism groups
max_epochs 1 data_par_size 1 fsdp_size 1 simple_ddp_size 1 tensor_par_size 32 seq_par_size 1 cpu_offloading False
lr  2e-05 beta_1  0.9 beta_2 0.95 weight_decay 1e-05 class_path climax.arch.ClimaX default_vars ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_70', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000']
img_size_x 128 img_size_y 256 patch_size 4 emb_dim 8192 depth 32 decoder_depth 2 num_heads 32 mlp_ratio 4 drop_path 0.1 drop_rate 0.1 aggregated_variables 1
dict_root_dirs {'mpi-esm': '/lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0'} dict_start_idx {'mpi-esm': 0} dict_end_idx {'mpi-esm': 1} batch_size 2 num_workers 1
warmup_steps 1000 max_steps 20000 warmup_start_lr 1e-08 eta_min 1e-08
checkpoint_path /lustre/orion/stf218/world-shared/atsaris/tmp/tmp_checkpoints checkpoint_filename multi_last resume_from_checkpoint False
rank 0 Before initialize parallelism groups
rank 0 After initialize parallelism groups
rank 13 After initialize model
rank 13 Before the second dist.barrier()
rank 2 After initialize model
rank 2 Before the second dist.barrier()
rank 26 After initialize model
rank 26 Before the second dist.barrier()
rank 22 After initialize model
rank 22 Before the second dist.barrier()
rank 20 After initialize model
rank 20 Before the second dist.barrier()
rank 6 After initialize model
rank 6 Before the second dist.barrier()
rank 1 After initialize model
rank 1 Before the second dist.barrier()
rank 4 After initialize model
rank 4 Before the second dist.barrier()
rank 7 After initialize model
rank 7 Before the second dist.barrier()
rank 5 After initialize model
rank 5 Before the second dist.barrier()
rank 27 After initialize model
rank 27 Before the second dist.barrier()
rank 19 After initialize model
rank 19 Before the second dist.barrier()
rank 3 After initialize model
rank 3 Before the second dist.barrier()
rank 18 After initialize model
rank 18 Before the second dist.barrier()
rank 23 After initialize model
rank 23 Before the second dist.barrier()
rank 15 After initialize model
rank 15 Before the second dist.barrier()
rank 0 After initialize model
rank 0 Before the second dist.barrier()
rank 30 After initialize model
rank 30 Before the second dist.barrier()
rank 11 After initialize model
rank 11 Before the second dist.barrier()
rank 24 After initialize model
rank 24 Before the second dist.barrier()
rank 17 After initialize model
rank 17 Before the second dist.barrier()
rank 21 After initialize model
rank 21 Before the second dist.barrier()
rank 14 After initialize model
rank 14 Before the second dist.barrier()
rank 12 After initialize model
rank 12 Before the second dist.barrier()
rank 16 After initialize model
rank 16 Before the second dist.barrier()
rank 28 After initialize model
rank 28 Before the second dist.barrier()
rank 10 After initialize model
rank 10 Before the second dist.barrier()
rank 9 After initialize model
rank 9 Before the second dist.barrier()
rank 8 After initialize model
rank 8 Before the second dist.barrier()
rank 25 After initialize model
rank 25 Before the second dist.barrier()
rank 31 After initialize model
rank 31 Before the second dist.barrier()
rank 29 After initialize model
rank 29 Before the second dist.barrier()
rank 16 After the second dist.barrier()
rank 17 After the second dist.barrier()
rank 8 After the second dist.barrier()
rank 0 After the second dist.barrier()
rank 20 After the second dist.barrier()
rank 9 After the second dist.barrier()
rank 24 After the second dist.barrier()
rank 1 After the second dist.barrier()
rank 21 After the second dist.barrier()
rank 14 After the second dist.barrier()
rank 25 After the second dist.barrier()
rank 5 After the second dist.barrier()
rank 22 After the second dist.barrier()
rank 15 After the second dist.barrier()
rank 28 After the second dist.barrier()
parameter name  var_embed  requires_gradient  True size torch.Size([1, 256, 8192])
rank 23 After the second dist.barrier()
rank 10 After the second dist.barrier()
rank 29 After the second dist.barrier()
rank 4 After the second dist.barrier()
rank 19 After the second dist.barrier()
rank 12 After the second dist.barrier()
rank 30 After the second dist.barrier()
rank 6 After the second dist.barrier()
rank 18 After the second dist.barrier()
rank 13 After the second dist.barrier()
rank 31 After the second dist.barrier()
parameter name  var_query  requires_gradient  True size torch.Size([1, 1, 8192])
rank 11 After the second dist.barrier()
rank 27 After the second dist.barrier()
rank 7 After the second dist.barrier()
rank 26 After the second dist.barrier()
parameter name  pos_embed  requires_gradient  True size torch.Size([1, 2048, 8192])
parameter name  token_embeds.0.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.0.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.1.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
rank 3 After the second dist.barrier()
parameter name  token_embeds.1.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.2.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.2.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.3.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.3.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.4.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
rank 2 After the second dist.barrier()
parameter name  token_embeds.4.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.5.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.5.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.6.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.6.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.7.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.7.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.8.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.8.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.9.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.9.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.10.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.10.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.11.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.11.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.12.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.12.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.13.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.13.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.14.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.14.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.15.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.15.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.16.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.16.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.17.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.17.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.18.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.18.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.19.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.19.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.20.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.20.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.21.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.21.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.22.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.22.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.23.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.23.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.24.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.24.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.25.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.25.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.26.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.26.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.27.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.27.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.28.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.28.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.29.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.29.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.30.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.30.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.31.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.31.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.32.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.32.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.33.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.33.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.34.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.34.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.35.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.35.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.36.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.36.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.37.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.37.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.38.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.38.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.39.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.39.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.40.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.40.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.41.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.41.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.42.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.42.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.43.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.43.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.44.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.44.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.45.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.45.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.46.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.46.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.47.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.47.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.48.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.48.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.49.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.49.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.50.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.50.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.51.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.51.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.52.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.52.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.53.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.53.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.54.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.54.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.55.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.55.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.56.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.56.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.57.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.57.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.58.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.58.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.59.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.59.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.60.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.60.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.61.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.61.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.62.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.62.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.63.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.63.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.64.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.64.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.65.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.65.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.66.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.66.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.67.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.67.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.68.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.68.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.69.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.69.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.70.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.70.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.71.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.71.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.72.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.72.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.73.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.73.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.74.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.74.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.75.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.75.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.76.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.76.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.77.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.77.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.78.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.78.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.79.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.79.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.80.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.80.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.81.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.81.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.82.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.82.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.83.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.83.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.84.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.84.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.85.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.85.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.86.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.86.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.87.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.87.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.88.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.88.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.89.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.89.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.90.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.90.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.91.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.91.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.92.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.92.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.93.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.93.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.94.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.94.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.95.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.95.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.96.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.96.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.97.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.97.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.98.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.98.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.99.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.99.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.100.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.100.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.101.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.101.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.102.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.102.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.103.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.103.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.104.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.104.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.105.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.105.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.106.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.106.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.107.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.107.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.108.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.108.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.109.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.109.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.110.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.110.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.111.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.111.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.112.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.112.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.113.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.113.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.114.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.114.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.115.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.115.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.116.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.116.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.117.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.117.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.118.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.118.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.119.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.119.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.120.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.120.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.121.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.121.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.122.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.122.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.123.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.123.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.124.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.124.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.125.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.125.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.126.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.126.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.127.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.127.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.128.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.128.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.129.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.129.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.130.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.130.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.131.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.131.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.132.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.132.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.133.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.133.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.134.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.134.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.135.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.135.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.136.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.136.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.137.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.137.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.138.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.138.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.139.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.139.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.140.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.140.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.141.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.141.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.142.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.142.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.143.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.143.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.144.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.144.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.145.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.145.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.146.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.146.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.147.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.147.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.148.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.148.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.149.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.149.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.150.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.150.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.151.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.151.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.152.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.152.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.153.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.153.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.154.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.154.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.155.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.155.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.156.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.156.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.157.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.157.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.158.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.158.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.159.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.159.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.160.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.160.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.161.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.161.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.162.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.162.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.163.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.163.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.164.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.164.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.165.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.165.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.166.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.166.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.167.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.167.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.168.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.168.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.169.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.169.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.170.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.170.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.171.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.171.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.172.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.172.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.173.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.173.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.174.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.174.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.175.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.175.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.176.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.176.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.177.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.177.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.178.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.178.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.179.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.179.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.180.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.180.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.181.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.181.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.182.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.182.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.183.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.183.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.184.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.184.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.185.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.185.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.186.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.186.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.187.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.187.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.188.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.188.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.189.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.189.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.190.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.190.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.191.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.191.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.192.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.192.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.193.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.193.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.194.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.194.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.195.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.195.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.196.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.196.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.197.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.197.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.198.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.198.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.199.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.199.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.200.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.200.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.201.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.201.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.202.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.202.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.203.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.203.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.204.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.204.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.205.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.205.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.206.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.206.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.207.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.207.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.208.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.208.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.209.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.209.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.210.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.210.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.211.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.211.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.212.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.212.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.213.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.213.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.214.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.214.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.215.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.215.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.216.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.216.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.217.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.217.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.218.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.218.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.219.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.219.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.220.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.220.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.221.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.221.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.222.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.222.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.223.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.223.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.224.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.224.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.225.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.225.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.226.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.226.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.227.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.227.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.228.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.228.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.229.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.229.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.230.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.230.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.231.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.231.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.232.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.232.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.233.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.233.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.234.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.234.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.235.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.235.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.236.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.236.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.237.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.237.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.238.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.238.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.239.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.239.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.240.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.240.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.241.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.241.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.242.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.242.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.243.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.243.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.244.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.244.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.245.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.245.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.246.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.246.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.247.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.247.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.248.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.248.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.249.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.249.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.250.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.250.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.251.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.251.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.252.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.252.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.253.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.253.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.254.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.254.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.255.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.255.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  var_agg.q.weight  requires_gradient  True size torch.Size([256, 8192])
parameter name  var_agg.kv.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  var_agg.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  var_agg.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  lead_time_embed.weight  requires_gradient  True size torch.Size([8192, 1])
parameter name  lead_time_embed.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.0.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.0.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.0.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.0.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.0.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.0.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.1.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.1.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.1.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.1.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.1.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.1.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.2.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.2.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.2.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.2.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.2.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.2.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.3.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.3.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.3.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.3.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.3.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.3.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.4.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.4.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.4.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.4.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.4.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.4.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.5.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.5.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.5.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.5.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.5.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.5.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.6.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.6.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.6.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.6.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.6.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.6.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.7.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.7.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.7.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.7.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.7.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.7.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.8.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.8.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.8.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.8.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.8.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.8.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.9.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.9.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.9.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.9.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.9.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.9.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.10.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.10.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.10.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.10.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.10.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.10.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.11.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.11.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.11.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.11.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.11.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.11.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.12.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.12.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.12.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.12.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.12.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.12.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.13.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.13.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.13.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.13.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.13.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.13.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.14.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.14.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.14.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.14.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.14.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.14.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.15.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.15.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.15.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.15.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.15.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.15.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.16.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.16.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.16.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.16.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.16.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.16.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.17.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.17.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.17.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.17.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.17.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.17.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.18.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.18.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.18.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.18.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.18.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.18.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.19.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.19.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.19.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.19.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.19.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.19.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.20.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.20.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.20.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.20.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.20.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.20.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.21.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.21.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.21.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.21.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.21.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.21.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.22.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.22.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.22.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.22.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.22.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.22.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.23.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.23.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.23.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.23.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.23.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.23.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.24.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.24.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.24.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.24.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.24.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.24.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.25.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.25.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.25.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.25.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.25.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.25.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.26.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.26.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.26.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.26.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.26.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.26.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.27.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.27.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.27.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.27.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.27.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.27.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.28.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.28.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.28.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.28.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.28.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.28.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.29.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.29.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.29.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.29.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.29.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.29.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.30.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.30.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.30.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.30.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.30.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.30.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.31.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.31.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.31.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.31.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.31.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.31.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  norm.weight  requires_gradient  True size torch.Size([8192])
parameter name  norm.bias  requires_gradient  True size torch.Size([8192])
parameter name  head.0.weight  requires_gradient  True size torch.Size([8192, 8192])
parameter name  head.0.bias  requires_gradient  True size torch.Size([8192])
parameter name  head.2.weight  requires_gradient  True size torch.Size([8192, 8192])
parameter name  head.2.bias  requires_gradient  True size torch.Size([8192])
parameter name  head.5.weight  requires_gradient  True size torch.Size([4096, 8192])
parameter name  head.5.bias  requires_gradient  True size torch.Size([4096])
total_params before FSDP tensor(26280521728) params_per_gpu tensor(1037692928)
total_params after FSDP FullyShardedDataParallel(
  (_fsdp_wrapped_module): ClimaX(
    (token_embeds): ModuleList(
      (0-255): 256 x PatchEmbed(
        (proj): Conv2d(1, 8192, kernel_size=(4, 4), stride=(4, 4))
        (norm): Identity()
      )
    )
    (var_agg): FullyShardedDataParallel(
      (_fsdp_wrapped_module): CheckpointWrapper(
        (_checkpoint_wrapped_module): VariableMapping_Attention(
          (q): Linear(in_features=8192, out_features=256, bias=False)
          (kv): Linear(in_features=8192, out_features=512, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=8192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (lead_time_embed): Linear(in_features=1, out_features=8192, bias=True)
    (pos_drop): Dropout(p=0.1, inplace=False)
    (blocks): ModuleList(
      (0-31): 32 x FullyShardedDataParallel(
        (_fsdp_wrapped_module): CheckpointWrapper(
          (_checkpoint_wrapped_module): Block(
            (norm1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=8192, out_features=768, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=256, out_features=8192, bias=True)
              (proj_drop): Dropout(p=0.1, inplace=False)
            )
            (ls1): Identity()
            (norm2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=8192, out_features=1024, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.1, inplace=False)
              (fc2): Linear(in_features=1024, out_features=8192, bias=True)
            )
            (ls2): Identity()
          )
        )
      )
    )
    (norm): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
    (head): Sequential(
      (0): Linear(in_features=8192, out_features=8192, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=8192, out_features=8192, bias=True)
      (3): GELU(approximate='none')
      (4): Pred_Rearrange()
      (5): Linear(in_features=8192, out_features=4096, bias=True)
    )
  )
)
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
rank  31 lister_train reset: mpi-esm 9 9 9
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
rank  0 lister_train reset: mpi-esm 9 9 9
use_ddstore is : 0
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
memory stats reset, ready to track
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
3 torch.Size([2, 2048, 8192])
epoch-train:  0 batch_idx 0 world_rank 0  loss  6.875
global rank 0: ddp rank 0 iter_start,iter_end = 0 8
global rank 0: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 21: ddp rank 0 iter_start,iter_end = 0 8
global rank 21: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 17: ddp rank 0 iter_start,iter_end = 0 8
global rank 17: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 27: ddp rank 0 iter_start,iter_end = 0 8
global rank 27: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 25: ddp rank 0 iter_start,iter_end = 0 8
global rank 25: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 29: ddp rank 0 iter_start,iter_end = 0 8
global rank 29: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 8: ddp rank 0 iter_start,iter_end = 0 8
global rank 8: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 3: ddp rank 0 iter_start,iter_end = 0 8
global rank 3: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 5: ddp rank 0 iter_start,iter_end = 0 8
global rank 5: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 16: ddp rank 0 iter_start,iter_end = 0 8
global rank 16: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 6: ddp rank 0 iter_start,iter_end = 0 8
global rank 6: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 22: ddp rank 0 iter_start,iter_end = 0 8
global rank 22: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 1: ddp rank 0 iter_start,iter_end = 0 8
global rank 1: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 20: ddp rank 0 iter_start,iter_end = 0 8
global rank 20: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 19: ddp rank 0 iter_start,iter_end = 0 8
global rank 19: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 7: ddp rank 0 iter_start,iter_end = 0 8
global rank 7: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 4: ddp rank 0 iter_start,iter_end = 0 8
global rank 4: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 24: ddp rank 0 iter_start,iter_end = 0 8
global rank 24: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 26: ddp rank 0 iter_start,iter_end = 0 8
global rank 26: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 30: ddp rank 0 iter_start,iter_end = 0 8
global rank 30: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 28: ddp rank 0 iter_start,iter_end = 0 8
global rank 28: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 2: ddp rank 0 iter_start,iter_end = 0 8
global rank 2: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 18: ddp rank 0 iter_start,iter_end = 0 8
global rank 18: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 23: ddp rank 0 iter_start,iter_end = 0 8
global rank 23: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 31: ddp rank 0 iter_start,iter_end = 0 8
global rank 31: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 14: ddp rank 0 iter_start,iter_end = 0 8
global rank 14: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 10: ddp rank 0 iter_start,iter_end = 0 8
global rank 10: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
3 torch.Size([2, 2048, 8192])
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
3 torch.Size([2, 2048, 8192])
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
3 torch.Size([2, 2048, 8192])
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
3 torch.Size([2, 2048, 8192])
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
3 torch.Size([2, 2048, 8192])
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
3 torch.Size([2, 2048, 8192])
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
3 torch.Size([2, 2048, 8192])
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
3 torch.Size([2, 2048, 8192])
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
[rank3]: Traceback (most recent call last):
[rank3]:   File "train.py", line 986, in <module>
[rank3]:     main(args, device, world_size, world_rank, local_rank)
[rank3]:   File "train.py", line 890, in main
[rank3]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank3]:   File "train.py", line 372, in training_step
[rank3]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank3]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank3]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank3]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank3]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank3]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 358, in forward_encoder
[rank3]:     x = self.aggregate_variables(x)
[rank3]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 302, in aggregate_variables
[rank3]:     x = x.flatten(0, 1)  # BxL, V, D
[rank3]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 16.00 GiB. GPU 3 has a total capacity of 63.98 GiB of which 5.21 GiB is free. Of the allocated memory 48.49 GiB is allocated by PyTorch, and 8.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank5]: Traceback (most recent call last):
[rank5]:   File "train.py", line 986, in <module>
[rank5]:     main(args, device, world_size, world_rank, local_rank)
[rank5]:   File "train.py", line 890, in main
[rank5]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank5]:   File "train.py", line 372, in training_step
[rank5]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank5]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank5]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank5]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank5]:     return forward_call(*args, **kwargs)
[rank5]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank5]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank5]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 358, in forward_encoder
[rank5]:     x = self.aggregate_variables(x)
[rank5]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 302, in aggregate_variables
[rank5]:     x = x.flatten(0, 1)  # BxL, V, D
[rank5]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 16.00 GiB. GPU 5 has a total capacity of 63.98 GiB of which 5.40 GiB is free. Of the allocated memory 48.49 GiB is allocated by PyTorch, and 8.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank16]: Traceback (most recent call last):
[rank16]:   File "train.py", line 986, in <module>
[rank16]:     main(args, device, world_size, world_rank, local_rank)
[rank16]:   File "train.py", line 890, in main
[rank16]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank16]:   File "train.py", line 372, in training_step
[rank16]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank16]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank16]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank16]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank16]:     return self._call_impl(*args, **kwargs)
[rank6]: Traceback (most recent call last):
[rank6]:   File "train.py", line 986, in <module>
[rank6]:     main(args, device, world_size, world_rank, local_rank)
[rank6]:   File "train.py", line 890, in main
[rank6]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank6]:   File "train.py", line 372, in training_step
[rank6]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank6]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank6]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank6]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank16]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank16]:     return forward_call(*args, **kwargs)
[rank16]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank16]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank16]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 358, in forward_encoder
[rank16]:     x = self.aggregate_variables(x)
[rank16]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 302, in aggregate_variables
[rank16]:     x = x.flatten(0, 1)  # BxL, V, D
[rank6]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank6]:     return forward_call(*args, **kwargs)
[rank6]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank6]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank6]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 358, in forward_encoder
[rank6]:     x = self.aggregate_variables(x)
[rank6]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 302, in aggregate_variables
[rank6]:     x = x.flatten(0, 1)  # BxL, V, D
[rank16]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 63.98 GiB of which 5.40 GiB is free. Of the allocated memory 48.49 GiB is allocated by PyTorch, and 8.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank6]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 16.00 GiB. GPU 6 has a total capacity of 63.98 GiB of which 5.21 GiB is free. Of the allocated memory 48.49 GiB is allocated by PyTorch, and 8.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank22]: Traceback (most recent call last):
[rank22]:   File "train.py", line 986, in <module>
[rank22]:     main(args, device, world_size, world_rank, local_rank)
[rank22]:   File "train.py", line 890, in main
[rank22]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank22]:   File "train.py", line 372, in training_step
[rank22]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank22]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank22]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank22]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank22]:     return self._call_impl(*args, **kwargs)
[rank1]: Traceback (most recent call last):
[rank1]:   File "train.py", line 986, in <module>
[rank1]:     main(args, device, world_size, world_rank, local_rank)
[rank1]:   File "train.py", line 890, in main
[rank1]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank1]:   File "train.py", line 372, in training_step
[rank1]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank1]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank1]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank1]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank22]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank22]:     return forward_call(*args, **kwargs)
[rank22]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank22]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank22]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 358, in forward_encoder
[rank22]:     x = self.aggregate_variables(x)
[rank22]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 302, in aggregate_variables
[rank22]:     x = x.flatten(0, 1)  # BxL, V, D
[rank1]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank1]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank1]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 358, in forward_encoder
[rank1]:     x = self.aggregate_variables(x)
[rank1]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 302, in aggregate_variables
[rank1]:     x = x.flatten(0, 1)  # BxL, V, D
[rank22]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 16.00 GiB. GPU 6 has a total capacity of 63.98 GiB of which 5.21 GiB is free. Of the allocated memory 48.49 GiB is allocated by PyTorch, and 8.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank1]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 16.00 GiB. GPU 1 has a total capacity of 63.98 GiB of which 5.21 GiB is free. Of the allocated memory 48.49 GiB is allocated by PyTorch, and 8.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank20]: Traceback (most recent call last):
[rank20]:   File "train.py", line 986, in <module>
[rank20]:     main(args, device, world_size, world_rank, local_rank)
[rank20]:   File "train.py", line 890, in main
[rank20]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank20]:   File "train.py", line 372, in training_step
[rank20]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank20]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank20]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank20]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank20]:     return self._call_impl(*args, **kwargs)
[rank20]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank20]:     return forward_call(*args, **kwargs)
[rank20]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank20]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank20]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 358, in forward_encoder
[rank20]:     x = self.aggregate_variables(x)
[rank20]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 302, in aggregate_variables
[rank20]:     x = x.flatten(0, 1)  # BxL, V, D
[rank20]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 16.00 GiB. GPU 4 has a total capacity of 63.98 GiB of which 5.21 GiB is free. Of the allocated memory 48.49 GiB is allocated by PyTorch, and 8.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank19]: Traceback (most recent call last):
[rank19]:   File "train.py", line 986, in <module>
[rank19]:     main(args, device, world_size, world_rank, local_rank)
[rank19]:   File "train.py", line 890, in main
[rank19]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank19]:   File "train.py", line 372, in training_step
[rank19]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank19]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank19]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank19]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank19]:     return self._call_impl(*args, **kwargs)
[rank19]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank19]:     return forward_call(*args, **kwargs)
[rank19]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank19]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank19]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 358, in forward_encoder
[rank19]:     x = self.aggregate_variables(x)
[rank19]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 302, in aggregate_variables
[rank19]:     x = x.flatten(0, 1)  # BxL, V, D
[rank19]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 16.00 GiB. GPU 3 has a total capacity of 63.98 GiB of which 5.21 GiB is free. Of the allocated memory 48.49 GiB is allocated by PyTorch, and 8.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
3 torch.Size([2, 2048, 8192])
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
3 torch.Size([2, 2048, 8192])
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
3 torch.Size([2, 2048, 8192])
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
3 torch.Size([2, 2048, 8192])
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
3 torch.Size([2, 2048, 8192])
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
3 torch.Size([2, 2048, 8192])
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
3 torch.Size([2, 2048, 8192])
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank4]: Traceback (most recent call last):
[rank4]:   File "train.py", line 986, in <module>
[rank4]:     main(args, device, world_size, world_rank, local_rank)
[rank4]:   File "train.py", line 890, in main
[rank4]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank4]:   File "train.py", line 372, in training_step
[rank4]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank4]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank4]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank4]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank4]:     return forward_call(*args, **kwargs)
[rank4]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank4]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank4]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 358, in forward_encoder
[rank4]:     x = self.aggregate_variables(x)
[rank4]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 302, in aggregate_variables
[rank4]:     x = x.flatten(0, 1)  # BxL, V, D
[rank4]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 16.00 GiB. GPU 4 has a total capacity of 63.98 GiB of which 4.71 GiB is free. Of the allocated memory 48.49 GiB is allocated by PyTorch, and 9.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank17]: Traceback (most recent call last):
[rank17]:   File "train.py", line 986, in <module>
[rank17]:     main(args, device, world_size, world_rank, local_rank)
[rank17]:   File "train.py", line 890, in main
[rank17]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank17]:   File "train.py", line 372, in training_step
[rank17]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank17]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank17]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank17]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank17]:     return self._call_impl(*args, **kwargs)
[rank7]: Traceback (most recent call last):
[rank7]:   File "train.py", line 986, in <module>
[rank7]:     main(args, device, world_size, world_rank, local_rank)
[rank7]:   File "train.py", line 890, in main
[rank7]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank7]:   File "train.py", line 372, in training_step
[rank7]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank7]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank7]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank7]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank17]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank17]:     return forward_call(*args, **kwargs)
[rank17]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank17]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank17]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 358, in forward_encoder
[rank17]:     x = self.aggregate_variables(x)
[rank17]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 302, in aggregate_variables
[rank17]:     x = x.flatten(0, 1)  # BxL, V, D
[rank7]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank7]:     return forward_call(*args, **kwargs)
[rank7]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank7]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank7]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 358, in forward_encoder
[rank7]:     x = self.aggregate_variables(x)
[rank7]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 302, in aggregate_variables
[rank7]:     x = x.flatten(0, 1)  # BxL, V, D
[rank17]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 16.00 GiB. GPU 1 has a total capacity of 63.98 GiB of which 4.71 GiB is free. Of the allocated memory 48.49 GiB is allocated by PyTorch, and 9.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank7]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 16.00 GiB. GPU 7 has a total capacity of 63.98 GiB of which 5.40 GiB is free. Of the allocated memory 48.49 GiB is allocated by PyTorch, and 8.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
3 torch.Size([2, 2048, 8192])
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
3 torch.Size([2, 2048, 8192])
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]: Traceback (most recent call last):
[rank0]:   File "train.py", line 986, in <module>
[rank0]:     main(args, device, world_size, world_rank, local_rank)
[rank0]:   File "train.py", line 890, in main
[rank0]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank0]:   File "train.py", line 372, in training_step
[rank0]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank0]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank0]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank0]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank0]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank0]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 358, in forward_encoder
[rank0]:     x = self.aggregate_variables(x)
[rank0]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 302, in aggregate_variables
[rank0]:     x = x.flatten(0, 1)  # BxL, V, D
[rank0]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 63.98 GiB of which 5.40 GiB is free. Of the allocated memory 48.49 GiB is allocated by PyTorch, and 8.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank2]: Traceback (most recent call last):
[rank2]:   File "train.py", line 986, in <module>
[rank2]:     main(args, device, world_size, world_rank, local_rank)
[rank2]:   File "train.py", line 890, in main
[rank2]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank2]:   File "train.py", line 372, in training_step
[rank2]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank2]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank2]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank2]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank2]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank2]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 358, in forward_encoder
[rank2]:     x = self.aggregate_variables(x)
[rank2]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 302, in aggregate_variables
[rank2]:     x = x.flatten(0, 1)  # BxL, V, D
[rank2]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 16.00 GiB. GPU 2 has a total capacity of 63.98 GiB of which 5.40 GiB is free. Of the allocated memory 48.49 GiB is allocated by PyTorch, and 8.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
3 torch.Size([2, 2048, 8192])
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
3 torch.Size([2, 2048, 8192])
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
3 torch.Size([2, 2048, 8192])
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank21]: Traceback (most recent call last):
[rank21]:   File "train.py", line 986, in <module>
[rank21]:     main(args, device, world_size, world_rank, local_rank)
[rank21]:   File "train.py", line 890, in main
[rank21]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank21]:   File "train.py", line 372, in training_step
[rank21]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank21]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank21]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank21]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank21]:     return self._call_impl(*args, **kwargs)
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
3 torch.Size([2, 2048, 8192])
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank21]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank21]:     return forward_call(*args, **kwargs)
[rank21]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank21]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank21]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 358, in forward_encoder
[rank21]:     x = self.aggregate_variables(x)
[rank21]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 302, in aggregate_variables
[rank21]:     x = x.flatten(0, 1)  # BxL, V, D
[rank21]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 16.00 GiB. GPU 5 has a total capacity of 63.98 GiB of which 5.40 GiB is free. Of the allocated memory 48.49 GiB is allocated by PyTorch, and 8.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank28]: Traceback (most recent call last):
[rank28]:   File "train.py", line 986, in <module>
[rank28]:     main(args, device, world_size, world_rank, local_rank)
[rank28]:   File "train.py", line 890, in main
[rank28]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank28]:   File "train.py", line 372, in training_step
[rank28]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank28]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank28]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank28]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank28]:     return self._call_impl(*args, **kwargs)
[rank28]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank28]:     return forward_call(*args, **kwargs)
[rank28]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank28]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank28]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 358, in forward_encoder
[rank28]:     x = self.aggregate_variables(x)
[rank28]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 302, in aggregate_variables
[rank28]:     x = x.flatten(0, 1)  # BxL, V, D
[rank28]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 16.00 GiB. GPU 4 has a total capacity of 63.98 GiB of which 5.27 GiB is free. Of the allocated memory 48.49 GiB is allocated by PyTorch, and 8.80 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank24]: Traceback (most recent call last):
[rank24]:   File "train.py", line 986, in <module>
[rank24]:     main(args, device, world_size, world_rank, local_rank)
[rank24]:   File "train.py", line 890, in main
[rank24]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank24]:   File "train.py", line 372, in training_step
[rank24]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank24]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank24]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank24]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank24]:     return self._call_impl(*args, **kwargs)
[rank24]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank24]:     return forward_call(*args, **kwargs)
[rank24]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank24]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank24]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 358, in forward_encoder
[rank24]:     x = self.aggregate_variables(x)
[rank24]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 302, in aggregate_variables
[rank24]:     x = x.flatten(0, 1)  # BxL, V, D
[rank24]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 63.98 GiB of which 5.46 GiB is free. Of the allocated memory 48.49 GiB is allocated by PyTorch, and 8.80 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank30]: Traceback (most recent call last):
[rank30]:   File "train.py", line 986, in <module>
[rank30]:     main(args, device, world_size, world_rank, local_rank)
[rank30]:   File "train.py", line 890, in main
[rank30]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank30]:   File "train.py", line 372, in training_step
[rank30]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank30]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank30]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank30]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank30]:     return self._call_impl(*args, **kwargs)
[rank30]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank30]:     return forward_call(*args, **kwargs)
[rank30]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank30]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank30]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 358, in forward_encoder
[rank30]:     x = self.aggregate_variables(x)
[rank30]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 302, in aggregate_variables
[rank30]:     x = x.flatten(0, 1)  # BxL, V, D
[rank30]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 16.00 GiB. GPU 6 has a total capacity of 63.98 GiB of which 5.21 GiB is free. Of the allocated memory 48.49 GiB is allocated by PyTorch, and 8.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank26]: Traceback (most recent call last):
[rank26]:   File "train.py", line 986, in <module>
[rank26]:     main(args, device, world_size, world_rank, local_rank)
[rank26]:   File "train.py", line 890, in main
[rank26]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank26]:   File "train.py", line 372, in training_step
[rank26]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank26]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank26]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank26]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank26]:     return self._call_impl(*args, **kwargs)
[rank26]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank26]:     return forward_call(*args, **kwargs)
[rank26]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank26]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank26]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 358, in forward_encoder
[rank26]:     x = self.aggregate_variables(x)
[rank26]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 302, in aggregate_variables
[rank26]:     x = x.flatten(0, 1)  # BxL, V, D
[rank23]: Traceback (most recent call last):
[rank23]:   File "train.py", line 986, in <module>
[rank23]:     main(args, device, world_size, world_rank, local_rank)
[rank23]:   File "train.py", line 890, in main
[rank23]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank23]:   File "train.py", line 372, in training_step
[rank23]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank23]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank23]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank23]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank23]:     return self._call_impl(*args, **kwargs)
[rank26]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 16.00 GiB. GPU 2 has a total capacity of 63.98 GiB of which 4.90 GiB is free. Of the allocated memory 48.49 GiB is allocated by PyTorch, and 9.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank23]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank23]:     return forward_call(*args, **kwargs)
[rank23]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank23]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank23]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 358, in forward_encoder
[rank23]:     x = self.aggregate_variables(x)
[rank23]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 302, in aggregate_variables
[rank23]:     x = x.flatten(0, 1)  # BxL, V, D
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
3 torch.Size([2, 2048, 8192])
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
[rank23]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 16.00 GiB. GPU 7 has a total capacity of 63.98 GiB of which 5.40 GiB is free. Of the allocated memory 48.49 GiB is allocated by PyTorch, and 8.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank18]: Traceback (most recent call last):
[rank18]:   File "train.py", line 986, in <module>
[rank18]:     main(args, device, world_size, world_rank, local_rank)
[rank18]:   File "train.py", line 890, in main
[rank18]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank18]:   File "train.py", line 372, in training_step
[rank18]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank18]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank18]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank18]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank18]:     return self._call_impl(*args, **kwargs)
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank18]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank18]:     return forward_call(*args, **kwargs)
[rank18]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank18]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank18]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 358, in forward_encoder
[rank18]:     x = self.aggregate_variables(x)
[rank18]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 302, in aggregate_variables
[rank18]:     x = x.flatten(0, 1)  # BxL, V, D
[rank18]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 16.00 GiB. GPU 2 has a total capacity of 63.98 GiB of which 5.40 GiB is free. Of the allocated memory 48.49 GiB is allocated by PyTorch, and 8.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank27]: Traceback (most recent call last):
[rank27]:   File "train.py", line 986, in <module>
[rank27]:     main(args, device, world_size, world_rank, local_rank)
[rank27]:   File "train.py", line 890, in main
[rank27]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank27]:   File "train.py", line 372, in training_step
[rank27]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank27]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank27]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank27]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank27]:     return self._call_impl(*args, **kwargs)
[rank27]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank27]:     return forward_call(*args, **kwargs)
[rank27]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank27]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank27]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 358, in forward_encoder
[rank27]:     x = self.aggregate_variables(x)
[rank27]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 302, in aggregate_variables
[rank27]:     x = x.flatten(0, 1)  # BxL, V, D
[rank27]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 16.00 GiB. GPU 3 has a total capacity of 63.98 GiB of which 5.21 GiB is free. Of the allocated memory 48.49 GiB is allocated by PyTorch, and 8.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank25]: Traceback (most recent call last):
[rank25]:   File "train.py", line 986, in <module>
[rank25]:     main(args, device, world_size, world_rank, local_rank)
[rank25]:   File "train.py", line 890, in main
[rank25]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank25]:   File "train.py", line 372, in training_step
[rank25]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank25]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank25]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank25]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank25]:     return self._call_impl(*args, **kwargs)
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
3 torch.Size([2, 2048, 8192])
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
[rank25]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank25]:     return forward_call(*args, **kwargs)
[rank25]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank25]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank25]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 358, in forward_encoder
[rank25]:     x = self.aggregate_variables(x)
[rank25]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 302, in aggregate_variables
[rank25]:     x = x.flatten(0, 1)  # BxL, V, D
[rank25]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 16.00 GiB. GPU 1 has a total capacity of 63.98 GiB of which 5.21 GiB is free. Of the allocated memory 48.49 GiB is allocated by PyTorch, and 8.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank31]: Traceback (most recent call last):
[rank31]:   File "train.py", line 986, in <module>
[rank31]:     main(args, device, world_size, world_rank, local_rank)
[rank31]:   File "train.py", line 890, in main
[rank31]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank31]:   File "train.py", line 372, in training_step
[rank31]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank31]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank31]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank31]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank31]:     return self._call_impl(*args, **kwargs)
[rank31]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank31]:     return forward_call(*args, **kwargs)
[rank31]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank31]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank31]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 358, in forward_encoder
[rank31]:     x = self.aggregate_variables(x)
[rank31]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 302, in aggregate_variables
[rank31]:     x = x.flatten(0, 1)  # BxL, V, D
[rank31]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 16.00 GiB. GPU 7 has a total capacity of 63.98 GiB of which 5.40 GiB is free. Of the allocated memory 48.49 GiB is allocated by PyTorch, and 8.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
global rank 9: ddp rank 0 iter_start,iter_end = 0 8
global rank 9: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
[rank29]: Traceback (most recent call last):
[rank29]:   File "train.py", line 986, in <module>
[rank29]:     main(args, device, world_size, world_rank, local_rank)
[rank29]:   File "train.py", line 890, in main
[rank29]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank29]:   File "train.py", line 372, in training_step
[rank29]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank29]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank29]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank29]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank29]:     return self._call_impl(*args, **kwargs)
[rank29]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank29]:     return forward_call(*args, **kwargs)
[rank29]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank29]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank29]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 358, in forward_encoder
[rank29]:     x = self.aggregate_variables(x)
[rank29]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 302, in aggregate_variables
[rank29]:     x = x.flatten(0, 1)  # BxL, V, D
[rank29]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 16.00 GiB. GPU 5 has a total capacity of 63.98 GiB of which 5.40 GiB is free. Of the allocated memory 48.49 GiB is allocated by PyTorch, and 8.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
global rank 13: ddp rank 0 iter_start,iter_end = 0 8
global rank 13: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 15: ddp rank 0 iter_start,iter_end = 0 8
global rank 15: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 11: ddp rank 0 iter_start,iter_end = 0 8
global rank 11: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 12: ddp rank 0 iter_start,iter_end = 0 8
global rank 12: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
3 torch.Size([2, 2048, 8192])
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
3 torch.Size([2, 2048, 8192])
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
[rank14]: Traceback (most recent call last):
[rank14]:   File "train.py", line 986, in <module>
[rank14]:     main(args, device, world_size, world_rank, local_rank)
[rank14]:   File "train.py", line 890, in main
[rank14]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank14]:   File "train.py", line 372, in training_step
[rank14]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank14]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank14]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank14]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank14]:     return self._call_impl(*args, **kwargs)
[rank14]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank14]:     return forward_call(*args, **kwargs)
[rank14]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank14]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank14]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 358, in forward_encoder
[rank14]:     x = self.aggregate_variables(x)
[rank14]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 302, in aggregate_variables
[rank14]:     x = x.flatten(0, 1)  # BxL, V, D
[rank14]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 16.00 GiB. GPU 6 has a total capacity of 63.98 GiB of which 4.71 GiB is free. Of the allocated memory 48.49 GiB is allocated by PyTorch, and 9.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank10]: Traceback (most recent call last):
[rank10]:   File "train.py", line 986, in <module>
[rank10]:     main(args, device, world_size, world_rank, local_rank)
[rank10]:   File "train.py", line 890, in main
[rank10]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank10]:   File "train.py", line 372, in training_step
[rank10]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank10]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank10]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank10]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank10]:     return self._call_impl(*args, **kwargs)
[rank10]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank10]:     return forward_call(*args, **kwargs)
[rank10]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank10]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank10]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 358, in forward_encoder
[rank10]:     x = self.aggregate_variables(x)
[rank10]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 302, in aggregate_variables
[rank10]:     x = x.flatten(0, 1)  # BxL, V, D
[rank10]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 16.00 GiB. GPU 2 has a total capacity of 63.98 GiB of which 5.40 GiB is free. Of the allocated memory 48.49 GiB is allocated by PyTorch, and 8.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
3 torch.Size([2, 2048, 8192])
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
3 torch.Size([2, 2048, 8192])
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
3 torch.Size([2, 2048, 8192])
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
3 torch.Size([2, 2048, 8192])
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank9]: Traceback (most recent call last):
[rank9]:   File "train.py", line 986, in <module>
[rank9]:     main(args, device, world_size, world_rank, local_rank)
[rank9]:   File "train.py", line 890, in main
[rank9]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank9]:   File "train.py", line 372, in training_step
[rank9]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank9]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank9]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank9]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank9]:     return self._call_impl(*args, **kwargs)
[rank9]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank9]:     return forward_call(*args, **kwargs)
[rank9]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank9]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank9]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 358, in forward_encoder
[rank9]:     x = self.aggregate_variables(x)
[rank9]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 302, in aggregate_variables
[rank9]:     x = x.flatten(0, 1)  # BxL, V, D
[rank9]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 16.00 GiB. GPU 1 has a total capacity of 63.98 GiB of which 5.21 GiB is free. Of the allocated memory 48.49 GiB is allocated by PyTorch, and 8.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank13]: Traceback (most recent call last):
[rank13]:   File "train.py", line 986, in <module>
[rank13]:     main(args, device, world_size, world_rank, local_rank)
[rank13]:   File "train.py", line 890, in main
[rank13]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank13]:   File "train.py", line 372, in training_step
[rank13]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank13]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank13]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank13]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank13]:     return self._call_impl(*args, **kwargs)
[rank13]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank13]:     return forward_call(*args, **kwargs)
[rank13]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank13]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank13]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 358, in forward_encoder
[rank13]:     x = self.aggregate_variables(x)
[rank13]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 302, in aggregate_variables
[rank13]:     x = x.flatten(0, 1)  # BxL, V, D
[rank13]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 16.00 GiB. GPU 5 has a total capacity of 63.98 GiB of which 5.40 GiB is free. Of the allocated memory 48.49 GiB is allocated by PyTorch, and 8.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank15]: Traceback (most recent call last):
[rank15]:   File "train.py", line 986, in <module>
[rank15]:     main(args, device, world_size, world_rank, local_rank)
[rank15]:   File "train.py", line 890, in main
[rank15]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank15]:   File "train.py", line 372, in training_step
[rank15]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank15]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank15]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank15]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank15]:     return self._call_impl(*args, **kwargs)
[rank15]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank15]:     return forward_call(*args, **kwargs)
[rank15]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank15]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank15]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 358, in forward_encoder
[rank15]:     x = self.aggregate_variables(x)
[rank15]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 302, in aggregate_variables
[rank15]:     x = x.flatten(0, 1)  # BxL, V, D
[rank15]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 16.00 GiB. GPU 7 has a total capacity of 63.98 GiB of which 4.90 GiB is free. Of the allocated memory 48.49 GiB is allocated by PyTorch, and 9.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
3 torch.Size([2, 2048, 8192])
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank8]: Traceback (most recent call last):
[rank8]:   File "train.py", line 986, in <module>
[rank8]:     main(args, device, world_size, world_rank, local_rank)
[rank8]:   File "train.py", line 890, in main
[rank8]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank8]:   File "train.py", line 372, in training_step
[rank8]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank8]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank8]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank8]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank8]:     return self._call_impl(*args, **kwargs)
[rank8]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank8]:     return forward_call(*args, **kwargs)
[rank8]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank8]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank8]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 358, in forward_encoder
[rank8]:     x = self.aggregate_variables(x)
[rank8]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 302, in aggregate_variables
[rank8]:     x = x.flatten(0, 1)  # BxL, V, D
[rank8]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 63.98 GiB of which 4.90 GiB is free. Of the allocated memory 48.49 GiB is allocated by PyTorch, and 9.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank11]: Traceback (most recent call last):
[rank11]:   File "train.py", line 986, in <module>
[rank11]:     main(args, device, world_size, world_rank, local_rank)
[rank11]:   File "train.py", line 890, in main
[rank11]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank11]:   File "train.py", line 372, in training_step
[rank11]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank11]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank11]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank11]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank11]:     return self._call_impl(*args, **kwargs)
[rank11]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank11]:     return forward_call(*args, **kwargs)
[rank11]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank11]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank11]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 358, in forward_encoder
[rank11]:     x = self.aggregate_variables(x)
[rank11]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 302, in aggregate_variables
[rank11]:     x = x.flatten(0, 1)  # BxL, V, D
[rank11]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 16.00 GiB. GPU 3 has a total capacity of 63.98 GiB of which 5.21 GiB is free. Of the allocated memory 48.49 GiB is allocated by PyTorch, and 8.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
3 torch.Size([2, 2048, 8192])
torch.Size([2, 8, 2048, 8192])
2 torch.Size([2, 256, 2048, 8192])
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/backends/cuda/__init__.py:320: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::allreduce_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/graph.py:744: UserWarning: c10d::broadcast_: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:63.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank12]: Traceback (most recent call last):
[rank12]:   File "train.py", line 986, in <module>
[rank12]:     main(args, device, world_size, world_rank, local_rank)
[rank12]:   File "train.py", line 890, in main
[rank12]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank12]:   File "train.py", line 372, in training_step
[rank12]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank12]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank12]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank12]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank12]:     return self._call_impl(*args, **kwargs)
[rank12]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank12]:     return forward_call(*args, **kwargs)
[rank12]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank12]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank12]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 358, in forward_encoder
[rank12]:     x = self.aggregate_variables(x)
[rank12]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 302, in aggregate_variables
[rank12]:     x = x.flatten(0, 1)  # BxL, V, D
[rank12]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 16.00 GiB. GPU 4 has a total capacity of 63.98 GiB of which 4.71 GiB is free. Of the allocated memory 48.49 GiB is allocated by PyTorch, and 9.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:12:13,558] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:12:13,558] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:12:13,558] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:12:13,558] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:12:13,558] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:12:13,558] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:12:13,558] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:12:13,558] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:12:13,565] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:12:13,565] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:12:13,565] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:12:13,565] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:12:13,565] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:12:13,565] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:12:13,565] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:12:13,565] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:12:13,566] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:12:13,566] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:12:13,566] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:12:13,566] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:12:13,566] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:12:13,566] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:12:13,566] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:12:13,566] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:12:13,567] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:12:13,567] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:12:13,567] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:12:13,567] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:12:13,567] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:12:13,567] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:12:13,567] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:12:13,567] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
HERE0 Namespace(arch='orbit_hier_token_noagg_self', batch_size=2, channels=512, depth=32, embed_dim=8192, fa2=True, fsdp_size=1, imagex=128, imagey=256, max_epochs=1, num_heads=32, real=False, seq_par_size=1, simple_ddp_size=1, tensor_par_size=32, yaml_config='configs/ERA5-100million-91variables.yaml')
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
rank 5 Before initialize parallelism groups
rank 2 Before initialize parallelism groups
rank 10 Before initialize parallelism groups
rank 9 Before initialize parallelism groups
Using dist.init_process_group. world_size  32
rank 24 Before initialize parallelism groups
rank 26 Before initialize parallelism groups
rank 31 Before initialize parallelism groups
rank 16 Before initialize parallelism groups
rank 19 Before initialize parallelism groups
rank 21 Before initialize parallelism groups
Using dist.init_process_group. world_size  32
rank 6 Before initialize parallelism groups
rank 7 Before initialize parallelism groups
rank 1 Before initialize parallelism groups
rank 15 Before initialize parallelism groups
rank 25 Before initialize parallelism groups
rank 27 Before initialize parallelism groups
Using dist.init_process_group. world_size  32
rank 28 Before initialize parallelism groups
Using dist.init_process_group. world_size  32
rank 2 After initialize parallelism groups
rank 29 Before initialize parallelism groups
rank 5 After initialize parallelism groups
rank 9 After initialize parallelism groups
rank 10 After initialize parallelism groups
rank 31 After initialize parallelism groups
rank 24 After initialize parallelism groups
rank 30 Before initialize parallelism groups
rank 26 After initialize parallelism groups
rank 19 After initialize parallelism groups
rank 16 After initialize parallelism groups
rank 21 After initialize parallelism groups
rank 3 Before initialize parallelism groups
rank 6 After initialize parallelism groups
rank 1 After initialize parallelism groups
rank 7 After initialize parallelism groups
rank 22 Before initialize parallelism groups
rank 17 Before initialize parallelism groups
rank 13 Before initialize parallelism groups
rank 15 After initialize parallelism groups
rank 25 After initialize parallelism groups
rank 27 After initialize parallelism groups
rank 11 Before initialize parallelism groups
rank 20 Before initialize parallelism groups
Using dist.init_process_group. world_size  32
config_path  configs/ERA5-100million-91variables.yaml
rank 28 After initialize parallelism groups
rank 29 After initialize parallelism groups
rank 30 After initialize parallelism groups
rank 4 Before initialize parallelism groups
rank 23 Before initialize parallelism groups
rank 3 After initialize parallelism groups
rank 18 Before initialize parallelism groups
rank 22 After initialize parallelism groups
rank 17 After initialize parallelism groups
rank 13 After initialize parallelism groups
rank 12 Before initialize parallelism groups
rank 11 After initialize parallelism groups
rank 20 After initialize parallelism groups
rank 14 Before initialize parallelism groups
rank 4 After initialize parallelism groups
rank 23 After initialize parallelism groups
rank 8 Before initialize parallelism groups
rank 18 After initialize parallelism groups
rank 12 After initialize parallelism groups
{'seed_everything': 42, 'trainer': {'checkpoint_path': '/lustre/orion/stf218/world-shared/atsaris/tmp/tmp_checkpoints', 'checkpoint_filename': 'multi_last', 'resume_from_checkpoint': False}, 'parallelism': {'cpu_offloading': False}, 'model': {'lr': 2e-05, 'beta_1': 0.9, 'beta_2': 0.95, 'weight_decay': '1e-5', 'warmup_steps': 1000, 'max_steps': 20000, 'warmup_start_lr': '1e-8', 'eta_min': '1e-8', 'net': {'class_path': 'climax.arch.ClimaX', 'init_args': {'default_vars': ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_70', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000'], 'patch_size': 4, 'decoder_depth': 2, 'mlp_ratio': 4, 'drop_path': 0.1, 'drop_rate': 0.1, 'aggregated_variables': 1}}}, 'data': {'dict_root_dirs': {'mpi-esm': '/lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0'}, 'dict_start_idx': {'mpi-esm': 0}, 'dict_end_idx': {'mpi-esm': 1}, 'dict_in_variables': {'mpi-esm': ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000']}, 'dict_out_variables': {'mpi-esm': ['geopotential_500', 'temperature_850', '2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind']}, 'dict_max_predict_ranges': {'mpi-esm': 24}, 'dict_random_lead_time': {'mpi-esm': False}, 'dict_hrs_each_step': {'mpi-esm': 1}, 'dict_buffer_sizes': {'mpi-esm': 1}, 'num_workers': 1, 'pin_memory': False}}
rank 14 After initialize parallelism groups
max_epochs 1 data_par_size 1 fsdp_size 1 simple_ddp_size 1 tensor_par_size 32 seq_par_size 1 cpu_offloading False
lr  2e-05 beta_1  0.9 beta_2 0.95 weight_decay 1e-05 class_path climax.arch.ClimaX default_vars ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_70', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000']
img_size_x 128 img_size_y 256 patch_size 4 emb_dim 8192 depth 32 decoder_depth 2 num_heads 32 mlp_ratio 4 drop_path 0.1 drop_rate 0.1 aggregated_variables 1
dict_root_dirs {'mpi-esm': '/lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0'} dict_start_idx {'mpi-esm': 0} dict_end_idx {'mpi-esm': 1} batch_size 2 num_workers 1
warmup_steps 1000 max_steps 20000 warmup_start_lr 1e-08 eta_min 1e-08
checkpoint_path /lustre/orion/stf218/world-shared/atsaris/tmp/tmp_checkpoints checkpoint_filename multi_last resume_from_checkpoint False
rank 0 Before initialize parallelism groups
rank 8 After initialize parallelism groups
rank 0 After initialize parallelism groups
rank 28 After initialize model
rank 28 Before the second dist.barrier()
rank 6 After initialize model
rank 6 Before the second dist.barrier()
rank 19 After initialize model
rank 19 Before the second dist.barrier()
rank 27 After initialize model
rank 27 Before the second dist.barrier()
rank 4 After initialize model
rank 4 Before the second dist.barrier()
rank 12 After initialize model
rank 12 Before the second dist.barrier()
rank 5 After initialize model
rank 5 Before the second dist.barrier()
rank 16 After initialize model
rank 16 Before the second dist.barrier()
rank 8 After initialize model
rank 8 Before the second dist.barrier()
rank 21 After initialize model
rank 21 Before the second dist.barrier()
rank 25 After initialize model
rank 25 Before the second dist.barrier()
rank 15 After initialize model
rank 15 Before the second dist.barrier()
rank 9 After initialize model
rank 9 Before the second dist.barrier()
rank 7 After initialize model
rank 0 After initialize model
rank 7 Before the second dist.barrier()
rank 0 Before the second dist.barrier()
rank 18 After initialize model
rank 18 Before the second dist.barrier()
rank 26 After initialize model
rank 26 Before the second dist.barrier()
rank 3 After initialize model
rank 3 Before the second dist.barrier()
rank 2 After initialize model
rank 2 Before the second dist.barrier()
rank 22 After initialize model
rank 22 Before the second dist.barrier()
rank 13 After initialize model
rank 13 Before the second dist.barrier()
rank 20 After initialize model
rank 20 Before the second dist.barrier()
rank 23 After initialize model
rank 23 Before the second dist.barrier()
rank 17 After initialize model
rank 17 Before the second dist.barrier()
rank 14 After initialize model
rank 14 Before the second dist.barrier()
rank 1 After initialize model
rank 1 Before the second dist.barrier()
rank 30 After initialize model
rank 30 Before the second dist.barrier()
rank 31 After initialize model
rank 31 Before the second dist.barrier()
rank 10 After initialize model
rank 10 Before the second dist.barrier()
rank 11 After initialize model
rank 11 Before the second dist.barrier()
rank 29 After initialize model
rank 29 Before the second dist.barrier()
rank 24 After initialize model
rank 24 Before the second dist.barrier()
rank 0 After the second dist.barrier()
rank 16 After the second dist.barrier()
rank 17 After the second dist.barrier()
rank 1 After the second dist.barrier()
rank 20 After the second dist.barrier()
rank 13 After the second dist.barrier()
rank 24 After the second dist.barrier()
parameter name  var_embed  requires_gradient  True size torch.Size([1, 512, 8192])
rank 21 After the second dist.barrier()
rank 14 After the second dist.barrier()
rank 25 After the second dist.barrier()
rank 4 After the second dist.barrier()
rank 22 After the second dist.barrier()
rank 15 After the second dist.barrier()
rank 29 After the second dist.barrier()
rank 5 After the second dist.barrier()
rank 23 After the second dist.barrier()
rank 12 After the second dist.barrier()
rank 28 After the second dist.barrier()
parameter name  var_query  requires_gradient  True size torch.Size([1, 1, 8192])
rank 19 After the second dist.barrier()
rank 8 After the second dist.barrier()
rank 30 After the second dist.barrier()
parameter name  pos_embed  requires_gradient  True size torch.Size([1, 2048, 8192])
rank 18 After the second dist.barrier()
rank 9 After the second dist.barrier()
rank 31 After the second dist.barrier()
rank 6 After the second dist.barrier()
rank 10 After the second dist.barrier()
rank 27 After the second dist.barrier()
rank 7 After the second dist.barrier()
rank 11 After the second dist.barrier()
rank 26 After the second dist.barrier()
parameter name  token_embeds.0.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.0.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.1.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.1.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.2.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.2.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.3.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.3.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.4.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.4.proj.bias  requires_gradient  True size torch.Size([8192])
rank 2 After the second dist.barrier()
rank 3 After the second dist.barrier()
parameter name  token_embeds.5.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.5.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.6.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.6.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.7.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.7.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.8.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.8.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.9.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.9.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.10.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.10.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.11.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.11.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.12.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.12.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.13.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.13.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.14.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.14.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.15.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.15.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.16.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.16.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.17.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.17.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.18.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.18.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.19.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.19.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.20.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.20.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.21.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.21.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.22.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.22.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.23.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.23.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.24.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.24.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.25.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.25.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.26.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.26.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.27.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.27.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.28.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.28.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.29.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.29.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.30.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.30.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.31.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.31.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.32.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.32.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.33.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.33.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.34.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.34.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.35.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.35.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.36.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.36.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.37.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.37.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.38.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.38.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.39.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.39.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.40.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.40.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.41.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.41.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.42.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.42.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.43.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.43.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.44.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.44.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.45.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.45.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.46.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.46.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.47.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.47.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.48.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.48.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.49.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.49.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.50.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.50.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.51.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.51.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.52.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.52.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.53.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.53.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.54.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.54.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.55.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.55.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.56.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.56.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.57.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.57.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.58.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.58.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.59.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.59.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.60.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.60.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.61.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.61.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.62.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.62.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.63.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.63.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.64.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.64.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.65.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.65.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.66.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.66.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.67.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.67.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.68.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.68.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.69.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.69.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.70.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.70.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.71.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.71.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.72.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.72.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.73.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.73.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.74.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.74.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.75.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.75.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.76.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.76.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.77.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.77.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.78.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.78.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.79.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.79.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.80.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.80.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.81.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.81.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.82.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.82.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.83.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.83.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.84.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.84.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.85.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.85.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.86.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.86.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.87.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.87.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.88.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.88.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.89.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.89.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.90.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.90.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.91.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.91.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.92.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.92.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.93.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.93.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.94.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.94.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.95.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.95.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.96.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.96.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.97.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.97.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.98.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.98.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.99.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.99.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.100.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.100.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.101.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.101.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.102.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.102.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.103.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.103.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.104.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.104.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.105.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.105.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.106.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.106.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.107.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.107.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.108.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.108.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.109.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.109.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.110.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.110.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.111.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.111.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.112.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.112.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.113.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.113.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.114.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.114.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.115.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.115.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.116.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.116.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.117.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.117.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.118.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.118.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.119.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.119.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.120.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.120.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.121.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.121.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.122.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.122.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.123.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.123.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.124.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.124.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.125.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.125.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.126.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.126.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.127.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.127.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.128.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.128.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.129.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.129.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.130.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.130.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.131.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.131.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.132.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.132.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.133.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.133.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.134.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.134.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.135.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.135.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.136.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.136.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.137.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.137.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.138.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.138.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.139.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.139.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.140.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.140.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.141.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.141.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.142.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.142.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.143.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.143.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.144.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.144.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.145.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.145.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.146.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.146.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.147.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.147.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.148.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.148.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.149.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.149.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.150.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.150.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.151.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.151.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.152.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.152.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.153.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.153.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.154.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.154.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.155.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.155.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.156.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.156.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.157.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.157.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.158.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.158.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.159.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.159.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.160.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.160.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.161.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.161.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.162.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.162.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.163.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.163.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.164.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.164.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.165.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.165.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.166.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.166.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.167.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.167.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.168.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.168.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.169.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.169.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.170.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.170.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.171.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.171.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.172.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.172.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.173.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.173.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.174.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.174.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.175.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.175.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.176.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.176.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.177.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.177.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.178.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.178.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.179.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.179.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.180.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.180.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.181.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.181.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.182.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.182.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.183.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.183.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.184.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.184.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.185.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.185.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.186.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.186.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.187.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.187.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.188.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.188.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.189.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.189.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.190.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.190.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.191.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.191.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.192.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.192.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.193.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.193.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.194.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.194.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.195.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.195.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.196.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.196.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.197.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.197.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.198.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.198.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.199.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.199.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.200.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.200.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.201.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.201.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.202.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.202.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.203.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.203.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.204.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.204.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.205.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.205.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.206.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.206.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.207.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.207.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.208.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.208.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.209.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.209.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.210.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.210.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.211.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.211.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.212.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.212.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.213.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.213.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.214.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.214.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.215.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.215.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.216.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.216.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.217.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.217.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.218.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.218.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.219.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.219.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.220.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.220.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.221.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.221.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.222.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.222.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.223.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.223.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.224.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.224.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.225.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.225.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.226.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.226.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.227.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.227.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.228.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.228.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.229.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.229.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.230.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.230.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.231.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.231.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.232.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.232.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.233.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.233.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.234.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.234.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.235.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.235.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.236.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.236.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.237.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.237.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.238.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.238.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.239.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.239.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.240.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.240.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.241.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.241.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.242.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.242.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.243.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.243.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.244.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.244.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.245.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.245.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.246.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.246.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.247.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.247.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.248.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.248.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.249.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.249.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.250.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.250.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.251.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.251.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.252.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.252.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.253.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.253.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.254.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.254.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.255.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.255.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.256.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.256.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.257.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.257.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.258.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.258.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.259.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.259.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.260.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.260.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.261.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.261.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.262.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.262.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.263.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.263.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.264.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.264.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.265.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.265.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.266.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.266.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.267.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.267.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.268.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.268.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.269.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.269.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.270.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.270.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.271.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.271.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.272.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.272.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.273.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.273.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.274.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.274.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.275.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.275.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.276.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.276.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.277.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.277.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.278.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.278.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.279.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.279.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.280.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.280.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.281.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.281.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.282.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.282.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.283.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.283.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.284.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.284.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.285.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.285.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.286.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.286.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.287.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.287.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.288.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.288.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.289.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.289.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.290.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.290.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.291.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.291.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.292.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.292.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.293.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.293.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.294.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.294.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.295.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.295.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.296.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.296.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.297.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.297.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.298.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.298.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.299.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.299.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.300.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.300.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.301.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.301.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.302.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.302.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.303.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.303.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.304.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.304.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.305.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.305.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.306.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.306.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.307.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.307.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.308.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.308.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.309.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.309.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.310.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.310.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.311.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.311.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.312.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.312.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.313.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.313.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.314.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.314.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.315.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.315.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.316.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.316.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.317.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.317.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.318.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.318.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.319.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.319.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.320.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.320.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.321.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.321.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.322.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.322.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.323.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.323.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.324.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.324.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.325.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.325.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.326.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.326.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.327.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.327.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.328.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.328.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.329.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.329.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.330.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.330.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.331.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.331.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.332.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.332.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.333.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.333.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.334.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.334.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.335.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.335.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.336.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.336.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.337.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.337.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.338.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.338.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.339.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.339.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.340.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.340.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.341.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.341.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.342.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.342.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.343.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.343.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.344.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.344.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.345.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.345.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.346.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.346.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.347.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.347.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.348.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.348.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.349.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.349.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.350.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.350.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.351.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.351.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.352.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.352.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.353.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.353.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.354.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.354.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.355.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.355.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.356.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.356.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.357.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.357.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.358.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.358.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.359.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.359.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.360.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.360.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.361.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.361.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.362.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.362.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.363.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.363.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.364.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.364.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.365.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.365.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.366.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.366.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.367.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.367.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.368.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.368.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.369.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.369.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.370.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.370.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.371.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.371.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.372.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.372.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.373.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.373.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.374.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.374.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.375.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.375.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.376.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.376.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.377.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.377.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.378.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.378.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.379.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.379.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.380.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.380.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.381.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.381.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.382.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.382.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.383.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.383.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.384.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.384.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.385.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.385.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.386.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.386.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.387.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.387.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.388.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.388.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.389.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.389.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.390.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.390.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.391.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.391.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.392.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.392.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.393.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.393.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.394.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.394.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.395.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.395.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.396.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.396.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.397.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.397.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.398.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.398.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.399.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.399.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.400.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.400.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.401.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.401.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.402.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.402.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.403.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.403.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.404.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.404.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.405.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.405.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.406.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.406.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.407.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.407.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.408.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.408.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.409.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.409.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.410.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.410.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.411.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.411.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.412.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.412.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.413.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.413.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.414.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.414.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.415.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.415.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.416.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.416.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.417.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.417.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.418.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.418.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.419.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.419.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.420.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.420.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.421.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.421.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.422.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.422.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.423.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.423.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.424.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.424.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.425.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.425.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.426.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.426.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.427.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.427.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.428.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.428.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.429.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.429.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.430.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.430.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.431.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.431.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.432.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.432.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.433.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.433.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.434.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.434.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.435.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.435.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.436.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.436.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.437.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.437.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.438.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.438.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.439.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.439.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.440.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.440.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.441.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.441.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.442.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.442.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.443.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.443.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.444.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.444.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.445.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.445.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.446.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.446.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.447.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.447.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.448.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.448.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.449.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.449.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.450.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.450.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.451.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.451.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.452.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.452.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.453.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.453.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.454.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.454.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.455.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.455.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.456.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.456.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.457.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.457.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.458.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.458.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.459.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.459.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.460.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.460.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.461.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.461.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.462.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.462.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.463.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.463.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.464.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.464.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.465.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.465.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.466.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.466.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.467.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.467.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.468.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.468.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.469.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.469.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.470.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.470.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.471.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.471.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.472.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.472.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.473.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.473.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.474.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.474.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.475.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.475.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.476.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.476.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.477.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.477.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.478.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.478.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.479.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.479.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.480.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.480.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.481.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.481.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.482.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.482.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.483.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.483.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.484.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.484.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.485.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.485.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.486.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.486.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.487.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.487.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.488.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.488.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.489.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.489.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.490.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.490.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.491.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.491.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.492.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.492.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.493.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.493.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.494.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.494.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.495.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.495.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.496.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.496.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.497.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.497.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.498.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.498.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.499.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.499.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.500.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.500.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.501.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.501.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.502.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.502.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.503.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.503.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.504.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.504.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.505.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.505.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.506.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.506.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.507.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.507.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.508.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.508.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.509.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.509.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.510.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.510.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.511.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.511.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  var_agg.q.weight  requires_gradient  True size torch.Size([256, 8192])
parameter name  var_agg.kv.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  var_agg.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  var_agg.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  lead_time_embed.weight  requires_gradient  True size torch.Size([8192, 1])
parameter name  lead_time_embed.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.0.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.0.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.0.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.0.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.0.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.0.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.1.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.1.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.1.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.1.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.1.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.1.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.2.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.2.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.2.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.2.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.2.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.2.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.3.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.3.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.3.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.3.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.3.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.3.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.4.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.4.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.4.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.4.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.4.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.4.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.5.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.5.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.5.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.5.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.5.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.5.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.6.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.6.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.6.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.6.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.6.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.6.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.7.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.7.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.7.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.7.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.7.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.7.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.8.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.8.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.8.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.8.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.8.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.8.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.9.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.9.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.9.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.9.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.9.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.9.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.10.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.10.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.10.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.10.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.10.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.10.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.11.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.11.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.11.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.11.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.11.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.11.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.12.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.12.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.12.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.12.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.12.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.12.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.13.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.13.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.13.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.13.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.13.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.13.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.14.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.14.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.14.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.14.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.14.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.14.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.15.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.15.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.15.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.15.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.15.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.15.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.16.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.16.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.16.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.16.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.16.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.16.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.17.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.17.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.17.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.17.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.17.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.17.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.18.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.18.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.18.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.18.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.18.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.18.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.19.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.19.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.19.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.19.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.19.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.19.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.20.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.20.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.20.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.20.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.20.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.20.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.21.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.21.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.21.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.21.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.21.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.21.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.22.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.22.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.22.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.22.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.22.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.22.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.23.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.23.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.23.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.23.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.23.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.23.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.24.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.24.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.24.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.24.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.24.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.24.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.25.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.25.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.25.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.25.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.25.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.25.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.26.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.26.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.26.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.26.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.26.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.26.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.27.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.27.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.27.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.27.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.27.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.27.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.28.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.28.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.28.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.28.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.28.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.28.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.29.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.29.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.29.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.29.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.29.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.29.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.30.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.30.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.30.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.30.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.30.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.30.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.31.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.31.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.31.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.31.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.31.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.31.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  norm.weight  requires_gradient  True size torch.Size([8192])
parameter name  norm.bias  requires_gradient  True size torch.Size([8192])
parameter name  head.0.weight  requires_gradient  True size torch.Size([8192, 8192])
parameter name  head.0.bias  requires_gradient  True size torch.Size([8192])
parameter name  head.2.weight  requires_gradient  True size torch.Size([8192, 8192])
parameter name  head.2.bias  requires_gradient  True size torch.Size([8192])
parameter name  head.5.weight  requires_gradient  True size torch.Size([8192, 8192])
parameter name  head.5.bias  requires_gradient  True size torch.Size([8192])
total_params before FSDP tensor(26351828992) params_per_gpu tensor(1109000192)
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
rank  31 lister_train reset: mpi-esm 9 9 9
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
total_params after FSDP FullyShardedDataParallel(
  (_fsdp_wrapped_module): ClimaX(
    (token_embeds): ModuleList(
      (0-511): 512 x PatchEmbed(
        (proj): Conv2d(1, 8192, kernel_size=(4, 4), stride=(4, 4))
        (norm): Identity()
      )
    )
    (var_agg): FullyShardedDataParallel(
      (_fsdp_wrapped_module): CheckpointWrapper(
        (_checkpoint_wrapped_module): VariableMapping_Attention(
          (q): Linear(in_features=8192, out_features=256, bias=False)
          (kv): Linear(in_features=8192, out_features=512, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=8192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (lead_time_embed): Linear(in_features=1, out_features=8192, bias=True)
    (pos_drop): Dropout(p=0.1, inplace=False)
    (blocks): ModuleList(
      (0-31): 32 x FullyShardedDataParallel(
        (_fsdp_wrapped_module): CheckpointWrapper(
          (_checkpoint_wrapped_module): Block(
            (norm1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=8192, out_features=768, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=256, out_features=8192, bias=True)
              (proj_drop): Dropout(p=0.1, inplace=False)
            )
            (ls1): Identity()
            (norm2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=8192, out_features=1024, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.1, inplace=False)
              (fc2): Linear(in_features=1024, out_features=8192, bias=True)
            )
            (ls2): Identity()
          )
        )
      )
    )
    (norm): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
    (head): Sequential(
      (0): Linear(in_features=8192, out_features=8192, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=8192, out_features=8192, bias=True)
      (3): GELU(approximate='none')
      (4): Pred_Rearrange()
      (5): Linear(in_features=8192, out_features=8192, bias=True)
    )
  )
)
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
len(dict_root_dirs) 1
rank  0 lister_train reset: mpi-esm 9 9 9
use_ddstore is : 0
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
memory stats reset, ready to track
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
global rank 3: ddp rank 0 iter_start,iter_end = 0 8
global rank 3: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 5: ddp rank 0 iter_start,iter_end = 0 8
global rank 5: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 21: ddp rank 0 iter_start,iter_end = 0 8
global rank 21: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 23: ddp rank 0 iter_start,iter_end = 0 8
global rank 23: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 7: ddp rank 0 iter_start,iter_end = 0 8
global rank 7: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 8: ddp rank 0 iter_start,iter_end = 0 8
global rank 8: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 2: ddp rank 0 iter_start,iter_end = 0 8
global rank 2: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 18: ddp rank 0 iter_start,iter_end = 0 8
global rank 18: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 16: ddp rank 0 iter_start,iter_end = 0 8
global rank 16: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 0: ddp rank 0 iter_start,iter_end = 0 8
global rank 0: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 1: ddp rank 0 iter_start,iter_end = 0 8
global rank 1: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 12: ddp rank 0 iter_start,iter_end = 0 8
global rank 12: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 17: ddp rank 0 iter_start,iter_end = 0 8
global rank 17: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 29: ddp rank 0 iter_start,iter_end = 0 8
global rank 29: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 30: ddp rank 0 iter_start,iter_end = 0 8
global rank 30: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 20: ddp rank 0 iter_start,iter_end = 0 8
global rank 20: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 26: ddp rank 0 iter_start,iter_end = 0 8
global rank 26: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 9: ddp rank 0 iter_start,iter_end = 0 8
global rank 9: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 13: ddp rank 0 iter_start,iter_end = 0 8
global rank 13: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 19: ddp rank 0 iter_start,iter_end = 0 8
global rank 19: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 22: ddp rank 0 iter_start,iter_end = 0 8
global rank 22: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 15: ddp rank 0 iter_start,iter_end = 0 8
global rank 15: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 4: ddp rank 0 iter_start,iter_end = 0 8
global rank 4: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 10: ddp rank 0 iter_start,iter_end = 0 8
global rank 10: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 11: ddp rank 0 iter_start,iter_end = 0 8
global rank 11: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 24: ddp rank 0 iter_start,iter_end = 0 8
global rank 24: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 27: ddp rank 0 iter_start,iter_end = 0 8
global rank 27: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 25: ddp rank 0 iter_start,iter_end = 0 8
global rank 25: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 31: ddp rank 0 iter_start,iter_end = 0 8
global rank 31: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 28: ddp rank 0 iter_start,iter_end = 0 8
global rank 28: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
torch.Size([2, 16, 2048, 8192])
torch.Size([2, 16, 2048, 8192])
torch.Size([2, 16, 2048, 8192])
torch.Size([2, 16, 2048, 8192])
torch.Size([2, 16, 2048, 8192])
torch.Size([2, 16, 2048, 8192])
torch.Size([2, 16, 2048, 8192])
torch.Size([2, 16, 2048, 8192])
[rank3]: Traceback (most recent call last):
[rank3]:   File "train.py", line 986, in <module>
[rank3]:     main(args, device, world_size, world_rank, local_rank)
[rank3]:   File "train.py", line 890, in main
[rank3]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank3]:   File "train.py", line 372, in training_step
[rank3]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank3]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank3]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank3]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank3]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank3]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank3]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank3]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank3]:     return _AllGather.apply(group, tensor)
[rank3]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank3]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank3]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank3]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank3]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank3]:     return func(*args, **kwargs)
[rank3]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank3]:     work = group.allgather([tensor_list], [tensor])
[rank3]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 32.00 GiB. GPU 3 has a total capacity of 63.98 GiB of which 23.78 GiB is free. Of the allocated memory 38.87 GiB is allocated by PyTorch, and 120.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank7]: Traceback (most recent call last):
[rank7]:   File "train.py", line 986, in <module>
[rank7]:     main(args, device, world_size, world_rank, local_rank)
[rank7]:   File "train.py", line 890, in main
[rank7]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank7]:   File "train.py", line 372, in training_step
[rank7]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank7]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank7]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank7]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank7]:     return forward_call(*args, **kwargs)
[rank7]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank7]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank7]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank7]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank7]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank7]:     return _AllGather.apply(group, tensor)
[rank7]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank7]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank7]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank7]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank7]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank7]:     return func(*args, **kwargs)
[rank7]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank7]:     work = group.allgather([tensor_list], [tensor])
[rank7]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 32.00 GiB. GPU 7 has a total capacity of 63.98 GiB of which 23.96 GiB is free. Of the allocated memory 38.87 GiB is allocated by PyTorch, and 120.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
torch.Size([2, 16, 2048, 8192])
torch.Size([2, 16, 2048, 8192])
[rank9]: Traceback (most recent call last):
[rank9]:   File "train.py", line 986, in <module>
[rank9]:     main(args, device, world_size, world_rank, local_rank)
[rank9]:   File "train.py", line 890, in main
[rank9]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank9]:   File "train.py", line 372, in training_step
[rank9]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank9]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank9]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank9]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank9]:     return self._call_impl(*args, **kwargs)
[rank9]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank9]:     return forward_call(*args, **kwargs)
[rank9]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank9]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank9]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank9]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank9]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank9]:     return _AllGather.apply(group, tensor)
[rank9]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank9]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank9]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank9]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank9]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank9]:     return func(*args, **kwargs)
[rank9]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank9]:     work = group.allgather([tensor_list], [tensor])
[rank9]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 32.00 GiB. GPU 1 has a total capacity of 63.98 GiB of which 23.78 GiB is free. Of the allocated memory 38.87 GiB is allocated by PyTorch, and 120.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank13]: Traceback (most recent call last):
[rank13]:   File "train.py", line 986, in <module>
[rank13]:     main(args, device, world_size, world_rank, local_rank)
[rank13]:   File "train.py", line 890, in main
[rank13]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank13]:   File "train.py", line 372, in training_step
[rank13]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank13]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank13]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank13]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank13]:     return self._call_impl(*args, **kwargs)
[rank13]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank13]:     return forward_call(*args, **kwargs)
[rank13]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank13]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank13]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank13]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank13]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank13]:     return _AllGather.apply(group, tensor)
[rank13]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank13]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank13]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank13]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank13]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank13]:     return func(*args, **kwargs)
[rank13]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank13]:     work = group.allgather([tensor_list], [tensor])
[rank13]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 32.00 GiB. GPU 5 has a total capacity of 63.98 GiB of which 23.96 GiB is free. Of the allocated memory 38.87 GiB is allocated by PyTorch, and 120.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank15]: Traceback (most recent call last):
[rank15]:   File "train.py", line 986, in <module>
[rank15]:     main(args, device, world_size, world_rank, local_rank)
[rank15]:   File "train.py", line 890, in main
[rank15]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank15]:   File "train.py", line 372, in training_step
[rank15]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank15]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank15]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank15]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank15]:     return self._call_impl(*args, **kwargs)
[rank15]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank15]:     return forward_call(*args, **kwargs)
[rank15]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank15]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank15]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank15]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank15]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank15]:     return _AllGather.apply(group, tensor)
[rank15]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank15]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank15]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank15]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank15]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank15]:     return func(*args, **kwargs)
[rank15]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank15]:     work = group.allgather([tensor_list], [tensor])
[rank15]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 32.00 GiB. GPU 7 has a total capacity of 63.98 GiB of which 23.96 GiB is free. Of the allocated memory 38.87 GiB is allocated by PyTorch, and 120.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
torch.Size([2, 16, 2048, 8192])
[rank19]: Traceback (most recent call last):
[rank19]:   File "train.py", line 986, in <module>
[rank19]:     main(args, device, world_size, world_rank, local_rank)
[rank19]:   File "train.py", line 890, in main
[rank19]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank19]:   File "train.py", line 372, in training_step
[rank19]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank19]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank19]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank19]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank19]:     return self._call_impl(*args, **kwargs)
[rank19]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank19]:     return forward_call(*args, **kwargs)
[rank19]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank19]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank19]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank19]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank19]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank19]:     return _AllGather.apply(group, tensor)
[rank19]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank19]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank19]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank19]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank19]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank19]:     return func(*args, **kwargs)
[rank19]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank19]:     work = group.allgather([tensor_list], [tensor])
[rank19]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 32.00 GiB. GPU 3 has a total capacity of 63.98 GiB of which 23.78 GiB is free. Of the allocated memory 38.87 GiB is allocated by PyTorch, and 120.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank22]: Traceback (most recent call last):
[rank22]:   File "train.py", line 986, in <module>
[rank22]:     main(args, device, world_size, world_rank, local_rank)
[rank22]:   File "train.py", line 890, in main
[rank22]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank22]:   File "train.py", line 372, in training_step
[rank22]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank22]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank22]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank22]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank22]:     return self._call_impl(*args, **kwargs)
[rank22]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank22]:     return forward_call(*args, **kwargs)
[rank22]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank22]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank22]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank22]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank22]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank22]:     return _AllGather.apply(group, tensor)
[rank4]: Traceback (most recent call last):
[rank4]:   File "train.py", line 986, in <module>
[rank4]:     main(args, device, world_size, world_rank, local_rank)
[rank4]:   File "train.py", line 890, in main
[rank4]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank4]:   File "train.py", line 372, in training_step
[rank4]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank4]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank4]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank4]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank22]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank22]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank22]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank22]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank22]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank22]:     return func(*args, **kwargs)
[rank22]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank22]:     work = group.allgather([tensor_list], [tensor])
[rank4]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank4]:     return forward_call(*args, **kwargs)
[rank4]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank4]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank4]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank4]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank4]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank4]:     return _AllGather.apply(group, tensor)
[rank22]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 32.00 GiB. GPU 6 has a total capacity of 63.98 GiB of which 23.78 GiB is free. Of the allocated memory 38.87 GiB is allocated by PyTorch, and 120.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank4]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank4]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank4]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank4]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank4]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank4]:     return func(*args, **kwargs)
[rank4]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank4]:     work = group.allgather([tensor_list], [tensor])
[rank21]: Traceback (most recent call last):
[rank21]:   File "train.py", line 986, in <module>
[rank21]:     main(args, device, world_size, world_rank, local_rank)
[rank21]:   File "train.py", line 890, in main
[rank21]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank21]:   File "train.py", line 372, in training_step
[rank21]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank21]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank21]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank21]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank21]:     return self._call_impl(*args, **kwargs)
torch.Size([2, 16, 2048, 8192])
[rank4]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 32.00 GiB. GPU 4 has a total capacity of 63.98 GiB of which 23.78 GiB is free. Of the allocated memory 38.87 GiB is allocated by PyTorch, and 120.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
torch.Size([2, 16, 2048, 8192])
[rank21]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank21]:     return forward_call(*args, **kwargs)
[rank21]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank21]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank21]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank21]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank21]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank21]:     return _AllGather.apply(group, tensor)
[rank5]: Traceback (most recent call last):
[rank5]:   File "train.py", line 986, in <module>
[rank5]:     main(args, device, world_size, world_rank, local_rank)
[rank5]:   File "train.py", line 890, in main
[rank5]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank5]:   File "train.py", line 372, in training_step
[rank5]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank5]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank5]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank5]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank21]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank21]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank21]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank21]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank21]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank21]:     return func(*args, **kwargs)
[rank21]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank21]:     work = group.allgather([tensor_list], [tensor])
[rank5]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank5]:     return forward_call(*args, **kwargs)
[rank5]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank5]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank5]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank5]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank5]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank5]:     return _AllGather.apply(group, tensor)
[rank21]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 32.00 GiB. GPU 5 has a total capacity of 63.98 GiB of which 23.96 GiB is free. Of the allocated memory 38.87 GiB is allocated by PyTorch, and 120.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank5]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank5]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank5]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank5]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank5]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank5]:     return func(*args, **kwargs)
[rank5]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank5]:     work = group.allgather([tensor_list], [tensor])
[rank5]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 32.00 GiB. GPU 5 has a total capacity of 63.98 GiB of which 23.96 GiB is free. Of the allocated memory 38.87 GiB is allocated by PyTorch, and 120.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
torch.Size([2, 16, 2048, 8192])
[rank0]: Traceback (most recent call last):
[rank0]:   File "train.py", line 986, in <module>
[rank0]:     main(args, device, world_size, world_rank, local_rank)
[rank0]:   File "train.py", line 890, in main
[rank0]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank0]:   File "train.py", line 372, in training_step
[rank0]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank0]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank0]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank0]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank0]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank0]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank0]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank0]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank0]:     return _AllGather.apply(group, tensor)
[rank0]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank0]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank0]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank0]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank0]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank0]:     work = group.allgather([tensor_list], [tensor])
[rank0]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 63.98 GiB of which 23.96 GiB is free. Of the allocated memory 38.87 GiB is allocated by PyTorch, and 122.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank16]: Traceback (most recent call last):
[rank16]:   File "train.py", line 986, in <module>
[rank16]:     main(args, device, world_size, world_rank, local_rank)
[rank16]:   File "train.py", line 890, in main
[rank16]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank16]:   File "train.py", line 372, in training_step
[rank16]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank16]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank16]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank16]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank16]:     return self._call_impl(*args, **kwargs)
[rank16]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank16]:     return forward_call(*args, **kwargs)
[rank16]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank16]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank16]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank16]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank16]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank16]:     return _AllGather.apply(group, tensor)
[rank16]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank16]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank16]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank16]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank16]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank16]:     return func(*args, **kwargs)
[rank16]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank16]:     work = group.allgather([tensor_list], [tensor])
[rank16]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 63.98 GiB of which 23.96 GiB is free. Of the allocated memory 38.87 GiB is allocated by PyTorch, and 120.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
torch.Size([2, 16, 2048, 8192])
[rank23]: Traceback (most recent call last):
[rank23]:   File "train.py", line 986, in <module>
[rank23]:     main(args, device, world_size, world_rank, local_rank)
[rank23]:   File "train.py", line 890, in main
[rank23]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank23]:   File "train.py", line 372, in training_step
[rank23]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank23]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank23]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank23]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank23]:     return self._call_impl(*args, **kwargs)
[rank23]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank23]:     return forward_call(*args, **kwargs)
[rank23]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank23]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank23]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank23]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank23]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank23]:     return _AllGather.apply(group, tensor)
[rank23]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank23]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank23]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank23]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank23]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank23]:     return func(*args, **kwargs)
[rank23]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank23]:     work = group.allgather([tensor_list], [tensor])
[rank23]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 32.00 GiB. GPU 7 has a total capacity of 63.98 GiB of which 23.96 GiB is free. Of the allocated memory 38.87 GiB is allocated by PyTorch, and 120.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
torch.Size([2, 16, 2048, 8192])
[rank10]: Traceback (most recent call last):
[rank10]:   File "train.py", line 986, in <module>
[rank10]:     main(args, device, world_size, world_rank, local_rank)
[rank10]:   File "train.py", line 890, in main
[rank10]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank10]:   File "train.py", line 372, in training_step
[rank10]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank10]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank10]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank10]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank10]:     return self._call_impl(*args, **kwargs)
[rank10]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank10]:     return forward_call(*args, **kwargs)
[rank10]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank10]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank10]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank10]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank10]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank10]:     return _AllGather.apply(group, tensor)
torch.Size([2, 16, 2048, 8192])
[rank10]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank10]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank10]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank10]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank10]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank10]:     return func(*args, **kwargs)
[rank10]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank10]:     work = group.allgather([tensor_list], [tensor])
[rank10]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 32.00 GiB. GPU 2 has a total capacity of 63.98 GiB of which 23.96 GiB is free. Of the allocated memory 38.87 GiB is allocated by PyTorch, and 120.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
torch.Size([2, 16, 2048, 8192])
[rank8]: Traceback (most recent call last):
[rank8]:   File "train.py", line 986, in <module>
[rank8]:     main(args, device, world_size, world_rank, local_rank)
[rank8]:   File "train.py", line 890, in main
[rank8]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank8]:   File "train.py", line 372, in training_step
[rank8]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank8]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank8]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank8]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank8]:     return self._call_impl(*args, **kwargs)
[rank8]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank8]:     return forward_call(*args, **kwargs)
[rank8]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank8]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank8]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank8]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank8]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank8]:     return _AllGather.apply(group, tensor)
[rank8]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank8]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank8]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank8]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank8]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank8]:     return func(*args, **kwargs)
[rank8]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank8]:     work = group.allgather([tensor_list], [tensor])
[rank8]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 63.98 GiB of which 23.96 GiB is free. Of the allocated memory 38.87 GiB is allocated by PyTorch, and 120.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank18]: Traceback (most recent call last):
[rank18]:   File "train.py", line 986, in <module>
[rank18]:     main(args, device, world_size, world_rank, local_rank)
[rank18]:   File "train.py", line 890, in main
[rank18]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank18]:   File "train.py", line 372, in training_step
[rank18]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank18]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank18]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank18]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank18]:     return self._call_impl(*args, **kwargs)
[rank18]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank18]:     return forward_call(*args, **kwargs)
[rank18]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank18]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank18]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank18]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank18]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank18]:     return _AllGather.apply(group, tensor)
[rank18]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank18]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank18]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank18]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank18]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank18]:     return func(*args, **kwargs)
[rank18]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank18]:     work = group.allgather([tensor_list], [tensor])
[rank18]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 32.00 GiB. GPU 2 has a total capacity of 63.98 GiB of which 23.96 GiB is free. Of the allocated memory 38.87 GiB is allocated by PyTorch, and 120.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank2]: Traceback (most recent call last):
[rank2]:   File "train.py", line 986, in <module>
[rank2]:     main(args, device, world_size, world_rank, local_rank)
[rank2]:   File "train.py", line 890, in main
[rank2]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank2]:   File "train.py", line 372, in training_step
[rank2]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank2]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank2]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank2]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank2]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank2]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank2]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank2]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank2]:     return _AllGather.apply(group, tensor)
[rank2]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank2]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank2]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank2]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank2]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank2]:     return func(*args, **kwargs)
[rank2]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank2]:     work = group.allgather([tensor_list], [tensor])
[rank11]: Traceback (most recent call last):
[rank11]:   File "train.py", line 986, in <module>
[rank11]:     main(args, device, world_size, world_rank, local_rank)
[rank11]:   File "train.py", line 890, in main
[rank11]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank11]:   File "train.py", line 372, in training_step
[rank11]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank11]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank11]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank11]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank11]:     return self._call_impl(*args, **kwargs)
[rank2]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 32.00 GiB. GPU 2 has a total capacity of 63.98 GiB of which 23.96 GiB is free. Of the allocated memory 38.87 GiB is allocated by PyTorch, and 120.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank11]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank11]:     return forward_call(*args, **kwargs)
[rank11]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank11]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank11]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank11]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank11]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank11]:     return _AllGather.apply(group, tensor)
[rank11]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank11]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank11]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank11]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank11]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank11]:     return func(*args, **kwargs)
[rank11]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank11]:     work = group.allgather([tensor_list], [tensor])
[rank11]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 32.00 GiB. GPU 3 has a total capacity of 63.98 GiB of which 23.78 GiB is free. Of the allocated memory 38.87 GiB is allocated by PyTorch, and 120.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
torch.Size([2, 16, 2048, 8192])
torch.Size([2, 16, 2048, 8192])
torch.Size([2, 16, 2048, 8192])
[rank1]: Traceback (most recent call last):
[rank1]:   File "train.py", line 986, in <module>
[rank1]:     main(args, device, world_size, world_rank, local_rank)
[rank1]:   File "train.py", line 890, in main
[rank1]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank1]:   File "train.py", line 372, in training_step
[rank1]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank1]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank1]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank1]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank1]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank1]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank1]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank1]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank1]:     return _AllGather.apply(group, tensor)
[rank1]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank1]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank1]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank1]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank1]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank1]:     return func(*args, **kwargs)
[rank1]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank1]:     work = group.allgather([tensor_list], [tensor])
[rank1]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 32.00 GiB. GPU 1 has a total capacity of 63.98 GiB of which 23.78 GiB is free. Of the allocated memory 38.87 GiB is allocated by PyTorch, and 120.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank12]: Traceback (most recent call last):
[rank12]:   File "train.py", line 986, in <module>
[rank12]:     main(args, device, world_size, world_rank, local_rank)
[rank12]:   File "train.py", line 890, in main
[rank12]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank12]:   File "train.py", line 372, in training_step
[rank12]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank12]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank12]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank12]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank12]:     return self._call_impl(*args, **kwargs)
[rank12]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank12]:     return forward_call(*args, **kwargs)
[rank12]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank12]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank12]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank12]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank12]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank12]:     return _AllGather.apply(group, tensor)
[rank12]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank12]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank12]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank12]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank12]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank12]:     return func(*args, **kwargs)
[rank12]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank12]:     work = group.allgather([tensor_list], [tensor])
[rank12]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 32.00 GiB. GPU 4 has a total capacity of 63.98 GiB of which 23.78 GiB is free. Of the allocated memory 38.87 GiB is allocated by PyTorch, and 120.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
torch.Size([2, 16, 2048, 8192])
torch.Size([2, 16, 2048, 8192])
torch.Size([2, 16, 2048, 8192])
[rank17]: Traceback (most recent call last):
[rank17]:   File "train.py", line 986, in <module>
[rank17]:     main(args, device, world_size, world_rank, local_rank)
[rank17]:   File "train.py", line 890, in main
[rank17]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank17]:   File "train.py", line 372, in training_step
[rank17]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank17]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank17]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank17]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank17]:     return self._call_impl(*args, **kwargs)
[rank17]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank17]:     return forward_call(*args, **kwargs)
[rank17]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank17]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank17]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank17]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank17]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank17]:     return _AllGather.apply(group, tensor)
[rank17]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank17]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank17]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank17]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank17]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank17]:     return func(*args, **kwargs)
[rank17]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank17]:     work = group.allgather([tensor_list], [tensor])
[rank17]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 32.00 GiB. GPU 1 has a total capacity of 63.98 GiB of which 23.78 GiB is free. Of the allocated memory 38.87 GiB is allocated by PyTorch, and 120.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
torch.Size([2, 16, 2048, 8192])
torch.Size([2, 16, 2048, 8192])
torch.Size([2, 16, 2048, 8192])
[rank24]: Traceback (most recent call last):
[rank24]:   File "train.py", line 986, in <module>
[rank24]:     main(args, device, world_size, world_rank, local_rank)
[rank24]:   File "train.py", line 890, in main
[rank24]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank24]:   File "train.py", line 372, in training_step
[rank24]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank24]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank24]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank24]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank24]:     return self._call_impl(*args, **kwargs)
[rank24]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank24]:     return forward_call(*args, **kwargs)
[rank24]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank24]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank24]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank24]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank24]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank24]:     return _AllGather.apply(group, tensor)
[rank24]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank24]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank24]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank24]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank24]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank24]:     return func(*args, **kwargs)
[rank24]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank24]:     work = group.allgather([tensor_list], [tensor])
[rank24]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 32.00 GiB. GPU 0 has a total capacity of 63.98 GiB of which 23.96 GiB is free. Of the allocated memory 38.87 GiB is allocated by PyTorch, and 120.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank30]: Traceback (most recent call last):
[rank30]:   File "train.py", line 986, in <module>
[rank30]:     main(args, device, world_size, world_rank, local_rank)
[rank30]:   File "train.py", line 890, in main
[rank30]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank30]:   File "train.py", line 372, in training_step
[rank30]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank30]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank30]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank30]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank30]:     return self._call_impl(*args, **kwargs)
[rank30]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank30]:     return forward_call(*args, **kwargs)
[rank30]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank30]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank30]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank30]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank30]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank30]:     return _AllGather.apply(group, tensor)
[rank20]: Traceback (most recent call last):
[rank20]:   File "train.py", line 986, in <module>
[rank20]:     main(args, device, world_size, world_rank, local_rank)
[rank20]:   File "train.py", line 890, in main
[rank20]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank20]:   File "train.py", line 372, in training_step
[rank20]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank20]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank20]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank20]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank20]:     return self._call_impl(*args, **kwargs)
[rank30]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank30]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank30]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank30]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank30]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank30]:     return func(*args, **kwargs)
[rank30]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank30]:     work = group.allgather([tensor_list], [tensor])
[rank20]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank20]:     return forward_call(*args, **kwargs)
[rank20]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank20]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank20]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank20]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank20]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank20]:     return _AllGather.apply(group, tensor)
[rank30]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 32.00 GiB. GPU 6 has a total capacity of 63.98 GiB of which 23.78 GiB is free. Of the allocated memory 38.87 GiB is allocated by PyTorch, and 120.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank20]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank20]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank20]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank20]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank20]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank20]:     return func(*args, **kwargs)
[rank20]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank20]:     work = group.allgather([tensor_list], [tensor])
[rank29]: Traceback (most recent call last):
[rank29]:   File "train.py", line 986, in <module>
[rank29]:     main(args, device, world_size, world_rank, local_rank)
[rank29]:   File "train.py", line 890, in main
[rank29]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank29]:   File "train.py", line 372, in training_step
[rank29]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank29]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank29]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank29]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank29]:     return self._call_impl(*args, **kwargs)
[rank20]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 32.00 GiB. GPU 4 has a total capacity of 63.98 GiB of which 23.78 GiB is free. Of the allocated memory 38.87 GiB is allocated by PyTorch, and 120.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank29]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank29]:     return forward_call(*args, **kwargs)
[rank29]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank29]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank29]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank29]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank29]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank29]:     return _AllGather.apply(group, tensor)
[rank29]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank29]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank29]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank29]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank29]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank29]:     return func(*args, **kwargs)
[rank29]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank29]:     work = group.allgather([tensor_list], [tensor])
[rank29]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 32.00 GiB. GPU 5 has a total capacity of 63.98 GiB of which 23.96 GiB is free. Of the allocated memory 38.87 GiB is allocated by PyTorch, and 120.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank27]: Traceback (most recent call last):
[rank27]:   File "train.py", line 986, in <module>
[rank27]:     main(args, device, world_size, world_rank, local_rank)
[rank27]:   File "train.py", line 890, in main
[rank27]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank27]:   File "train.py", line 372, in training_step
[rank27]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank27]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank27]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank27]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank27]:     return self._call_impl(*args, **kwargs)
[rank27]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank27]:     return forward_call(*args, **kwargs)
[rank27]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank27]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank27]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank27]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank27]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank27]:     return _AllGather.apply(group, tensor)
[rank27]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank27]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank27]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank27]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank27]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank27]:     return func(*args, **kwargs)
[rank27]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank27]:     work = group.allgather([tensor_list], [tensor])
[rank27]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 32.00 GiB. GPU 3 has a total capacity of 63.98 GiB of which 23.78 GiB is free. Of the allocated memory 38.87 GiB is allocated by PyTorch, and 120.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
torch.Size([2, 16, 2048, 8192])
torch.Size([2, 16, 2048, 8192])
torch.Size([2, 16, 2048, 8192])
[rank25]: Traceback (most recent call last):
[rank25]:   File "train.py", line 986, in <module>
[rank25]:     main(args, device, world_size, world_rank, local_rank)
[rank25]:   File "train.py", line 890, in main
[rank25]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank25]:   File "train.py", line 372, in training_step
[rank25]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank25]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank25]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank25]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank25]:     return self._call_impl(*args, **kwargs)
[rank25]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank25]:     return forward_call(*args, **kwargs)
[rank25]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank25]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank25]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank25]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank25]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank25]:     return _AllGather.apply(group, tensor)
[rank25]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank25]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank25]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank25]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank25]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank25]:     return func(*args, **kwargs)
[rank25]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank25]:     work = group.allgather([tensor_list], [tensor])
[rank25]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 32.00 GiB. GPU 1 has a total capacity of 63.98 GiB of which 23.78 GiB is free. Of the allocated memory 38.87 GiB is allocated by PyTorch, and 120.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank28]: Traceback (most recent call last):
[rank28]:   File "train.py", line 986, in <module>
[rank28]:     main(args, device, world_size, world_rank, local_rank)
[rank28]:   File "train.py", line 890, in main
[rank28]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank28]:   File "train.py", line 372, in training_step
[rank28]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank28]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank28]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank28]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank28]:     return self._call_impl(*args, **kwargs)
[rank28]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank28]:     return forward_call(*args, **kwargs)
[rank28]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank28]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank28]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank28]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank28]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank28]:     return _AllGather.apply(group, tensor)
[rank28]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank28]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank28]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank28]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank28]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank28]:     return func(*args, **kwargs)
[rank28]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank28]:     work = group.allgather([tensor_list], [tensor])
[rank28]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 32.00 GiB. GPU 4 has a total capacity of 63.98 GiB of which 23.78 GiB is free. Of the allocated memory 38.87 GiB is allocated by PyTorch, and 120.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank26]: Traceback (most recent call last):
[rank26]:   File "train.py", line 986, in <module>
[rank26]:     main(args, device, world_size, world_rank, local_rank)
[rank26]:   File "train.py", line 890, in main
[rank26]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank26]:   File "train.py", line 372, in training_step
[rank26]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank26]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank26]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank26]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank26]:     return self._call_impl(*args, **kwargs)
[rank26]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank26]:     return forward_call(*args, **kwargs)
[rank26]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank26]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank26]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank26]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank26]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank26]:     return _AllGather.apply(group, tensor)
[rank26]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank26]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank26]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank26]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank26]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank26]:     return func(*args, **kwargs)
[rank26]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank26]:     work = group.allgather([tensor_list], [tensor])
[rank26]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 32.00 GiB. GPU 2 has a total capacity of 63.98 GiB of which 23.96 GiB is free. Of the allocated memory 38.87 GiB is allocated by PyTorch, and 120.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank31]: Traceback (most recent call last):
[rank31]:   File "train.py", line 986, in <module>
[rank31]:     main(args, device, world_size, world_rank, local_rank)
[rank31]:   File "train.py", line 890, in main
[rank31]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank31]:   File "train.py", line 372, in training_step
[rank31]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank31]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank31]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank31]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank31]:     return self._call_impl(*args, **kwargs)
[rank31]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank31]:     return forward_call(*args, **kwargs)
[rank31]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank31]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank31]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank31]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank31]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank31]:     return _AllGather.apply(group, tensor)
[rank31]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank31]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank31]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank31]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank31]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank31]:     return func(*args, **kwargs)
[rank31]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank31]:     work = group.allgather([tensor_list], [tensor])
[rank31]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 32.00 GiB. GPU 7 has a total capacity of 63.98 GiB of which 23.96 GiB is free. Of the allocated memory 38.87 GiB is allocated by PyTorch, and 120.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
global rank 14: ddp rank 0 iter_start,iter_end = 0 8
global rank 14: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 6: ddp rank 0 iter_start,iter_end = 0 8
global rank 6: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
torch.Size([2, 16, 2048, 8192])
[rank6]: Traceback (most recent call last):
[rank6]:   File "train.py", line 986, in <module>
[rank6]:     main(args, device, world_size, world_rank, local_rank)
[rank6]:   File "train.py", line 890, in main
[rank6]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank6]:   File "train.py", line 372, in training_step
[rank6]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank6]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank6]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank6]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank6]:     return forward_call(*args, **kwargs)
[rank6]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank6]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank6]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank6]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank6]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank6]:     return _AllGather.apply(group, tensor)
[rank6]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank6]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank6]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank6]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank6]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank6]:     return func(*args, **kwargs)
[rank6]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank6]:     work = group.allgather([tensor_list], [tensor])
[rank6]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 32.00 GiB. GPU 6 has a total capacity of 63.98 GiB of which 23.78 GiB is free. Of the allocated memory 38.87 GiB is allocated by PyTorch, and 120.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
torch.Size([2, 16, 2048, 8192])
[rank14]: Traceback (most recent call last):
[rank14]:   File "train.py", line 986, in <module>
[rank14]:     main(args, device, world_size, world_rank, local_rank)
[rank14]:   File "train.py", line 890, in main
[rank14]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank14]:   File "train.py", line 372, in training_step
[rank14]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank14]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank14]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank14]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank14]:     return self._call_impl(*args, **kwargs)
[rank14]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank14]:     return forward_call(*args, **kwargs)
[rank14]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank14]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank14]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank14]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank14]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank14]:     return _AllGather.apply(group, tensor)
[rank14]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank14]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank14]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank14]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank14]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank14]:     return func(*args, **kwargs)
[rank14]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank14]:     work = group.allgather([tensor_list], [tensor])
[rank14]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 32.00 GiB. GPU 6 has a total capacity of 63.98 GiB of which 23.78 GiB is free. Of the allocated memory 38.87 GiB is allocated by PyTorch, and 120.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:13:05,445] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:13:05,445] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:13:05,445] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:13:05,445] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:13:05,445] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:13:05,445] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:13:05,445] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:13:05,445] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:13:05,447] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:13:05,447] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:13:05,447] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:13:05,447] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:13:05,447] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:13:05,447] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:13:05,447] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:13:05,447] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:13:05,520] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:13:05,520] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:13:05,520] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:13:05,520] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:13:05,520] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:13:05,520] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:13:05,520] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:13:05,520] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:13:05,657] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:13:05,657] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:13:05,657] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:13:05,657] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:13:05,657] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:13:05,657] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:13:05,657] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
--> total memory per gpu (GB) = 63.9844
[2025-03-12 17:13:05,657] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)
HERE0 Namespace(arch='orbit_hier_token_noagg_self', batch_size=2, channels=768, depth=32, embed_dim=8192, fa2=True, fsdp_size=1, imagex=128, imagey=256, max_epochs=1, num_heads=32, real=False, seq_par_size=1, simple_ddp_size=1, tensor_par_size=32, yaml_config='configs/ERA5-100million-91variables.yaml')
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
rank 8 Before initialize parallelism groups
rank 11 Before initialize parallelism groups
rank 13 Before initialize parallelism groups
Using dist.init_process_group. world_size  32
rank 5 Before initialize parallelism groups
rank 4 Before initialize parallelism groups
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
rank 10 Before initialize parallelism groups
Using dist.init_process_group. world_size  32
rank 12 Before initialize parallelism groups
rank 8 After initialize parallelism groups
rank 13 After initialize parallelism groups
rank 11 After initialize parallelism groups
rank 1 Before initialize parallelism groups
rank 5 After initialize parallelism groups
rank 4 After initialize parallelism groups
rank 15 Before initialize parallelism groups
rank 14 Before initialize parallelism groups
rank 10 After initialize parallelism groups
rank 12 After initialize parallelism groups
rank 1 After initialize parallelism groups
rank 9 Before initialize parallelism groups
rank 6 Before initialize parallelism groups
rank 15 After initialize parallelism groups
rank 14 After initialize parallelism groups
rank 7 Before initialize parallelism groups
rank 3 Before initialize parallelism groups
rank 9 After initialize parallelism groups
rank 2 Before initialize parallelism groups
rank 6 After initialize parallelism groups
rank 3 After initialize parallelism groups
rank 7 After initialize parallelism groups
rank 2 After initialize parallelism groups
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
rank 16 Before initialize parallelism groups
rank 19 Before initialize parallelism groups
rank 20 Before initialize parallelism groups
rank 18 Before initialize parallelism groups
rank 17 Before initialize parallelism groups
rank 16 After initialize parallelism groups
rank 19 After initialize parallelism groups
rank 22 Before initialize parallelism groups
rank 20 After initialize parallelism groups
rank 18 After initialize parallelism groups
rank 17 After initialize parallelism groups
rank 23 Before initialize parallelism groups
rank 21 Before initialize parallelism groups
rank 22 After initialize parallelism groups
rank 23 After initialize parallelism groups
rank 21 After initialize parallelism groups
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
rank 27 Before initialize parallelism groups
rank 28 Before initialize parallelism groups
rank 29 Before initialize parallelism groups
rank 30 Before initialize parallelism groups
rank 24 Before initialize parallelism groups
Using dist.init_process_group. world_size  32
Using dist.init_process_group. world_size  32
rank 27 After initialize parallelism groups
rank 24 After initialize parallelism groups
rank 30 After initialize parallelism groups
rank 28 After initialize parallelism groups
rank 29 After initialize parallelism groups
Using dist.init_process_group. world_size  32
config_path  configs/ERA5-100million-91variables.yaml
rank 31 Before initialize parallelism groups
rank 25 Before initialize parallelism groups
rank 31 After initialize parallelism groups
{'seed_everything': 42, 'trainer': {'checkpoint_path': '/lustre/orion/stf218/world-shared/atsaris/tmp/tmp_checkpoints', 'checkpoint_filename': 'multi_last', 'resume_from_checkpoint': False}, 'parallelism': {'cpu_offloading': False}, 'model': {'lr': 2e-05, 'beta_1': 0.9, 'beta_2': 0.95, 'weight_decay': '1e-5', 'warmup_steps': 1000, 'max_steps': 20000, 'warmup_start_lr': '1e-8', 'eta_min': '1e-8', 'net': {'class_path': 'climax.arch.ClimaX', 'init_args': {'default_vars': ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_70', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000'], 'patch_size': 4, 'decoder_depth': 2, 'mlp_ratio': 4, 'drop_path': 0.1, 'drop_rate': 0.1, 'aggregated_variables': 1}}}, 'data': {'dict_root_dirs': {'mpi-esm': '/lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0'}, 'dict_start_idx': {'mpi-esm': 0}, 'dict_end_idx': {'mpi-esm': 1}, 'dict_in_variables': {'mpi-esm': ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000']}, 'dict_out_variables': {'mpi-esm': ['geopotential_500', 'temperature_850', '2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind']}, 'dict_max_predict_ranges': {'mpi-esm': 24}, 'dict_random_lead_time': {'mpi-esm': False}, 'dict_hrs_each_step': {'mpi-esm': 1}, 'dict_buffer_sizes': {'mpi-esm': 1}, 'num_workers': 1, 'pin_memory': False}}
rank 26 Before initialize parallelism groups
rank 25 After initialize parallelism groups
max_epochs 1 data_par_size 1 fsdp_size 1 simple_ddp_size 1 tensor_par_size 32 seq_par_size 1 cpu_offloading False
lr  2e-05 beta_1  0.9 beta_2 0.95 weight_decay 1e-05 class_path climax.arch.ClimaX default_vars ['2m_temperature', '10m_u_component_of_wind', '10m_v_component_of_wind', 'geopotential_50', 'geopotential_70', 'geopotential_100', 'geopotential_150', 'geopotential_200', 'geopotential_250', 'geopotential_300', 'geopotential_400', 'geopotential_500', 'geopotential_600', 'geopotential_700', 'geopotential_850', 'geopotential_925', 'geopotential_1000', 'specific_humidity_10', 'specific_humidity_20', 'specific_humidity_30', 'specific_humidity_50', 'specific_humidity_70', 'specific_humidity_100', 'specific_humidity_150', 'specific_humidity_200', 'specific_humidity_250', 'specific_humidity_300', 'specific_humidity_400', 'specific_humidity_500', 'specific_humidity_600', 'specific_humidity_700', 'specific_humidity_850', 'specific_humidity_925', 'specific_humidity_1000', 'temperature_10', 'temperature_20', 'temperature_30', 'temperature_50', 'temperature_70', 'temperature_100', 'temperature_150', 'temperature_200', 'temperature_250', 'temperature_300', 'temperature_400', 'temperature_500', 'temperature_600', 'temperature_700', 'temperature_850', 'temperature_925', 'temperature_1000', 'u_component_of_wind_10', 'u_component_of_wind_20', 'u_component_of_wind_30', 'u_component_of_wind_50', 'u_component_of_wind_70', 'u_component_of_wind_100', 'u_component_of_wind_150', 'u_component_of_wind_200', 'u_component_of_wind_250', 'u_component_of_wind_300', 'u_component_of_wind_400', 'u_component_of_wind_500', 'u_component_of_wind_600', 'u_component_of_wind_700', 'u_component_of_wind_850', 'u_component_of_wind_925', 'u_component_of_wind_1000', 'v_component_of_wind_10', 'v_component_of_wind_20', 'v_component_of_wind_30', 'v_component_of_wind_50', 'v_component_of_wind_70', 'v_component_of_wind_100', 'v_component_of_wind_150', 'v_component_of_wind_200', 'v_component_of_wind_250', 'v_component_of_wind_300', 'v_component_of_wind_400', 'v_component_of_wind_500', 'v_component_of_wind_600', 'v_component_of_wind_700', 'v_component_of_wind_850', 'v_component_of_wind_925', 'v_component_of_wind_1000']
img_size_x 128 img_size_y 256 patch_size 4 emb_dim 8192 depth 32 decoder_depth 2 num_heads 32 mlp_ratio 4 drop_path 0.1 drop_rate 0.1 aggregated_variables 1
dict_root_dirs {'mpi-esm': '/lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0'} dict_start_idx {'mpi-esm': 0} dict_end_idx {'mpi-esm': 1} batch_size 2 num_workers 1
warmup_steps 1000 max_steps 20000 warmup_start_lr 1e-08 eta_min 1e-08
checkpoint_path /lustre/orion/stf218/world-shared/atsaris/tmp/tmp_checkpoints checkpoint_filename multi_last resume_from_checkpoint False
rank 0 Before initialize parallelism groups
rank 26 After initialize parallelism groups
rank 0 After initialize parallelism groups
rank 15 After initialize model
rank 15 Before the second dist.barrier()
rank 2 After initialize model
rank 2 Before the second dist.barrier()
rank 1 After initialize model
rank 1 Before the second dist.barrier()
rank 22 After initialize model
rank 22 Before the second dist.barrier()
rank 12 After initialize model
rank 12 Before the second dist.barrier()
rank 13 After initialize model
rank 13 Before the second dist.barrier()
rank 0 After initialize model
rank 0 Before the second dist.barrier()
rank 10 After initialize model
rank 10 Before the second dist.barrier()
rank 14 After initialize model
rank 14 Before the second dist.barrier()
rank 11 After initialize model
rank 11 Before the second dist.barrier()
rank 5 After initialize model
rank 5 Before the second dist.barrier()
rank 9 After initialize model
rank 9 Before the second dist.barrier()
rank 4 After initialize model
rank 4 Before the second dist.barrier()
rank 6 After initialize model
rank 6 Before the second dist.barrier()
rank 8 After initialize model
rank 8 Before the second dist.barrier()
rank 7 After initialize model
rank 7 Before the second dist.barrier()
rank 20 After initialize model
rank 20 Before the second dist.barrier()
rank 3 After initialize model
rank 3 Before the second dist.barrier()
rank 23 After initialize model
rank 23 Before the second dist.barrier()
rank 25 After initialize model
rank 25 Before the second dist.barrier()
rank 27 After initialize model
rank 27 Before the second dist.barrier()
rank 18 After initialize model
rank 18 Before the second dist.barrier()
rank 19 After initialize model
rank 19 Before the second dist.barrier()
rank 21 After initialize model
rank 21 Before the second dist.barrier()
rank 16 After initialize model
rank 16 Before the second dist.barrier()
rank 17 After initialize model
rank 17 Before the second dist.barrier()
rank 28 After initialize model
rank 28 Before the second dist.barrier()
rank 26 After initialize model
rank 26 Before the second dist.barrier()
rank 31 After initialize model
rank 31 Before the second dist.barrier()
rank 29 After initialize model
rank 29 Before the second dist.barrier()
rank 24 After initialize model
rank 24 Before the second dist.barrier()
rank 30 After initialize model
rank 30 Before the second dist.barrier()
rank 1 After the second dist.barrier()
rank 16 After the second dist.barrier()
rank 17 After the second dist.barrier()
rank 9 After the second dist.barrier()
rank 10 After the second dist.barrier()
rank 11 After the second dist.barrier()
rank 12 After the second dist.barrier()
rank 13 After the second dist.barrier()
rank 14 After the second dist.barrier()
rank 8 After the second dist.barrier()
rank 0 After the second dist.barrier()
rank 20 After the second dist.barrier()
rank 4 After the second dist.barrier()
rank 21 After the second dist.barrier()
rank 24 After the second dist.barrier()
rank 5 After the second dist.barrier()
parameter name  var_embed  requires_gradient  True size torch.Size([1, 768, 8192])
rank 15 After the second dist.barrier()
rank 25 After the second dist.barrier()
rank 6 After the second dist.barrier()
rank 22 After the second dist.barrier()
rank 23 After the second dist.barrier()
rank 28 After the second dist.barrier()
rank 29 After the second dist.barrier()
parameter name  var_query  requires_gradient  True size torch.Size([1, 1, 8192])
rank 7 After the second dist.barrier()
parameter name  pos_embed  requires_gradient  True size torch.Size([1, 2048, 8192])
parameter name  token_embeds.0.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.0.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.1.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
rank 3 After the second dist.barrier()
parameter name  token_embeds.1.proj.bias  requires_gradient  True size torch.Size([8192])
rank 18 After the second dist.barrier()
parameter name  token_embeds.2.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
rank 19 After the second dist.barrier()
parameter name  token_embeds.2.proj.bias  requires_gradient  True size torch.Size([8192])
rank 30 After the second dist.barrier()
parameter name  token_embeds.3.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.3.proj.bias  requires_gradient  True size torch.Size([8192])
rank 2 After the second dist.barrier()
parameter name  token_embeds.4.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.4.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.5.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
rank 31 After the second dist.barrier()
parameter name  token_embeds.5.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.6.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.6.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.7.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
rank 27 After the second dist.barrier()
parameter name  token_embeds.7.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.8.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.8.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.9.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.9.proj.bias  requires_gradient  True size torch.Size([8192])
rank 26 After the second dist.barrier()
parameter name  token_embeds.10.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.10.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.11.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.11.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.12.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.12.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.13.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.13.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.14.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.14.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.15.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.15.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.16.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.16.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.17.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.17.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.18.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.18.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.19.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.19.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.20.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.20.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.21.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.21.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.22.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.22.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.23.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.23.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.24.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.24.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.25.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.25.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.26.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.26.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.27.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.27.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.28.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.28.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.29.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.29.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.30.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.30.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.31.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.31.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.32.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.32.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.33.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.33.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.34.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.34.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.35.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.35.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.36.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.36.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.37.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.37.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.38.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.38.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.39.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.39.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.40.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.40.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.41.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.41.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.42.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.42.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.43.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.43.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.44.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.44.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.45.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.45.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.46.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.46.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.47.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.47.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.48.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.48.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.49.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.49.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.50.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.50.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.51.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.51.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.52.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.52.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.53.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.53.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.54.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.54.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.55.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.55.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.56.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.56.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.57.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.57.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.58.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.58.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.59.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.59.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.60.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.60.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.61.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.61.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.62.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.62.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.63.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.63.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.64.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.64.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.65.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.65.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.66.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.66.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.67.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.67.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.68.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.68.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.69.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.69.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.70.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.70.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.71.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.71.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.72.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.72.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.73.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.73.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.74.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.74.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.75.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.75.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.76.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.76.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.77.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.77.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.78.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.78.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.79.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.79.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.80.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.80.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.81.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.81.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.82.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.82.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.83.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.83.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.84.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.84.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.85.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.85.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.86.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.86.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.87.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.87.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.88.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.88.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.89.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.89.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.90.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.90.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.91.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.91.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.92.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.92.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.93.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.93.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.94.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.94.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.95.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.95.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.96.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.96.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.97.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.97.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.98.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.98.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.99.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.99.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.100.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.100.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.101.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.101.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.102.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.102.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.103.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.103.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.104.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.104.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.105.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.105.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.106.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.106.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.107.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.107.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.108.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.108.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.109.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.109.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.110.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.110.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.111.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.111.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.112.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.112.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.113.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.113.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.114.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.114.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.115.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.115.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.116.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.116.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.117.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.117.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.118.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.118.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.119.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.119.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.120.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.120.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.121.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.121.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.122.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.122.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.123.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.123.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.124.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.124.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.125.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.125.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.126.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.126.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.127.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.127.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.128.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.128.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.129.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.129.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.130.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.130.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.131.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.131.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.132.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.132.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.133.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.133.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.134.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.134.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.135.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.135.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.136.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.136.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.137.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.137.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.138.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.138.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.139.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.139.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.140.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.140.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.141.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.141.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.142.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.142.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.143.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.143.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.144.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.144.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.145.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.145.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.146.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.146.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.147.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.147.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.148.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.148.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.149.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.149.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.150.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.150.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.151.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.151.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.152.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.152.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.153.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.153.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.154.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.154.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.155.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.155.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.156.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.156.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.157.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.157.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.158.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.158.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.159.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.159.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.160.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.160.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.161.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.161.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.162.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.162.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.163.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.163.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.164.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.164.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.165.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.165.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.166.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.166.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.167.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.167.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.168.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.168.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.169.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.169.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.170.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.170.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.171.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.171.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.172.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.172.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.173.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.173.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.174.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.174.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.175.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.175.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.176.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.176.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.177.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.177.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.178.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.178.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.179.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.179.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.180.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.180.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.181.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.181.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.182.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.182.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.183.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.183.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.184.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.184.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.185.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.185.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.186.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.186.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.187.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.187.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.188.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.188.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.189.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.189.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.190.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.190.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.191.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.191.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.192.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.192.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.193.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.193.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.194.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.194.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.195.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.195.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.196.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.196.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.197.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.197.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.198.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.198.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.199.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.199.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.200.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.200.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.201.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.201.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.202.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.202.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.203.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.203.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.204.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.204.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.205.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.205.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.206.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.206.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.207.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.207.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.208.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.208.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.209.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.209.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.210.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.210.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.211.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.211.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.212.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.212.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.213.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.213.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.214.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.214.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.215.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.215.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.216.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.216.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.217.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.217.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.218.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.218.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.219.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.219.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.220.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.220.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.221.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.221.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.222.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.222.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.223.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.223.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.224.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.224.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.225.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.225.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.226.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.226.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.227.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.227.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.228.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.228.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.229.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.229.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.230.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.230.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.231.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.231.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.232.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.232.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.233.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.233.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.234.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.234.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.235.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.235.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.236.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.236.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.237.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.237.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.238.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.238.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.239.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.239.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.240.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.240.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.241.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.241.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.242.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.242.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.243.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.243.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.244.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.244.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.245.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.245.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.246.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.246.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.247.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.247.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.248.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.248.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.249.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.249.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.250.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.250.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.251.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.251.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.252.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.252.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.253.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.253.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.254.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.254.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.255.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.255.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.256.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.256.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.257.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.257.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.258.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.258.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.259.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.259.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.260.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.260.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.261.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.261.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.262.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.262.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.263.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.263.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.264.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.264.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.265.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.265.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.266.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.266.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.267.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.267.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.268.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.268.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.269.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.269.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.270.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.270.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.271.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.271.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.272.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.272.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.273.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.273.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.274.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.274.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.275.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.275.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.276.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.276.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.277.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.277.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.278.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.278.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.279.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.279.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.280.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.280.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.281.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.281.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.282.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.282.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.283.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.283.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.284.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.284.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.285.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.285.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.286.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.286.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.287.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.287.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.288.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.288.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.289.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.289.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.290.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.290.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.291.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.291.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.292.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.292.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.293.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.293.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.294.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.294.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.295.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.295.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.296.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.296.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.297.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.297.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.298.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.298.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.299.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.299.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.300.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.300.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.301.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.301.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.302.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.302.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.303.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.303.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.304.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.304.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.305.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.305.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.306.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.306.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.307.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.307.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.308.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.308.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.309.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.309.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.310.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.310.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.311.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.311.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.312.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.312.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.313.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.313.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.314.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.314.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.315.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.315.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.316.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.316.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.317.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.317.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.318.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.318.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.319.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.319.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.320.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.320.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.321.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.321.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.322.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.322.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.323.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.323.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.324.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.324.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.325.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.325.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.326.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.326.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.327.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.327.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.328.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.328.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.329.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.329.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.330.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.330.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.331.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.331.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.332.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.332.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.333.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.333.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.334.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.334.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.335.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.335.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.336.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.336.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.337.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.337.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.338.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.338.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.339.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.339.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.340.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.340.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.341.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.341.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.342.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.342.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.343.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.343.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.344.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.344.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.345.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.345.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.346.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.346.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.347.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.347.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.348.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.348.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.349.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.349.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.350.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.350.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.351.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.351.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.352.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.352.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.353.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.353.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.354.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.354.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.355.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.355.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.356.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.356.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.357.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.357.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.358.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.358.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.359.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.359.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.360.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.360.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.361.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.361.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.362.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.362.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.363.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.363.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.364.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.364.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.365.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.365.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.366.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.366.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.367.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.367.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.368.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.368.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.369.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.369.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.370.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.370.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.371.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.371.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.372.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.372.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.373.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.373.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.374.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.374.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.375.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.375.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.376.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.376.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.377.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.377.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.378.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.378.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.379.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.379.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.380.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.380.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.381.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.381.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.382.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.382.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.383.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.383.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.384.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.384.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.385.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.385.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.386.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.386.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.387.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.387.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.388.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.388.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.389.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.389.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.390.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.390.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.391.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.391.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.392.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.392.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.393.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.393.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.394.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.394.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.395.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.395.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.396.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.396.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.397.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.397.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.398.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.398.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.399.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.399.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.400.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.400.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.401.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.401.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.402.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.402.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.403.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.403.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.404.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.404.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.405.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.405.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.406.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.406.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.407.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.407.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.408.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.408.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.409.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.409.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.410.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.410.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.411.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.411.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.412.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.412.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.413.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.413.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.414.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.414.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.415.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.415.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.416.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.416.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.417.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.417.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.418.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.418.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.419.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.419.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.420.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.420.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.421.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.421.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.422.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.422.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.423.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.423.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.424.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.424.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.425.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.425.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.426.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.426.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.427.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.427.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.428.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.428.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.429.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.429.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.430.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.430.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.431.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.431.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.432.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.432.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.433.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.433.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.434.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.434.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.435.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.435.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.436.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.436.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.437.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.437.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.438.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.438.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.439.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.439.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.440.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.440.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.441.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.441.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.442.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.442.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.443.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.443.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.444.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.444.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.445.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.445.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.446.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.446.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.447.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.447.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.448.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.448.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.449.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.449.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.450.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.450.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.451.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.451.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.452.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.452.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.453.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.453.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.454.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.454.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.455.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.455.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.456.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.456.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.457.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.457.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.458.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.458.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.459.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.459.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.460.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.460.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.461.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.461.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.462.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.462.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.463.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.463.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.464.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.464.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.465.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.465.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.466.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.466.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.467.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.467.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.468.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.468.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.469.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.469.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.470.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.470.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.471.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.471.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.472.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.472.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.473.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.473.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.474.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.474.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.475.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.475.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.476.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.476.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.477.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.477.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.478.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.478.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.479.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.479.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.480.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.480.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.481.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.481.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.482.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.482.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.483.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.483.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.484.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.484.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.485.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.485.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.486.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.486.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.487.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.487.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.488.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.488.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.489.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.489.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.490.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.490.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.491.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.491.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.492.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.492.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.493.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.493.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.494.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.494.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.495.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.495.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.496.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.496.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.497.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.497.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.498.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.498.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.499.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.499.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.500.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.500.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.501.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.501.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.502.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.502.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.503.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.503.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.504.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.504.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.505.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.505.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.506.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.506.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.507.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.507.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.508.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.508.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.509.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.509.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.510.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.510.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.511.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.511.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.512.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.512.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.513.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.513.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.514.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.514.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.515.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.515.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.516.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.516.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.517.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.517.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.518.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.518.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.519.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.519.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.520.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.520.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.521.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.521.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.522.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.522.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.523.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.523.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.524.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.524.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.525.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.525.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.526.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.526.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.527.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.527.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.528.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.528.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.529.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.529.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.530.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.530.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.531.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.531.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.532.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.532.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.533.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.533.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.534.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.534.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.535.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.535.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.536.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.536.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.537.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.537.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.538.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.538.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.539.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.539.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.540.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.540.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.541.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.541.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.542.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.542.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.543.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.543.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.544.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.544.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.545.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.545.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.546.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.546.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.547.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.547.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.548.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.548.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.549.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.549.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.550.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.550.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.551.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.551.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.552.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.552.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.553.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.553.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.554.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.554.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.555.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.555.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.556.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.556.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.557.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.557.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.558.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.558.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.559.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.559.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.560.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.560.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.561.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.561.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.562.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.562.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.563.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.563.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.564.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.564.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.565.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.565.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.566.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.566.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.567.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.567.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.568.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.568.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.569.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.569.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.570.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.570.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.571.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.571.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.572.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.572.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.573.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.573.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.574.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.574.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.575.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.575.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.576.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.576.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.577.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.577.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.578.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.578.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.579.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.579.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.580.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.580.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.581.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.581.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.582.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.582.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.583.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.583.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.584.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.584.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.585.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.585.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.586.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.586.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.587.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.587.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.588.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.588.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.589.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.589.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.590.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.590.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.591.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.591.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.592.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.592.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.593.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.593.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.594.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.594.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.595.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.595.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.596.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.596.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.597.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.597.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.598.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.598.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.599.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.599.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.600.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.600.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.601.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.601.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.602.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.602.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.603.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.603.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.604.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.604.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.605.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.605.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.606.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.606.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.607.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.607.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.608.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.608.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.609.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.609.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.610.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.610.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.611.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.611.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.612.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.612.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.613.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.613.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.614.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.614.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.615.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.615.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.616.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.616.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.617.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.617.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.618.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.618.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.619.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.619.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.620.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.620.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.621.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.621.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.622.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.622.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.623.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.623.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.624.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.624.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.625.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.625.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.626.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.626.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.627.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.627.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.628.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.628.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.629.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.629.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.630.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.630.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.631.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.631.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.632.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.632.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.633.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.633.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.634.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.634.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.635.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.635.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.636.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.636.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.637.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.637.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.638.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.638.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.639.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.639.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.640.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.640.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.641.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.641.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.642.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.642.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.643.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.643.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.644.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.644.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.645.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.645.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.646.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.646.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.647.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.647.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.648.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.648.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.649.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.649.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.650.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.650.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.651.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.651.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.652.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.652.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.653.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.653.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.654.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.654.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.655.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.655.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.656.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.656.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.657.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.657.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.658.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.658.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.659.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.659.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.660.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.660.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.661.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.661.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.662.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.662.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.663.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.663.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.664.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.664.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.665.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.665.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.666.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.666.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.667.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.667.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.668.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.668.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.669.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.669.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.670.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.670.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.671.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.671.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.672.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.672.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.673.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.673.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.674.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.674.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.675.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.675.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.676.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.676.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.677.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.677.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.678.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.678.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.679.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.679.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.680.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.680.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.681.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.681.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.682.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.682.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.683.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.683.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.684.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.684.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.685.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.685.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.686.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.686.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.687.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.687.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.688.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.688.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.689.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.689.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.690.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.690.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.691.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.691.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.692.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.692.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.693.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.693.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.694.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.694.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.695.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.695.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.696.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.696.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.697.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.697.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.698.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.698.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.699.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.699.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.700.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.700.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.701.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.701.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.702.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.702.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.703.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.703.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.704.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.704.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.705.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.705.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.706.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.706.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.707.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.707.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.708.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.708.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.709.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.709.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.710.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.710.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.711.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.711.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.712.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.712.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.713.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.713.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.714.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.714.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.715.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.715.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.716.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.716.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.717.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.717.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.718.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.718.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.719.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.719.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.720.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.720.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.721.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.721.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.722.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.722.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.723.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.723.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.724.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.724.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.725.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.725.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.726.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.726.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.727.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.727.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.728.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.728.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.729.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.729.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.730.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.730.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.731.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.731.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.732.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.732.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.733.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.733.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.734.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.734.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.735.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.735.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.736.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.736.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.737.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.737.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.738.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.738.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.739.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.739.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.740.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.740.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.741.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.741.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.742.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.742.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.743.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.743.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.744.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.744.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.745.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.745.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.746.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.746.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.747.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.747.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.748.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.748.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.749.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.749.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.750.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.750.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.751.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.751.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.752.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.752.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.753.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.753.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.754.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.754.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.755.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.755.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.756.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.756.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.757.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.757.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.758.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.758.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.759.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.759.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.760.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.760.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.761.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.761.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.762.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.762.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.763.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.763.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.764.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.764.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.765.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.765.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.766.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.766.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  token_embeds.767.proj.weight  requires_gradient  True size torch.Size([8192, 1, 4, 4])
parameter name  token_embeds.767.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  var_agg.q.weight  requires_gradient  True size torch.Size([256, 8192])
parameter name  var_agg.kv.weight  requires_gradient  True size torch.Size([512, 8192])
parameter name  var_agg.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  var_agg.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  lead_time_embed.weight  requires_gradient  True size torch.Size([8192, 1])
parameter name  lead_time_embed.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.0.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.0.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.0.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.0.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.0.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.0.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.0.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.1.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.1.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.1.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.1.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.1.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.1.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.1.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.2.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.2.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.2.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.2.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.2.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.2.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.2.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.3.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.3.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.3.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.3.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.3.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.3.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.3.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.4.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.4.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.4.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.4.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.4.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.4.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.4.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.5.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.5.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.5.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.5.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.5.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.5.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.5.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.6.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.6.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.6.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.6.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.6.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.6.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.6.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.7.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.7.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.7.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.7.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.7.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.7.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.7.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.8.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.8.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.8.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.8.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.8.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.8.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.8.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.9.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.9.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.9.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.9.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.9.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.9.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.9.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.10.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.10.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.10.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.10.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.10.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.10.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.10.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.11.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.11.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.11.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.11.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.11.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.11.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.11.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.12.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.12.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.12.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.12.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.12.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.12.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.12.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.13.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.13.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.13.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.13.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.13.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.13.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.13.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.14.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.14.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.14.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.14.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.14.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.14.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.14.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.15.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.15.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.15.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.15.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.15.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.15.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.15.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.16.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.16.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.16.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.16.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.16.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.16.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.16.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.17.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.17.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.17.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.17.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.17.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.17.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.17.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.18.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.18.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.18.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.18.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.18.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.18.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.18.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.19.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.19.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.19.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.19.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.19.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.19.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.19.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.20.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.20.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.20.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.20.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.20.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.20.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.20.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.21.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.21.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.21.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.21.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.21.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.21.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.21.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.22.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.22.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.22.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.22.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.22.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.22.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.22.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.23.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.23.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.23.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.23.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.23.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.23.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.23.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.24.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.24.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.24.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.24.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.24.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.24.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.24.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.25.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.25.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.25.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.25.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.25.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.25.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.25.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.26.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.26.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.26.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.26.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.26.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.26.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.26.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.27.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.27.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.27.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.27.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.27.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.27.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.27.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.28.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.28.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.28.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.28.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.28.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.28.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.28.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.29.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.29.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.29.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.29.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.29.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.29.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.29.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.30.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.30.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.30.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.30.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.30.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.30.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.30.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm1.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm1.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.attn.qkv.weight  requires_gradient  True size torch.Size([768, 8192])
parameter name  blocks.31.attn.qkv.bias  requires_gradient  True size torch.Size([768])
parameter name  blocks.31.attn.proj.weight  requires_gradient  True size torch.Size([8192, 256])
parameter name  blocks.31.attn.proj.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm2.weight  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.norm2.bias  requires_gradient  True size torch.Size([8192])
parameter name  blocks.31.mlp.fc1.weight  requires_gradient  True size torch.Size([1024, 8192])
parameter name  blocks.31.mlp.fc1.bias  requires_gradient  True size torch.Size([1024])
parameter name  blocks.31.mlp.fc2.weight  requires_gradient  True size torch.Size([8192, 1024])
parameter name  blocks.31.mlp.fc2.bias  requires_gradient  True size torch.Size([8192])
parameter name  norm.weight  requires_gradient  True size torch.Size([8192])
parameter name  norm.bias  requires_gradient  True size torch.Size([8192])
parameter name  head.0.weight  requires_gradient  True size torch.Size([8192, 8192])
parameter name  head.0.bias  requires_gradient  True size torch.Size([8192])
parameter name  head.2.weight  requires_gradient  True size torch.Size([8192, 8192])
parameter name  head.2.bias  requires_gradient  True size torch.Size([8192])
parameter name  head.5.weight  requires_gradient  True size torch.Size([12288, 8192])
parameter name  head.5.bias  requires_gradient  True size torch.Size([12288])
total_params before FSDP tensor(26423136256) params_per_gpu tensor(1180307456)
total_params after FSDP FullyShardedDataParallel(
  (_fsdp_wrapped_module): ClimaX(
    (token_embeds): ModuleList(
      (0-767): 768 x PatchEmbed(
        (proj): Conv2d(1, 8192, kernel_size=(4, 4), stride=(4, 4))
        (norm): Identity()
      )
    )
    (var_agg): FullyShardedDataParallel(
      (_fsdp_wrapped_module): CheckpointWrapper(
        (_checkpoint_wrapped_module): VariableMapping_Attention(
          (q): Linear(in_features=8192, out_features=256, bias=False)
          (kv): Linear(in_features=8192, out_features=512, bias=False)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=8192, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (lead_time_embed): Linear(in_features=1, out_features=8192, bias=True)
    (pos_drop): Dropout(p=0.1, inplace=False)
    (blocks): ModuleList(
      (0-31): 32 x FullyShardedDataParallel(
        (_fsdp_wrapped_module): CheckpointWrapper(
          (_checkpoint_wrapped_module): Block(
            (norm1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
            (attn): Attention(
              (qkv): Linear(in_features=8192, out_features=768, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=256, out_features=8192, bias=True)
              (proj_drop): Dropout(p=0.1, inplace=False)
            )
            (ls1): Identity()
            (norm2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=8192, out_features=1024, bias=True)
              (act): GELU(approximate='none')
              (drop1): Dropout(p=0.1, inplace=False)
              (fc2): Linear(in_features=1024, out_features=8192, bias=True)
            )
            (ls2): Identity()
          )
        )
      )
    )
    (norm): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
    (head): Sequential(
      (0): Linear(in_features=8192, out_features=8192, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=8192, out_features=8192, bias=True)
      (3): GELU(approximate='none')
      (4): Pred_Rearrange()
      (5): Linear(in_features=8192, out_features=12288, bias=True)
    )
  )
)
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
rank  31 lister_train reset: mpi-esm 9 9 9
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
use_ddstore is : 0
len(dict_root_dirs) 1
rank  0 lister_train reset: mpi-esm 9 9 9
use_ddstore is : 0
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
memory stats reset, ready to track
/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torchvision-0.18.0a0+4c0f441-py3.8-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: ''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
global rank 16: ddp rank 0 iter_start,iter_end = 0 8
global rank 16: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 19: ddp rank 0 iter_start,iter_end = 0 8
global rank 19: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 23: ddp rank 0 iter_start,iter_end = 0 8
global rank 23: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 17: ddp rank 0 iter_start,iter_end = 0 8
global rank 17: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 6: ddp rank 0 iter_start,iter_end = 0 8
global rank 6: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 0: ddp rank 0 iter_start,iter_end = 0 8
global rank 0: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 5: ddp rank 0 iter_start,iter_end = 0 8
global rank 5: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 20: ddp rank 0 iter_start,iter_end = 0 8
global rank 20: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 2: ddp rank 0 iter_start,iter_end = 0 8
global rank 2: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 22: ddp rank 0 iter_start,iter_end = 0 8
global rank 22: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 7: ddp rank 0 iter_start,iter_end = 0 8
global rank 7: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 1: ddp rank 0 iter_start,iter_end = 0 8
global rank 1: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 18: ddp rank 0 iter_start,iter_end = 0 8
global rank 18: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 21: ddp rank 0 iter_start,iter_end = 0 8
global rank 21: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 30: ddp rank 0 iter_start,iter_end = 0 8
global rank 30: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 25: ddp rank 0 iter_start,iter_end = 0 8
global rank 25: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 4: ddp rank 0 iter_start,iter_end = 0 8
global rank 4: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
torch.Size([2, 24, 2048, 8192])
torch.Size([2, 24, 2048, 8192])
global rank 3: ddp rank 0 iter_start,iter_end = 0 8
global rank 3: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 27: ddp rank 0 iter_start,iter_end = 0 8
global rank 27: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
torch.Size([2, 24, 2048, 8192])
[rank5]: Traceback (most recent call last):
[rank5]:   File "train.py", line 986, in <module>
[rank5]:     main(args, device, world_size, world_rank, local_rank)
[rank5]:   File "train.py", line 890, in main
[rank5]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank5]:   File "train.py", line 372, in training_step
[rank5]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank5]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank5]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank5]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank5]:     return self._call_impl(*args, **kwargs)
[rank5]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank5]:     return forward_call(*args, **kwargs)
[rank5]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank5]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank5]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank5]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank5]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank5]:     return _AllGather.apply(group, tensor)
[rank5]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank5]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank5]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank5]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank5]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank5]:     return func(*args, **kwargs)
[rank5]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank5]:     work = group.allgather([tensor_list], [tensor])
[rank5]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 48.00 GiB. GPU 5 has a total capacity of 63.98 GiB of which 6.44 GiB is free. Of the allocated memory 56.36 GiB is allocated by PyTorch, and 154.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]: Traceback (most recent call last):
[rank0]:   File "train.py", line 986, in <module>
[rank0]:     main(args, device, world_size, world_rank, local_rank)
[rank0]:   File "train.py", line 890, in main
[rank0]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank0]:   File "train.py", line 372, in training_step
[rank0]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank0]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank0]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank0]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank0]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank0]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank0]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank0]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank0]:     return _AllGather.apply(group, tensor)
[rank0]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank0]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank0]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank0]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank0]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank0]:     work = group.allgather([tensor_list], [tensor])
[rank0]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 48.00 GiB. GPU 0 has a total capacity of 63.98 GiB of which 6.44 GiB is free. Of the allocated memory 56.36 GiB is allocated by PyTorch, and 156.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
torch.Size([2, 24, 2048, 8192])
[rank16]: Traceback (most recent call last):
[rank16]:   File "train.py", line 986, in <module>
[rank16]:     main(args, device, world_size, world_rank, local_rank)
[rank16]:   File "train.py", line 890, in main
[rank16]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank16]:   File "train.py", line 372, in training_step
[rank16]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank16]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank16]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank16]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank16]:     return self._call_impl(*args, **kwargs)
[rank16]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank16]:     return forward_call(*args, **kwargs)
[rank16]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank16]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank16]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank16]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank16]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank16]:     return _AllGather.apply(group, tensor)
[rank16]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank16]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank16]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank16]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank16]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank16]:     return func(*args, **kwargs)
[rank16]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank16]:     work = group.allgather([tensor_list], [tensor])
[rank16]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 48.00 GiB. GPU 0 has a total capacity of 63.98 GiB of which 6.44 GiB is free. Of the allocated memory 56.36 GiB is allocated by PyTorch, and 154.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank20]: Traceback (most recent call last):
[rank20]:   File "train.py", line 986, in <module>
[rank20]:     main(args, device, world_size, world_rank, local_rank)
[rank20]:   File "train.py", line 890, in main
[rank20]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank20]:   File "train.py", line 372, in training_step
[rank20]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank20]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank20]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank20]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank20]:     return self._call_impl(*args, **kwargs)
[rank20]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank20]:     return forward_call(*args, **kwargs)
[rank20]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank20]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank20]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank20]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank20]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank20]:     return _AllGather.apply(group, tensor)
[rank20]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank20]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank20]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank20]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank20]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank20]:     return func(*args, **kwargs)
[rank20]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank20]:     work = group.allgather([tensor_list], [tensor])
[rank20]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 48.00 GiB. GPU 4 has a total capacity of 63.98 GiB of which 6.25 GiB is free. Of the allocated memory 56.36 GiB is allocated by PyTorch, and 154.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
torch.Size([2, 24, 2048, 8192])
torch.Size([2, 24, 2048, 8192])
[rank2]: Traceback (most recent call last):
[rank2]:   File "train.py", line 986, in <module>
[rank2]:     main(args, device, world_size, world_rank, local_rank)
[rank2]:   File "train.py", line 890, in main
[rank2]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank2]:   File "train.py", line 372, in training_step
[rank2]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank2]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank2]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank2]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank2]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank2]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank2]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank2]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank2]:     return _AllGather.apply(group, tensor)
[rank2]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank2]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank2]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank2]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank2]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank2]:     return func(*args, **kwargs)
[rank2]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank2]:     work = group.allgather([tensor_list], [tensor])
[rank2]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 48.00 GiB. GPU 2 has a total capacity of 63.98 GiB of which 6.44 GiB is free. Of the allocated memory 56.36 GiB is allocated by PyTorch, and 154.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
torch.Size([2, 24, 2048, 8192])
[rank22]: Traceback (most recent call last):
[rank22]:   File "train.py", line 986, in <module>
[rank22]:     main(args, device, world_size, world_rank, local_rank)
[rank22]:   File "train.py", line 890, in main
[rank22]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank22]:   File "train.py", line 372, in training_step
[rank22]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank22]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank22]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank22]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank22]:     return self._call_impl(*args, **kwargs)
[rank22]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank22]:     return forward_call(*args, **kwargs)
[rank22]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank22]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank22]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank22]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank22]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank22]:     return _AllGather.apply(group, tensor)
[rank22]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank22]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank22]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank22]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank22]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank22]:     return func(*args, **kwargs)
[rank22]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank22]:     work = group.allgather([tensor_list], [tensor])
[rank22]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 48.00 GiB. GPU 6 has a total capacity of 63.98 GiB of which 6.25 GiB is free. Of the allocated memory 56.36 GiB is allocated by PyTorch, and 154.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
torch.Size([2, 24, 2048, 8192])
torch.Size([2, 24, 2048, 8192])
[rank7]: Traceback (most recent call last):
[rank7]:   File "train.py", line 986, in <module>
[rank7]:     main(args, device, world_size, world_rank, local_rank)
[rank7]:   File "train.py", line 890, in main
[rank7]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank7]:   File "train.py", line 372, in training_step
[rank7]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank7]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank7]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank7]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank7]:     return self._call_impl(*args, **kwargs)
[rank7]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank7]:     return forward_call(*args, **kwargs)
[rank7]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank7]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank7]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank7]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank7]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank7]:     return _AllGather.apply(group, tensor)
[rank7]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank7]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank7]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank7]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank7]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank7]:     return func(*args, **kwargs)
[rank7]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank7]:     work = group.allgather([tensor_list], [tensor])
[rank7]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 48.00 GiB. GPU 7 has a total capacity of 63.98 GiB of which 6.44 GiB is free. Of the allocated memory 56.36 GiB is allocated by PyTorch, and 154.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank1]: Traceback (most recent call last):
[rank1]:   File "train.py", line 986, in <module>
[rank1]:     main(args, device, world_size, world_rank, local_rank)
[rank1]:   File "train.py", line 890, in main
[rank1]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank1]:   File "train.py", line 372, in training_step
[rank1]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank1]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank1]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank1]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank1]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank1]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank1]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank1]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank1]:     return _AllGather.apply(group, tensor)
[rank1]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank1]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank1]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank1]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank1]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank1]:     return func(*args, **kwargs)
[rank1]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank1]:     work = group.allgather([tensor_list], [tensor])
[rank1]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 48.00 GiB. GPU 1 has a total capacity of 63.98 GiB of which 6.25 GiB is free. Of the allocated memory 56.36 GiB is allocated by PyTorch, and 154.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
torch.Size([2, 24, 2048, 8192])
[rank19]: Traceback (most recent call last):
[rank19]:   File "train.py", line 986, in <module>
[rank19]:     main(args, device, world_size, world_rank, local_rank)
[rank19]:   File "train.py", line 890, in main
[rank19]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank19]:   File "train.py", line 372, in training_step
[rank19]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank19]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank19]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank19]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank19]:     return self._call_impl(*args, **kwargs)
[rank19]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank19]:     return forward_call(*args, **kwargs)
[rank19]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank19]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank19]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank19]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank19]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank19]:     return _AllGather.apply(group, tensor)
[rank19]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank19]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank19]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank19]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank19]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank19]:     return func(*args, **kwargs)
[rank19]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank19]:     work = group.allgather([tensor_list], [tensor])
[rank19]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 48.00 GiB. GPU 3 has a total capacity of 63.98 GiB of which 6.25 GiB is free. Of the allocated memory 56.36 GiB is allocated by PyTorch, and 154.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank18]: Traceback (most recent call last):
[rank18]:   File "train.py", line 986, in <module>
[rank18]:     main(args, device, world_size, world_rank, local_rank)
[rank18]:   File "train.py", line 890, in main
[rank18]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank18]:   File "train.py", line 372, in training_step
[rank18]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank18]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank18]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank18]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank18]:     return self._call_impl(*args, **kwargs)
[rank18]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank18]:     return forward_call(*args, **kwargs)
[rank18]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank18]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank18]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank18]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank18]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank18]:     return _AllGather.apply(group, tensor)
[rank18]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank18]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank18]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank18]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank18]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank18]:     return func(*args, **kwargs)
[rank18]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank18]:     work = group.allgather([tensor_list], [tensor])
[rank18]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 48.00 GiB. GPU 2 has a total capacity of 63.98 GiB of which 6.44 GiB is free. Of the allocated memory 56.36 GiB is allocated by PyTorch, and 154.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
torch.Size([2, 24, 2048, 8192])
torch.Size([2, 24, 2048, 8192])
global rank 24: ddp rank 0 iter_start,iter_end = 0 8
global rank 24: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
[rank23]: Traceback (most recent call last):
[rank23]:   File "train.py", line 986, in <module>
[rank23]:     main(args, device, world_size, world_rank, local_rank)
[rank23]:   File "train.py", line 890, in main
[rank23]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank23]:   File "train.py", line 372, in training_step
[rank23]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank23]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank23]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank23]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank23]:     return self._call_impl(*args, **kwargs)
[rank23]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank23]:     return forward_call(*args, **kwargs)
[rank23]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank23]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank23]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank23]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank23]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank23]:     return _AllGather.apply(group, tensor)
[rank23]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank23]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank23]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank23]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank23]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank23]:     return func(*args, **kwargs)
[rank23]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank23]:     work = group.allgather([tensor_list], [tensor])
[rank23]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 48.00 GiB. GPU 7 has a total capacity of 63.98 GiB of which 6.44 GiB is free. Of the allocated memory 56.36 GiB is allocated by PyTorch, and 154.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank21]: Traceback (most recent call last):
[rank21]:   File "train.py", line 986, in <module>
[rank21]:     main(args, device, world_size, world_rank, local_rank)
[rank21]:   File "train.py", line 890, in main
[rank21]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank21]:   File "train.py", line 372, in training_step
[rank21]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank21]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank21]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank21]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank21]:     return self._call_impl(*args, **kwargs)
[rank21]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank21]:     return forward_call(*args, **kwargs)
[rank21]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank21]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank21]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank21]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank21]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank21]:     return _AllGather.apply(group, tensor)
[rank21]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank21]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank21]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank21]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank21]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank21]:     return func(*args, **kwargs)
[rank21]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank21]:     work = group.allgather([tensor_list], [tensor])
[rank21]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 48.00 GiB. GPU 5 has a total capacity of 63.98 GiB of which 6.44 GiB is free. Of the allocated memory 56.36 GiB is allocated by PyTorch, and 154.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
torch.Size([2, 24, 2048, 8192])
global rank 29: ddp rank 0 iter_start,iter_end = 0 8
global rank 29: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 31: ddp rank 0 iter_start,iter_end = 0 8
global rank 31: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
[rank17]: Traceback (most recent call last):
[rank17]:   File "train.py", line 986, in <module>
[rank17]:     main(args, device, world_size, world_rank, local_rank)
[rank17]:   File "train.py", line 890, in main
[rank17]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank17]:   File "train.py", line 372, in training_step
[rank17]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank17]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank17]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank17]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank17]:     return self._call_impl(*args, **kwargs)
[rank17]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank17]:     return forward_call(*args, **kwargs)
[rank17]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank17]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank17]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank17]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank17]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank17]:     return _AllGather.apply(group, tensor)
[rank17]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank17]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank17]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank17]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank17]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank17]:     return func(*args, **kwargs)
[rank17]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank17]:     work = group.allgather([tensor_list], [tensor])
[rank17]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 48.00 GiB. GPU 1 has a total capacity of 63.98 GiB of which 6.25 GiB is free. Of the allocated memory 56.36 GiB is allocated by PyTorch, and 154.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
global rank 26: ddp rank 0 iter_start,iter_end = 0 8
global rank 26: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
torch.Size([2, 24, 2048, 8192])
[rank6]: Traceback (most recent call last):
[rank6]:   File "train.py", line 986, in <module>
[rank6]:     main(args, device, world_size, world_rank, local_rank)
[rank6]:   File "train.py", line 890, in main
[rank6]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank6]:   File "train.py", line 372, in training_step
[rank6]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank6]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank6]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank6]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank6]:     return self._call_impl(*args, **kwargs)
[rank6]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank6]:     return forward_call(*args, **kwargs)
[rank6]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank6]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank6]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank6]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank6]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank6]:     return _AllGather.apply(group, tensor)
[rank6]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank6]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank6]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank6]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank6]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank6]:     return func(*args, **kwargs)
[rank6]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank6]:     work = group.allgather([tensor_list], [tensor])
[rank6]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 48.00 GiB. GPU 6 has a total capacity of 63.98 GiB of which 6.25 GiB is free. Of the allocated memory 56.36 GiB is allocated by PyTorch, and 154.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
torch.Size([2, 24, 2048, 8192])
global rank 28: ddp rank 0 iter_start,iter_end = 0 8
global rank 28: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
[rank4]: Traceback (most recent call last):
[rank4]:   File "train.py", line 986, in <module>
[rank4]:     main(args, device, world_size, world_rank, local_rank)
[rank4]:   File "train.py", line 890, in main
[rank4]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank4]:   File "train.py", line 372, in training_step
[rank4]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank4]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank4]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank4]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank4]:     return self._call_impl(*args, **kwargs)
[rank4]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank4]:     return forward_call(*args, **kwargs)
[rank4]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank4]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank4]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank4]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank4]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank4]:     return _AllGather.apply(group, tensor)
[rank4]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank4]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank4]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank4]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank4]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank4]:     return func(*args, **kwargs)
[rank4]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank4]:     work = group.allgather([tensor_list], [tensor])
[rank4]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 48.00 GiB. GPU 4 has a total capacity of 63.98 GiB of which 6.25 GiB is free. Of the allocated memory 56.36 GiB is allocated by PyTorch, and 154.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
torch.Size([2, 24, 2048, 8192])
[rank3]: Traceback (most recent call last):
[rank3]:   File "train.py", line 986, in <module>
[rank3]:     main(args, device, world_size, world_rank, local_rank)
[rank3]:   File "train.py", line 890, in main
[rank3]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank3]:   File "train.py", line 372, in training_step
[rank3]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank3]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank3]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank3]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank3]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank3]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank3]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank3]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank3]:     return _AllGather.apply(group, tensor)
[rank3]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank3]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank3]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank3]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank3]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank3]:     return func(*args, **kwargs)
[rank3]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank3]:     work = group.allgather([tensor_list], [tensor])
[rank3]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 48.00 GiB. GPU 3 has a total capacity of 63.98 GiB of which 6.25 GiB is free. Of the allocated memory 56.36 GiB is allocated by PyTorch, and 154.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
global rank 11: ddp rank 0 iter_start,iter_end = 0 8
global rank 11: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 14: ddp rank 0 iter_start,iter_end = 0 8
global rank 14: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
torch.Size([2, 24, 2048, 8192])
torch.Size([2, 24, 2048, 8192])
torch.Size([2, 24, 2048, 8192])
torch.Size([2, 24, 2048, 8192])
[rank29]: Traceback (most recent call last):
[rank29]:   File "train.py", line 986, in <module>
[rank29]:     main(args, device, world_size, world_rank, local_rank)
[rank29]:   File "train.py", line 890, in main
[rank29]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank29]:   File "train.py", line 372, in training_step
[rank29]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank29]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank29]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank29]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank29]:     return self._call_impl(*args, **kwargs)
[rank29]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank29]:     return forward_call(*args, **kwargs)
[rank29]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank29]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank29]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank29]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank29]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank29]:     return _AllGather.apply(group, tensor)
[rank29]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank29]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank29]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank29]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank29]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank29]:     return func(*args, **kwargs)
[rank29]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank29]:     work = group.allgather([tensor_list], [tensor])
[rank29]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 48.00 GiB. GPU 5 has a total capacity of 63.98 GiB of which 6.44 GiB is free. Of the allocated memory 56.36 GiB is allocated by PyTorch, and 154.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank31]: Traceback (most recent call last):
[rank31]:   File "train.py", line 986, in <module>
[rank31]:     main(args, device, world_size, world_rank, local_rank)
[rank31]:   File "train.py", line 890, in main
[rank31]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank31]:   File "train.py", line 372, in training_step
[rank31]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank31]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank31]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank31]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank31]:     return self._call_impl(*args, **kwargs)
[rank31]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank31]:     return forward_call(*args, **kwargs)
[rank31]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank31]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank31]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank31]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank31]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank31]:     return _AllGather.apply(group, tensor)
[rank31]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank31]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank31]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank31]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank31]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank31]:     return func(*args, **kwargs)
[rank31]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank31]:     work = group.allgather([tensor_list], [tensor])
[rank31]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 48.00 GiB. GPU 7 has a total capacity of 63.98 GiB of which 6.44 GiB is free. Of the allocated memory 56.36 GiB is allocated by PyTorch, and 154.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank26]: Traceback (most recent call last):
[rank26]:   File "train.py", line 986, in <module>
[rank26]:     main(args, device, world_size, world_rank, local_rank)
[rank26]:   File "train.py", line 890, in main
[rank26]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank26]:   File "train.py", line 372, in training_step
[rank26]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank26]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank26]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank26]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank26]:     return self._call_impl(*args, **kwargs)
[rank26]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank26]:     return forward_call(*args, **kwargs)
[rank26]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank26]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank26]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank26]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank26]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank26]:     return _AllGather.apply(group, tensor)
[rank26]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank26]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank26]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank26]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank26]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank26]:     return func(*args, **kwargs)
[rank26]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank26]:     work = group.allgather([tensor_list], [tensor])
[rank26]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 48.00 GiB. GPU 2 has a total capacity of 63.98 GiB of which 6.44 GiB is free. Of the allocated memory 56.36 GiB is allocated by PyTorch, and 154.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
torch.Size([2, 24, 2048, 8192])
[rank30]: Traceback (most recent call last):
[rank30]:   File "train.py", line 986, in <module>
[rank30]:     main(args, device, world_size, world_rank, local_rank)
[rank30]:   File "train.py", line 890, in main
[rank30]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank30]:   File "train.py", line 372, in training_step
[rank30]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank30]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank30]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank30]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank30]:     return self._call_impl(*args, **kwargs)
[rank30]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank30]:     return forward_call(*args, **kwargs)
[rank30]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank30]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank30]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank30]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank30]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank30]:     return _AllGather.apply(group, tensor)
[rank30]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank30]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank30]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank30]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank30]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank30]:     return func(*args, **kwargs)
[rank30]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank30]:     work = group.allgather([tensor_list], [tensor])
[rank30]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 48.00 GiB. GPU 6 has a total capacity of 63.98 GiB of which 6.25 GiB is free. Of the allocated memory 56.36 GiB is allocated by PyTorch, and 154.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
torch.Size([2, 24, 2048, 8192])
[rank25]: Traceback (most recent call last):
[rank25]:   File "train.py", line 986, in <module>
[rank25]:     main(args, device, world_size, world_rank, local_rank)
[rank25]:   File "train.py", line 890, in main
[rank25]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank25]:   File "train.py", line 372, in training_step
[rank25]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank25]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank25]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank25]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank25]:     return self._call_impl(*args, **kwargs)
[rank25]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank25]:     return forward_call(*args, **kwargs)
[rank25]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank25]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank25]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank25]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank25]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank25]:     return _AllGather.apply(group, tensor)
[rank25]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank25]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank25]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank25]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank25]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank25]:     return func(*args, **kwargs)
[rank25]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank25]:     work = group.allgather([tensor_list], [tensor])
[rank25]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 48.00 GiB. GPU 1 has a total capacity of 63.98 GiB of which 6.25 GiB is free. Of the allocated memory 56.36 GiB is allocated by PyTorch, and 154.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank28]: Traceback (most recent call last):
[rank28]:   File "train.py", line 986, in <module>
[rank28]:     main(args, device, world_size, world_rank, local_rank)
[rank28]:   File "train.py", line 890, in main
[rank28]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank28]:   File "train.py", line 372, in training_step
[rank28]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank28]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank28]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank28]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank28]:     return self._call_impl(*args, **kwargs)
[rank28]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank28]:     return forward_call(*args, **kwargs)
[rank28]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank28]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank28]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank28]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank28]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank28]:     return _AllGather.apply(group, tensor)
[rank28]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank28]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank28]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank28]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank28]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank28]:     return func(*args, **kwargs)
[rank28]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank28]:     work = group.allgather([tensor_list], [tensor])
[rank28]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 48.00 GiB. GPU 4 has a total capacity of 63.98 GiB of which 6.25 GiB is free. Of the allocated memory 56.36 GiB is allocated by PyTorch, and 154.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
global rank 9: ddp rank 0 iter_start,iter_end = 0 8
global rank 9: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
torch.Size([2, 24, 2048, 8192])
global rank 13: ddp rank 0 iter_start,iter_end = 0 8
global rank 13: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
[rank27]: Traceback (most recent call last):
[rank27]:   File "train.py", line 986, in <module>
[rank27]:     main(args, device, world_size, world_rank, local_rank)
[rank27]:   File "train.py", line 890, in main
[rank27]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank27]:   File "train.py", line 372, in training_step
[rank27]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank27]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank27]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank27]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank27]:     return self._call_impl(*args, **kwargs)
[rank27]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank27]:     return forward_call(*args, **kwargs)
[rank27]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank27]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank27]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank27]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank27]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank27]:     return _AllGather.apply(group, tensor)
[rank27]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank27]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank27]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank27]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank27]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank27]:     return func(*args, **kwargs)
[rank27]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank27]:     work = group.allgather([tensor_list], [tensor])
[rank27]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 48.00 GiB. GPU 3 has a total capacity of 63.98 GiB of which 6.25 GiB is free. Of the allocated memory 56.36 GiB is allocated by PyTorch, and 154.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
global rank 15: ddp rank 0 iter_start,iter_end = 0 8
global rank 15: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
torch.Size([2, 24, 2048, 8192])
[rank11]: Traceback (most recent call last):
[rank11]:   File "train.py", line 986, in <module>
[rank11]:     main(args, device, world_size, world_rank, local_rank)
[rank11]:   File "train.py", line 890, in main
[rank11]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank11]:   File "train.py", line 372, in training_step
[rank11]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank11]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank11]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank11]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank11]:     return self._call_impl(*args, **kwargs)
[rank11]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank11]:     return forward_call(*args, **kwargs)
[rank11]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank11]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank11]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank11]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank11]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank11]:     return _AllGather.apply(group, tensor)
[rank11]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank11]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank11]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank11]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank11]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank11]:     return func(*args, **kwargs)
[rank11]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank11]:     work = group.allgather([tensor_list], [tensor])
[rank11]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 48.00 GiB. GPU 3 has a total capacity of 63.98 GiB of which 6.25 GiB is free. Of the allocated memory 56.36 GiB is allocated by PyTorch, and 154.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
torch.Size([2, 24, 2048, 8192])
[rank24]: Traceback (most recent call last):
[rank24]:   File "train.py", line 986, in <module>
[rank24]:     main(args, device, world_size, world_rank, local_rank)
[rank24]:   File "train.py", line 890, in main
[rank24]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank24]:   File "train.py", line 372, in training_step
[rank24]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank24]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank24]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank24]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank24]:     return self._call_impl(*args, **kwargs)
[rank24]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank24]:     return forward_call(*args, **kwargs)
[rank24]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank24]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank24]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank24]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank24]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank24]:     return _AllGather.apply(group, tensor)
[rank24]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank24]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank24]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank24]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank24]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank24]:     return func(*args, **kwargs)
[rank24]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank24]:     work = group.allgather([tensor_list], [tensor])
[rank24]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 48.00 GiB. GPU 0 has a total capacity of 63.98 GiB of which 6.44 GiB is free. Of the allocated memory 56.36 GiB is allocated by PyTorch, and 154.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
global rank 8: ddp rank 0 iter_start,iter_end = 0 8
global rank 8: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
global rank 10: ddp rank 0 iter_start,iter_end = 0 8
global rank 10: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
torch.Size([2, 24, 2048, 8192])
[rank9]: Traceback (most recent call last):
[rank9]:   File "train.py", line 986, in <module>
[rank9]:     main(args, device, world_size, world_rank, local_rank)
[rank9]:   File "train.py", line 890, in main
[rank9]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank9]:   File "train.py", line 372, in training_step
[rank9]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank9]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank9]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank9]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank9]:     return self._call_impl(*args, **kwargs)
[rank9]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank9]:     return forward_call(*args, **kwargs)
[rank9]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank9]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank9]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank9]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank9]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank9]:     return _AllGather.apply(group, tensor)
[rank9]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank9]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank9]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank9]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank9]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank9]:     return func(*args, **kwargs)
[rank9]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank9]:     work = group.allgather([tensor_list], [tensor])
[rank9]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 48.00 GiB. GPU 1 has a total capacity of 63.98 GiB of which 6.25 GiB is free. Of the allocated memory 56.36 GiB is allocated by PyTorch, and 154.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
global rank 12: ddp rank 0 iter_start,iter_end = 0 8
global rank 12: ddp rank 0 NpyReader: /lustre/orion/stf218/scratch/atsaris/code/tmp25/climaxSC24_end2end/DATA_tmp_5.0/train/2018_0.npz
torch.Size([2, 24, 2048, 8192])
[rank13]: Traceback (most recent call last):
[rank13]:   File "train.py", line 986, in <module>
[rank13]:     main(args, device, world_size, world_rank, local_rank)
[rank13]:   File "train.py", line 890, in main
[rank13]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank13]:   File "train.py", line 372, in training_step
[rank13]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank13]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank13]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank13]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank13]:     return self._call_impl(*args, **kwargs)
[rank13]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank13]:     return forward_call(*args, **kwargs)
[rank13]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank13]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank13]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank13]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank13]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank13]:     return _AllGather.apply(group, tensor)
[rank13]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank13]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank13]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank13]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank13]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank13]:     return func(*args, **kwargs)
[rank13]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank13]:     work = group.allgather([tensor_list], [tensor])
[rank13]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 48.00 GiB. GPU 5 has a total capacity of 63.98 GiB of which 6.44 GiB is free. Of the allocated memory 56.36 GiB is allocated by PyTorch, and 154.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
torch.Size([2, 24, 2048, 8192])
[rank15]: Traceback (most recent call last):
[rank15]:   File "train.py", line 986, in <module>
[rank15]:     main(args, device, world_size, world_rank, local_rank)
[rank15]:   File "train.py", line 890, in main
[rank15]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank15]:   File "train.py", line 372, in training_step
[rank15]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank15]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank15]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank15]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank15]:     return self._call_impl(*args, **kwargs)
[rank15]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank15]:     return forward_call(*args, **kwargs)
[rank15]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank15]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank15]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank15]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank15]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank15]:     return _AllGather.apply(group, tensor)
[rank15]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank15]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank15]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank15]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank15]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank15]:     return func(*args, **kwargs)
[rank15]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank15]:     work = group.allgather([tensor_list], [tensor])
[rank15]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 48.00 GiB. GPU 7 has a total capacity of 63.98 GiB of which 6.44 GiB is free. Of the allocated memory 56.36 GiB is allocated by PyTorch, and 154.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
torch.Size([2, 24, 2048, 8192])
torch.Size([2, 24, 2048, 8192])
torch.Size([2, 24, 2048, 8192])
[rank14]: Traceback (most recent call last):
[rank14]:   File "train.py", line 986, in <module>
[rank14]:     main(args, device, world_size, world_rank, local_rank)
[rank14]:   File "train.py", line 890, in main
[rank14]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank14]:   File "train.py", line 372, in training_step
[rank14]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank14]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank14]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank14]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank14]:     return self._call_impl(*args, **kwargs)
[rank14]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank14]:     return forward_call(*args, **kwargs)
[rank14]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank14]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank14]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank14]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank14]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank14]:     return _AllGather.apply(group, tensor)
[rank14]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank14]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank14]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank14]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank14]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank14]:     return func(*args, **kwargs)
[rank14]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank14]:     work = group.allgather([tensor_list], [tensor])
[rank14]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 48.00 GiB. GPU 6 has a total capacity of 63.98 GiB of which 6.25 GiB is free. Of the allocated memory 56.36 GiB is allocated by PyTorch, and 154.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank10]: Traceback (most recent call last):
[rank10]:   File "train.py", line 986, in <module>
[rank10]:     main(args, device, world_size, world_rank, local_rank)
[rank10]:   File "train.py", line 890, in main
[rank10]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank10]:   File "train.py", line 372, in training_step
[rank10]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank10]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank10]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank10]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank10]:     return self._call_impl(*args, **kwargs)
[rank10]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank10]:     return forward_call(*args, **kwargs)
[rank10]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank10]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank10]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank10]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank10]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank10]:     return _AllGather.apply(group, tensor)
[rank10]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank10]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank10]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank10]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank10]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank10]:     return func(*args, **kwargs)
[rank10]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank10]:     work = group.allgather([tensor_list], [tensor])
[rank10]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 48.00 GiB. GPU 2 has a total capacity of 63.98 GiB of which 6.44 GiB is free. Of the allocated memory 56.36 GiB is allocated by PyTorch, and 154.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank8]: Traceback (most recent call last):
[rank8]:   File "train.py", line 986, in <module>
[rank8]:     main(args, device, world_size, world_rank, local_rank)
[rank8]:   File "train.py", line 890, in main
[rank8]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank8]:   File "train.py", line 372, in training_step
[rank8]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank8]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank8]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank8]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank8]:     return self._call_impl(*args, **kwargs)
[rank8]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank8]:     return forward_call(*args, **kwargs)
[rank8]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank8]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank8]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank8]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank8]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank8]:     return _AllGather.apply(group, tensor)
[rank8]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank8]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank8]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank8]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank8]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank8]:     return func(*args, **kwargs)
[rank8]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank8]:     work = group.allgather([tensor_list], [tensor])
[rank8]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 48.00 GiB. GPU 0 has a total capacity of 63.98 GiB of which 6.44 GiB is free. Of the allocated memory 56.36 GiB is allocated by PyTorch, and 154.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
torch.Size([2, 24, 2048, 8192])
[rank12]: Traceback (most recent call last):
[rank12]:   File "train.py", line 986, in <module>
[rank12]:     main(args, device, world_size, world_rank, local_rank)
[rank12]:   File "train.py", line 890, in main
[rank12]:     loss = training_step(batch_syn, device, batch_idx,model,lat)
[rank12]:   File "train.py", line 372, in training_step
[rank12]:     loss_dict, _ = net.forward(x, y, lead_times, variables, out_variables, [lat_weighted_mse], lat=lat)
[rank12]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 853, in forward
[rank12]:     output = self._fsdp_wrapped_module(*args, **kwargs)
[rank12]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
[rank12]:     return self._call_impl(*args, **kwargs)
[rank12]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
[rank12]:     return forward_call(*args, **kwargs)
[rank12]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 406, in forward
[rank12]:     out_transformers = self.forward_encoder(x, lead_times, variables)  # B, V~ * L, D
[rank12]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/arch_hier_token_noagg_self.py", line 353, in forward_encoder
[rank12]:     gathered_tensors = mod_all_gather(x, self.tensor_par_group)
[rank12]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 247, in all_gather
[rank12]:     return _AllGather.apply(group, tensor)
[rank12]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/autograd/function.py", line 562, in apply
[rank12]:     return super().apply(*args, **kwargs)  # type: ignore[misc]
[rank12]:   File "/lustre/orion/stf218/scratch/atsaris/code/dev_climax_gb/climax/dist_functions.py", line 576, in forward
[rank12]:     dist.all_gather(out_tensor_list, tensor, group=group)
[rank12]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank12]:     return func(*args, **kwargs)
[rank12]:   File "/lustre/orion/world-shared/stf218/atsaris/DEEPCAM_2022/new_env/miniconda/env_rocm/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2745, in all_gather
[rank12]:     work = group.allgather([tensor_list], [tensor])
[rank12]: torch.cuda.OutOfMemoryError: HIP out of memory. Tried to allocate 48.00 GiB. GPU 4 has a total capacity of 63.98 GiB of which 6.25 GiB is free. Of the allocated memory 56.36 GiB is allocated by PyTorch, and 154.61 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_HIP_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
sleeping...
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
Done
0.07user 0.57system 9:29.72elapsed 0%CPU (0avgtext+0avgdata 17408maxresident)k
6654inputs+5128outputs (10major+3228minor)pagefaults 0swaps
